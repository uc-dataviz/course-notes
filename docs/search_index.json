[["index.html", "Data Mining and Data Visualization for the Social Sciences MACS 24000/34000 Syllabus Contact information Course description Prerequisites Course schedule What do I need for this course? How will I be evaluated? Statement on Disabilities", " Data Mining and Data Visualization for the Social Sciences MACS 24000/34000 Benjamin Soltoff Philip Waggoner 2022-03-04 Syllabus Contact information Dr. Benjamin Soltoff Dr. Philip Waggoner Email soltoffbc@uchicago.edu pdwaggoner@uchicago.edu GitHub bensoltoff pdwaggoner Office hours TBD TBD Course description This course introduces students to techniques for extracting and communicating knowledge from data. In the first half, students study visualizations as a method for summarizing information and reporting analysis and conclusions in a compelling format. This introduces the ideas and methods of data visualization, with emphasis on both why you are doing something as well as how to produce optimal visualizations. In the second half, students are introduced to the rapidly developing world of data mining. Focus will be on knowledge discovery and pattern recognition in the context of social science problem solving. From partitioning and anomaly detection to text clustering, high-dimensional mining, and deep learning, students will be given a thorough introduction to prominent techniques for exploring and discovering patterns in data. Throughout the course, class sessions will combine lecture, coding challenges, and computational problem solving to encourage wide engagement with the techniques using the R programming language. Prerequisites MACS 20500, CS 10121, or a similar introductory programming course. Experience in R is required. STAT 23400 or similar introductory statistics course is expected. Experience with machine learning is helpful but not required. Course schedule Week 1 (Data visualization with Dr. Soltoff) Date Topic 27-Jul Introduction to data visualization 28-Jul Showing the right numbers 29-Jul Making plots pretty and clean 30-Jul Geospatial visualizations 31-Jul Interactive Shiny applications Week 2 (Data mining with Dr. Waggoner) Date Topic 3-Aug Foundations of Data Mining 4-Aug Patterns &amp; Associations 5-Aug Unsupervised Machine Learning 6-Aug Mining Labeled Data &amp; Text 7-Aug Deep Learning What do I need for this course? Class sessions are a mix of lecture, demonstration, and live coding. It is essential to have a computer so you can follow along and complete the exercises. Before the course starts, you should install the following software on your computer: R - easiest approach is to select a pre-compiled binary appropriate for your operating system. RStudio IDE - this is a powerful user interface for programming in R. You could use base R, but you would regret it. Git - Git is a version control system which is used to manage projects and track changes in computer files. Once installed, it can be integrated into RStudio to manage your course assignments and other projects. Comprehensive instructions for downloading and setting up this software can be found here. All readings (e.g., papers, book chapters) will be open source, with either links or citations provided. How will I be evaluated? Students will submit daily problem sets each worth 100 points. Each assignment is due prior to the start of class (10 am CDT) the following day. There is no final exam or project in the course. Assignments will be submitted via GitHub Classroom. Statement on Disabilities The University of Chicago is committed to diversity and rigorous inquiry from multiple perspectives. The MAPSS, CIR, and Computation programs share this commitment and seek to foster productive learning environments based upon inclusion, open communication, and mutual respect for a diverse range of identities, experiences, and positions. This course is open to all students who meet the academic requirements for participation. Any student who has a documented need for accommodation should contact Student Disability Services (773-702-6000 or disabilities@uchicago.edu) and provide us (Dr. Soltoff and Dr. Waggoner) with a copy of your Accommodation Determination Letter as soon as possible. "],["intro-data-viz.html", "Day 1 Introduction to data visualization Learning objectives Assigned readings 1.1 Why visualize data? 1.2 What makes visualizations bad 1.3 Purpose of visualizations 1.4 What makes a good visualization 1.5 Perception and data visualization 1.6 Identifying appropriate graphical forms 1.7 Grammar of graphics 1.8 Building Minard’s map in R Session info", " Day 1 Introduction to data visualization Learning objectives 1.0.1 Morning Introduce myself Identify major course objectives Review the purpose of data visualizations Examine how perception and cognition influence the interpretation of data visualizations Assess several historic visualizations for their strengths and weaknesses 1.0.2 Afternoon Review the grammar of graphics and ggplot2 Generate clean, interpretable visualizations Assigned readings Chapters 1-3, Healy (2018) - accessible via the book’s website Chapter 2, Cairo (2016) - accessible via electronic course reserve 1.1 Why visualize data? Research methods classes in graduate school generally teach important skills such as probability and statistical theory, regression, analysis of variance (ANOVA), maximum likelihood estimation (MLE), etc. While these are important methods for analyzing data and assessing research questions, sometimes drawing a picture (aka visualization) can be more precise than conventional statistical computations.1 1.1.1 Anscombe’s quartet 1.1.2 Datasaurus dozen Remarkably each of the datasets have the same summary statistics and linear relationships, yet they are drastically different in appearance! A good picture tells the reader much more than any table or text can provide. 1.1.3 Outliers 1.2 What makes visualizations bad 1.2.1 Lies 1.2.2 Dual axes 1.2.3 Lacking functionality 1.2.4 Junky 1.3 Purpose of visualizations A visualization is “any kind of visual representation of information designed to enable communication, analysis, discovery, exploration, etc.”2 However what you seek to communicate can vary widely depending on your goals, and therefore effects the type of visualization you should design. 1.3.1 Statistical graphics Statistical graphics seek to visualize abstract data typically of the quantitative form. The goal is to convey data accurately and reveal the underlying structure, but are generally not explorative and interactive and may not always yield an aesthetically pleasing form. 1.3.1.1 Examples 1.3.1.1.1 Double-time bar charts Figure 1.1: Double-time bar chart of crime in the city of San Francisco, 2009-10. Source: Visualizing Time with the Double-Time Bar Chart Each set of 24 bars show the same data. The top bars run from midnight to 11pm. The bottom bars run from noon to 11am. Highlighted regions represent 6-5 (6am-5pm; 6pm-5am) Colors represent (roughly) day and night (yellow for day, blue for night) Enables representing trends over a 24 hour period without breaking arbitrarily at midnight Figure 1.2: Double-time bar chart of crime in the city of San Francisco, 2009-10. Source: Visualizing Time with the Double-Time Bar Chart Compare different categories of crimes using small multiples (aka facets in ggplot2 language) 1.3.2 Information dashboards Information dashboards are popular in business and industry. They visualize abstract data, frequently (though not always) over time. The goal is to convey large amounts of information quickly and identify outliers and trends. The downside is that they can become extremely dense. 1.3.2.1 Examples COVID-19 United States Cases by County California COVID Assessment Tool - built in Shiny! 1.3.3 Infographics Infographics depict abstract data in an effort to be eye-catching and capture attention, and convey information quickly. Unfortunately they are frequently not accurate, do not use space efficiently, and may not encourage exploration of the data. 1.3.3.1 Examples Figure 1.3: Extremely sexual sun stroking. Source: The top 10 worst infographics ever created Figure 1.4: Source: 11 Most Useless And Misleading Infographics On The Internet Figure 1.5: Source: WTF Visualizations 1.3.4 Informative art Informative art visualizes abstract data in an effort to make visualization ambient or a part of everyday life. The goal is to aesthetically please the audience, not to be informative. 1.3.4.1 Examples Debussy, Clair de lune (piano music) 1.4 What makes a good visualization 1.4.1 Dr. John Snow and cholera outbreak in London Figure 1.6: Original map made by John Snow in 1854. Cholera cases are highlighted in black. Source: Wikipedia. At this point in time the theory of bacteria was not widely accepted by the medical community or the public.3 A mother washed her baby’s diaper in a well in 1854 in London, sparking an outbreak of cholera, an intestinal disease that causes vomiting, diarrhea, and eventually death. This disease had presented itself previously in London but its cause was still unknown. Dr. John Snow lived in Soho, the suburb of London where the disease manifested in 1854, and wanted to understand how cholera spreads through a population (an early day epidemiologist). Snow recorded the location of individuals who contracted cholera, including their places of residence and employment. He used this information to draw a map of the region, recording the location of individuals who contracted the disease. They seemed to be clustered around the well pump along Broad Street. Snow used this map to deduce the source of the outbreak was the well, along the way ruling out other causes by noting individuals who lived in the area and did not contract cholera, identifying that these individuals did not drink from the well. Based on this information, the government removed the handle from the well pump so the public could not draw water from it. As a result, the cholera epidemic ended. What makes this a good visualization? One of the earliest examples of statistical visualizations 1.4.2 Minard’s map of Napoleon’s march on Russia Figure 1.7: Charles Minard’s 1869 chart showing the number of men in Napoleon’s 1812 Russian campaign army, their movements, as well as the temperature they encountered on the return path. Source: Wikipedia. Figure 1.8: English translation of Minard’s map. Source: Wikipedia. This illustration is identifed in Edward Tufte’s The Visual Display of Quantitative Information as one of “the best statistical drawings ever created”.4 It also demonstrates a very important rule of warfare: never invade Russia in the winter. In 1812, Napoleon ruled most of Europe. He wanted to seize control of the British islands, but could not overcome the UK defenses. He decides to impose an embargo to weaken the nation in preparation for invasion, but Russia refused to participate. Angered at this decision, Napoleon launched an invasion of Russia with over 400,000 troops in the summer of 1812. Russia is unable to defeat Napoleon in battle, but instead waged a war of attrition. The Russian army was in near constant retreat, burning or destroying anything of value along the way to deny France usable resources. While Napoleon’s army maintained the military advantage, his lack of food and the emerging European winter decimated his forces. He left France with an army of approximately 422,000 soldiers; he returned to France with just 10,000. Charles Minard’s map is a stunning achievement for his era. It incorporates data across six dimensions to tell the story of Napoleon’s failure. The graph depicts: Size of the army Location in two-dimensions (latitude and longitude) Direction of the army’s movement Temperature on dates during Napoleon’s retreat What makes this such an effective visualization?5 Forces visual comparisons (colored bands for advancing and retreating) Shows causality (temperature chart) Captures multivariate complexity Integrates text and graphic into a coherent whole (perhaps the first infographic, and done well!) Illustrates high quality content (based on reliable data) Places comparisons adjacent to each other (all on the same page, no jumping back and forth between pages) Mimimalistic in nature (avoids what we will later term “chart junk”) Data maps were one of the first data visualizations, though it took thousands of years after the first cartographic maps before data maps came together. 1.5 Perception and data visualization Driven by science Lots to learn here, but we don’t have time this week to dive in deep! Important to remember human perception and cognition is a process - design visualizations remembering this fact Work to simplify perception so the story is clear and easy to tell 1.5.1 Visual tasks and decoding graphs A graphical form that involves elementary perceptual tasks that lead to more accurate judgments than another graphical form (with the same quantitiative information) will result in better organization and increase the chances of a correct perception of patterns and behavior. Figure 1.9: Figure 1.22 from Healy (2018) Figure 1.10: Figure 1.23 from Healy (2018) 1.5.2 Channels and decoding information Figure 1.11: Figure 1.24 from Healy (2018) Figure 1.12: Figure 1.26 from Healy (2018) 1.5.3 Example of decoding information 1.5.3.1 Bar chart vs. pie chart 1.5.4 Choropleths 1.6 Identifying appropriate graphical forms Once we learn more about elementary perceptual tasks and why some channels are better than others, choosing appropriate graphical forms will become more complex as we more carefully consider the specific marks and channels used to communicate our data. Here, we will identify at a high-level some suggestions to follow when determining an appropriate graphical form and consider several basic types of charts and their appropriate use cases. 1.6.1 Cairo’s Truthful Art suggested approach Think about the task or tasks you want to enable Try different graphic forms Arrange the components of the graphic Test the outcomes These suggestions are a bit vague, but I really like the last one: test the outcomes.6 Something that appears intuitive to you may not appear the same way to a different audience. Especially in the context of statistical graphics and presenting academic results, you become steeped in the data so much that you assume everyone else has lots of background knowledge about your dataset. However they do not possess this knowledge: that’s why they’re reading your visualization. 1.6.2 What is the story? Another of Cairo’s suggestions is really useful: what is the story you want to tell? Figure 1.13: Source: Cairo (2016) Consider the bottom two graphs: which is the better graph? Well, it depends. If our goal is the same as Piketty’s, then we want to compare Europe to the rest of the continents. The stacked area chart accomplishes this goal just fine by placing Europe on the bottom, sitting at the baseline. However if the goal is to compare all continents with one another, then this graph does not do a good job. Making comparisons with Africa and America are difficult because their baselines are dependent on Europe’s GDP percentage. Instead, we’d rather all continents start at the same baseline to enable easy comparisons, which is what the non-stacked grouped line chart accomplishes. Depending on your story, you want to choose a graph/chart type that best tells that story. 1.7 Grammar of graphics Google defines a grammar as “the whole system and structure of a language or of languages in general, usually taken as consisting of syntax and morphology (including inflections) and sometimes also phonology and semantics”.7 Others consider a grammar to be “the fundamental principles or rules of an art or science”.8 Applied to visualizations, a grammar of graphics is a grammar used to describe and create a wide range of statistical graphics.9 The layered grammar of graphics approach is implemented in ggplot2, a widely used graphics library for R. All graphics in this library are built using a layered approach, building layers up to create the final graphic. 1.7.1 Components of the layered grammar of graphics Layer Data Mapping Statistical transformation (stat) Geometric object (geom) Position adjustment (position) Scale Coordinate system (coord) Faceting (facet) Defaults Data Mapping 1.7.2 Layer Layers are used to create the objects on a plot. They are defined by five basic parts: Data Mapping Statistical transformation (stat) Geometric object (geom) Position adjustment (position) Layers are typically related to one another and share many common features. For instance, multiple layers can be built using the same underlying data. An example would be a scatterplot overlayed with a smoothed regression line to summarize the relationship between two variables: 1.7.3 Data and mapping Data defines the source of the information to be visualized, but is independent from the other elements. So a layered graphic can be built which utilizes different data sources while keeping the other components the same. For our running example, let’s use the county_data dataset in the socviz package.10 Mapping defines how the variables are applied to the plot. So if we were graphing information from county_data, we might map a county’s median household income to the \\(x\\) position and 2016 Democratic presidential vote percentage to the \\(y\\) position. county_data %&gt;% select(hh_income, per_dem_2016) %&gt;% rename( x = hh_income, y = per_dem_2016 ) ## # A tibble: 3,141 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 53682 0.240 ## 2 50221 0.196 ## 3 32911 0.467 ## 4 36447 0.214 ## 5 44145 0.0847 ## 6 32033 0.751 ## 7 29918 0.428 ## 8 39962 0.279 ## 9 32402 0.418 ## 10 34907 0.145 ## # … with 3,131 more rows 1.7.4 Statistical transformation A statistical transformation (stat) transforms the data, generally by summarizing the information. For instance, in a bar graph you typically are not trying to graph the raw data because this doesn’t make any inherent sense. Instead, you might summarize the data by graphing the total number of observations within a set of categories. Or if you have a dataset with many observations, you might transform the data into a smoothing line which summarizes the overall pattern of the relationship between variables by calculating the mean of \\(y\\) conditional on \\(x\\). A stat is a function that takes in a dataset as the input and returns a dataset as the output; a stat can add new variables to the original dataset, or create an entirely new dataset. So instead of graphing this data in its raw form: county_data %&gt;% select(state) ## # A tibble: 3,141 × 1 ## state ## &lt;fct&gt; ## 1 AL ## 2 AL ## 3 AL ## 4 AL ## 5 AL ## 6 AL ## 7 AL ## 8 AL ## 9 AL ## 10 AL ## # … with 3,131 more rows You would transform it to: county_data %&gt;% count(state) ## # A tibble: 51 × 2 ## state n ## &lt;fct&gt; &lt;int&gt; ## 1 AK 29 ## 2 AL 67 ## 3 AR 75 ## 4 AZ 15 ## 5 CA 58 ## 6 CO 64 ## 7 CT 8 ## 8 DC 1 ## 9 DE 3 ## 10 FL 67 ## # … with 41 more rows Sometimes you don’t need to make a statistical transformation. For example, in a scatterplot you use the raw values for the \\(x\\) and \\(y\\) variables to map onto the graph. In these situations, the statistical transformation is an identity transformation - the stat simply passes in the original dataset and exports the exact same dataset. 1.7.5 Geometric objects Geometric objects (geoms) control the type of plot you create. Geoms are classified by their dimensionality: 0 dimensions - point, text 1 dimension - path, line 2 dimensions - polygon, interval Each geom can only display certain aesthetics or visual attributes of the geom. For example, a point geom has position, color, shape, and size aesthetics. ggplot( data = county_data, mapping = aes( x = hh_income, y = per_dem_2016, color = flipped ) ) + geom_point() + ggtitle(&quot;A point geom with position and color aesthetics&quot;) Position defines where each point is drawn on the plot Color defines the color of each point. Here the color is determined by whether the county flipped parties from 2012 to 2016 Whereas a bar geom has position, height, width, and fill color. ggplot(data = county_data, mapping = aes(x = pop_dens)) + geom_bar() + ggtitle(&quot;A bar geom with position and height aesthetics&quot;) Position determines the starting location (origin) of each bar Height determines how tall to draw the bar. Here the height is based on the number of observations in the dataset for each population density category. 1.7.6 Position adjustment Sometimes with dense data we need to adjust the position of elements on the plot, otherwise data points might obscure one another. Bar plots frequently stack or dodge the bars to avoid overlap: count(x = county_data, su_gun4, pop_dens) %&gt;% ggplot(mapping = aes(x = pop_dens, y = n, fill = su_gun4)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(label = &quot;A stacked bar chart&quot;) count(x = county_data, su_gun4, pop_dens) %&gt;% ggplot(mapping = aes(x = pop_dens, y = n, fill = su_gun4)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + ggtitle(label = &quot;A dodged bar chart&quot;) Sometimes scatterplots with few unique \\(x\\) and \\(y\\) values are jittered (random noise is added) to reduce overplotting. ggplot(data = county_data, mapping = aes(x = pop_dens, y = per_dem_2016)) + geom_point() + ggtitle(&quot;A point geom with obscured data points&quot;) ggplot(data = county_data, mapping = aes(x = pop_dens, y = per_dem_2016)) + geom_jitter() + ggtitle(&quot;A point geom with jittered data points&quot;) 1.7.7 Scale A scale controls how data is mapped to aesthetic attributes, so we need one scale for every aesthetic property employed in a layer. For example, this graph defines a scale for color: ggplot( data = county_data, mapping = aes( x = hh_income, y = per_dem_2016, color = pop_dens ) ) + geom_point() Note that the scale is consistent - every point for a different population density category is the same color. The scale can be changed to use a different color palette: ggplot( data = county_data, mapping = aes( x = hh_income, y = per_dem_2016, color = pop_dens ) ) + geom_point() + scale_color_brewer(type = &quot;seq&quot;) Now we are using a different palette, but the scale is still consistent. 1.7.8 Coordinate system A coordinate system (coord) maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn. Plots typically use two coordinates (\\(x, y\\)), but could use any number of coordinates. Most plots are drawn using the Cartesian coordinate system: This system requires a fixed and equal spacing between values on the axes. That is, the graph draws the same distance between 1 and 2 as it does between 5 and 6. The graph could be drawn using a semi-log coordinate system which logarithmically compresses the distance on an axis: Or could even be drawn using polar coordinates: 1.7.9 Faceting Faceting can be used to split the data up into subsets of the entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions, and allows the subsets to be visualized on the same plot (known as conditioned or trellis plots). The faceting specification describes which variables should be used to split up the data, and how they should be arranged. ggplot( data = county_data, mapping = aes(x = hh_income, y = per_dem_2016) ) + geom_point() + facet_wrap(~pop_dens) 1.7.10 Defaults Rather than explicitly declaring each component of a layered graphic (which will use more code and introduces opportunities for errors), we can establish intelligent defaults for specific geoms and scales. For instance, whenever we want to use a bar geom, we can default to using a stat that counts the number of observations in each group of our variable in the \\(x\\) position. Consider the following scenario: you wish to generate a scatterplot visualizing the relationship between household income and 2016 presidential Democratic vote share. With no defaults, the code to generate this graph is: ggplot() + layer( data = county_data, mapping = aes(x = hh_income, y = per_dem_2016), geom = &quot;point&quot;, stat = &quot;identity&quot;, position = &quot;identity&quot; ) + scale_x_continuous() + scale_y_continuous() + coord_cartesian() The above code: Creates a new plot object (ggplot) Adds a layer (layer) Specifies the data (county_data) Maps household income to the \\(x\\) position and 2016 Democratic vote share to the \\(y\\) position (mapping) Uses the point geometric transformation (geom = \"point\") Implements an identity transformation and position (stat = \"identity\" and position = \"identity\") Establishes two continuous position scales (scale_x_continuous and scale_y_continuous) Declares a cartesian coordinate system (coord_cartesian) How can we simplify this using intelligent defaults? We only need to specify one geom and stat, since each geom has a default stat. Cartesian coordinate systems are most commonly used, so it should be the default. Default scales can be added based on the aesthetic and type of variables. Continuous values are transformed with a linear scaling. Discrete values are mapped to integers. Scales for aesthetics such as color, fill, and size can also be intelligently defaulted. Using these defaults, we can rewrite the above code as: ggplot() + geom_point( data = county_data, mapping = aes(x = hh_income, y = per_dem_2016) ) This generates the exact same plot, but uses fewer lines of code. Because multiple layers can use the same components (data, mapping, etc.), we can also specify that information in the ggplot() function rather than in the layer() function: ggplot( data = county_data, mapping = aes(x = hh_income, y = per_dem_2016) ) + geom_point() And as we will learn, function arguments in R use specific ordering, so we can omit the explicit call to data and mapping: ggplot(county_data, aes(hh_income, per_dem_2016)) + geom_point() With this specification, it is easy to build the graphic up with additional layers, without modifying the original code: ggplot(county_data, aes(hh_income, per_dem_2016)) + geom_point() + geom_smooth() Because we called aes(hh_income, per_dem_2016) within the ggplot() function, it is automatically passed along to both geom_point() and geom_smooth(). If we fail to do this, we get an error: ggplot(county_data) + geom_point(aes(hh_income, per_dem_2016)) + geom_smooth() ## Error in `check_required_aesthetics()`: ## ! stat_smooth requires the following missing aesthetics: x and y 1.8 Building Minard’s map in R # get data on troop movements and city names troops &lt;- read_table(&quot;https://cfss.uchicago.edu/data/minard-troops.txt&quot;) cities &lt;- read_table(&quot;https://cfss.uchicago.edu/data/minard-cities.txt&quot;) troops ## # A tibble: 51 × 5 ## long lat survivors direction group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 24 54.9 340000 A 1 ## 2 24.5 55 340000 A 1 ## 3 25.5 54.5 340000 A 1 ## 4 26 54.7 320000 A 1 ## 5 27 54.8 300000 A 1 ## 6 28 54.9 280000 A 1 ## 7 28.5 55 240000 A 1 ## 8 29 55.1 210000 A 1 ## 9 30 55.2 180000 A 1 ## 10 30.3 55.3 175000 A 1 ## # … with 41 more rows cities ## # A tibble: 20 × 3 ## long lat city ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 24 55 Kowno ## 2 25.3 54.7 Wilna ## 3 26.4 54.4 Smorgoni ## 4 26.8 54.3 Moiodexno ## 5 27.7 55.2 Gloubokoe ## 6 27.6 53.9 Minsk ## 7 28.5 54.3 Studienska ## 8 28.7 55.5 Polotzk ## 9 29.2 54.4 Bobr ## 10 30.2 55.3 Witebsk ## 11 30.4 54.5 Orscha ## 12 30.4 53.9 Mohilow ## 13 32 54.8 Smolensk ## 14 33.2 54.9 Dorogobouge ## 15 34.3 55.2 Wixma ## 16 34.4 55.5 Chjat ## 17 36 55.5 Mojaisk ## 18 37.6 55.8 Moscou ## 19 36.6 55.3 Tarantino ## 20 36.5 55 Malo-Jarosewii 1.8.1 Exercise: Define the grammar of graphics for this graph Click here for solution Layer Data - troops Mapping \\(x\\) and \\(y\\) - troop position (lat and long) Size - survivors Color - direction Statistical transformation (stat) - identity Geometric object (geom) - path Position adjustment (position) - none Layer Data - cities Mapping \\(x\\) and \\(y\\) - city position (lat and long) Label - city Statistical transformation (stat) - identity Geometric object (geom) - text Position adjustment (position) - none Scale Size - range of widths for troop path Color - colors to indicate advancing or retreating troops Coordinate system - map projection (Mercator or something else) Faceting - none 1.8.2 Write the R code Download the necessary data files using usethis::use_course(\"css-data-mining-viz/grammar-of-graphics\"). First we want to build the layer for the troop movement: plot_troops &lt;- ggplot(data = troops, mapping = aes(x = long, y = lat)) + geom_path(mapping = aes( size = survivors, color = direction, group = group) ) plot_troops Next let’s add the cities layer: plot_both &lt;- plot_troops + geom_text(data = cities, mapping = aes(label = city), size = 4) plot_both Now that the basic information is on there, we want to clean up the graph and polish the visualization by: Adjusting the size scale aesthetics for troop movement to better highlight the loss of troops over the campaign. Change the default colors to mimic Minard’s original grey and tan palette. Change the coordinate system to a map-based system that draws the \\(x\\) and \\(y\\) axes at equal intervals. Give the map a title and remove the axis labels. plot_polished &lt;- plot_both + scale_size(range = c(0, 12), breaks = c(1e04, 5e04, 1e05, 3e05), labels = scales::comma) + scale_color_manual(values = c(&quot;tan&quot;, &quot;grey50&quot;)) + coord_map() + labs(title = &quot;Map of Napoleon&#39;s Russian campaign of 1812&quot;, x = NULL, y = NULL) plot_polished Finally we can change the default ggplot theme to remove the background and grid lines, as well as the legend for advance/retreat: plot_polished + theme_void() + scale_color_manual(values = c(&quot;tan&quot;, &quot;grey50&quot;), guide = FALSE) Session info ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.2 (2021-11-01) ## os macOS Monterey 12.2.1 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Chicago ## date 2022-03-04 ## pandoc 2.17.1.1 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.1.1) ## bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.1) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.1.1) ## broom * 0.7.12 2022-01-28 [1] CRAN (R 4.1.1) ## bslib 0.3.1 2021-10-06 [1] CRAN (R 4.1.1) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.1.1) ## callr 3.7.0 2021-04-20 [1] CRAN (R 4.1.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.1.0) ## class 7.3-20 2022-01-13 [1] CRAN (R 4.1.1) ## classInt 0.4-3 2020-04-07 [1] CRAN (R 4.1.0) ## cli 3.2.0 2022-02-14 [1] CRAN (R 4.1.1) ## codetools 0.2-18 2020-11-04 [1] CRAN (R 4.1.2) ## colorspace 2.0-3 2022-02-21 [1] CRAN (R 4.1.1) ## crayon 1.5.0 2022-02-14 [1] CRAN (R 4.1.1) ## crosstalk 1.2.0 2021-11-04 [1] CRAN (R 4.1.1) ## curl 4.3.2 2021-06-23 [1] CRAN (R 4.1.0) ## datasauRus * 0.1.4 2018-09-20 [1] CRAN (R 4.1.1) ## DBI 1.1.2 2021-12-20 [1] CRAN (R 4.1.1) ## dbplyr 2.1.1 2021-04-06 [1] CRAN (R 4.1.0) ## desc 1.4.0 2021-09-28 [1] CRAN (R 4.1.1) ## devtools 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.1.1) ## dplyr * 1.0.8 2022-02-08 [1] CRAN (R 4.1.1) ## DT 0.21 2022-02-26 [1] CRAN (R 4.1.2) ## e1071 1.7-9 2021-09-16 [1] CRAN (R 4.1.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) ## evaluate 0.15 2022-02-18 [1] CRAN (R 4.1.1) ## fansi 1.0.2 2022-01-14 [1] CRAN (R 4.1.1) ## farver 2.1.0 2021-02-28 [1] CRAN (R 4.1.0) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.1.1) ## foreign 0.8-82 2022-01-13 [1] CRAN (R 4.1.1) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.1.1) ## generics 0.1.2 2022-01-31 [1] CRAN (R 4.1.1) ## gganimate * 1.0.7 2020-10-15 [1] CRAN (R 4.1.1) ## ggplot2 * 3.3.5 2021-06-25 [1] CRAN (R 4.1.1) ## gifski 1.4.3-1 2021-05-02 [1] CRAN (R 4.1.0) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.1.1) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.1.1) ## haven 2.4.3 2021-08-04 [1] CRAN (R 4.1.1) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.1.0) ## highr 0.9 2021-04-16 [1] CRAN (R 4.1.0) ## hms 1.1.1 2021-09-26 [1] CRAN (R 4.1.1) ## htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.1) ## htmlwidgets 1.5.4 2021-09-08 [1] CRAN (R 4.1.1) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.1.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) ## jsonlite 1.8.0 2022-02-22 [1] CRAN (R 4.1.1) ## KernSmooth 2.23-20 2021-05-03 [1] CRAN (R 4.1.2) ## knitr * 1.37 2021-12-16 [1] CRAN (R 4.1.1) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.1.0) ## lattice 0.20-45 2021-09-22 [1] CRAN (R 4.1.2) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.1.1) ## lubridate 1.8.0 2021-10-07 [1] CRAN (R 4.1.1) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.1.1) ## mapproj 1.2.8 2022-01-12 [1] CRAN (R 4.1.1) ## maps 3.4.0 2021-09-25 [1] CRAN (R 4.1.1) ## maptools 1.1-2 2021-09-07 [1] CRAN (R 4.1.1) ## Matrix 1.4-0 2021-12-08 [1] CRAN (R 4.1.1) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.1.1) ## mgcv 1.8-38 2021-10-06 [1] CRAN (R 4.1.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.1.0) ## nlme 3.1-155 2022-01-13 [1] CRAN (R 4.1.1) ## patchwork * 1.1.1 2020-12-17 [1] CRAN (R 4.1.1) ## pillar 1.7.0 2022-02-01 [1] CRAN (R 4.1.1) ## pkgbuild 1.3.1 2021-12-20 [1] CRAN (R 4.1.1) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.1.0) ## pkgload 1.2.4 2021-11-30 [1] CRAN (R 4.1.1) ## plyr 1.8.6 2020-03-03 [1] CRAN (R 4.1.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.1.0) ## processx 3.5.2 2021-04-30 [1] CRAN (R 4.1.0) ## progress 1.2.2 2019-05-16 [1] CRAN (R 4.1.0) ## proxy 0.4-26 2021-06-07 [1] CRAN (R 4.1.0) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.1.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.1) ## rappdirs 0.3.3 2021-01-31 [1] CRAN (R 4.1.0) ## rcfss * 0.2.4 2022-02-17 [1] local ## RColorBrewer 1.1-2 2014-12-07 [1] CRAN (R 4.1.0) ## Rcpp 1.0.8 2022-01-13 [1] CRAN (R 4.1.1) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.1.1) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.1.0) ## remotes 2.4.2 2021-11-30 [1] CRAN (R 4.1.1) ## reprex 2.0.1 2021-08-05 [1] CRAN (R 4.1.1) ## rgdal 1.5-28 2021-12-15 [1] CRAN (R 4.1.1) ## rlang 1.0.1 2022-02-03 [1] CRAN (R 4.1.1) ## rmarkdown 2.11 2021-09-14 [1] CRAN (R 4.1.1) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.1.0) ## rvest 1.0.2 2021-10-16 [1] CRAN (R 4.1.1) ## sass 0.4.0 2021-05-12 [1] CRAN (R 4.1.0) ## scales 1.1.1 2020-05-11 [1] CRAN (R 4.1.0) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.1.1) ## sf 1.0-6 2022-02-04 [1] CRAN (R 4.1.1) ## socviz * 1.2 2020-06-10 [1] CRAN (R 4.1.0) ## sp 1.4-6 2021-11-14 [1] CRAN (R 4.1.1) ## statebins * 1.4.0 2020-07-08 [1] CRAN (R 4.1.0) ## stringi 1.7.6 2021-11-29 [1] CRAN (R 4.1.1) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.1.1) ## testthat 3.1.2 2022-01-20 [1] CRAN (R 4.1.1) ## tibble * 3.1.6 2021-11-07 [1] CRAN (R 4.1.1) ## tidycensus * 1.1.0.9000 2022-01-25 [1] Github (walkerke/tidycensus@8b8e38a) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.1.1) ## tidyselect 1.1.2 2022-02-21 [1] CRAN (R 4.1.1) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.1.0) ## tigris 1.6 2022-02-22 [1] CRAN (R 4.1.1) ## tweenr 1.0.2 2021-03-23 [1] CRAN (R 4.1.0) ## tzdb 0.2.0 2021-10-27 [1] CRAN (R 4.1.1) ## units 0.8-0 2022-02-05 [1] CRAN (R 4.1.1) ## usethis 2.1.5 2021-12-09 [1] CRAN (R 4.1.1) ## utf8 1.2.2 2021-07-24 [1] CRAN (R 4.1.0) ## uuid 1.0-3 2021-11-01 [1] CRAN (R 4.1.1) ## vctrs 0.3.8 2021-04-29 [1] CRAN (R 4.1.0) ## withr 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## xfun 0.29 2021-12-14 [1] CRAN (R 4.1.1) ## xml2 1.3.3 2021-11-30 [1] CRAN (R 4.1.1) ## yaml 2.3.5 2022-02-21 [1] CRAN (R 4.1.1) ## ## [1] /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── References "],["show-the-numbers.html", "Day 2 Showing the right numbers Learning objectives Assigned readings 2.1 Grammar of graphics (review) 2.2 Grouped data and the group aesthetic 2.3 Geoms can transform data 2.4 geom_*() functions call their default stat_*() functions behind the scenes 2.5 Histograms and kernel densities 2.6 Avoiding transformations when necessary 2.7 Cross-tabulation the awkward way 2.8 Continuous variables by group or category 2.9 Plot text directly 2.10 ggrepel::geom_text_repel() 2.11 Scales, guides, and themes 2.12 Mapping data to graphics 2.13 Amounts and proportions 2.14 Comparisons Acknowledgments Session info", " Day 2 Showing the right numbers library(tidyverse) library(gapminder) library(socviz) library(knitr) library(broom) library(forcats) library(stringr) library(ggrepel) library(here) Learning objectives 2.0.1 Morning Expand on the different types of geometric objects used by ggplot2 Demonstrate the ability of ggplot2 to calculate statistical transformations Review the grammar of graphics and ggplot2 workflow Combine ggplot2 with dplyr and data transformation prior to constructing the graph 2.0.2 Afternoon Demonstrate techniques for mapping data to graphics Introduce alternative visualizations for displaying amounts and proportions Consider advanced visualizations for comparisons Assigned readings Chapters 4-5, Healy (2018) - accessible via the book’s website 2.1 Grammar of graphics (review) The grammar of graphics is a set of rules for how to produce graphics from data, taking pieces of data and mapping them to geometric objects (like points and lines) that have aesthetic attributes (like position, color, and size), together with further rules for transforming the data if needed, adjusting scales, or projecting the results onto a coordinate system. 2.2 Grouped data and the group aesthetic p &lt;- ggplot( data = gapminder, mapping = aes( x = year, y = gdpPercap ) ) p + geom_line() This doesn’t look right. It is trying to draw a single line for all the observations. But look at the structure of gapminder: gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows It is one-row-per-country-per-year. We use group to identify the column that tells us this structure so we draw one line per country. p &lt;- ggplot(data = gapminder, mapping = aes( x = year, y = gdpPercap )) p + geom_line(mapping = aes(group = country)) p &lt;- ggplot( data = gapminder, mapping = aes( x = year, y = gdpPercap ) ) p + geom_line( mapping = aes(group = country) ) + facet_wrap(~continent) A facet is not a geom. It is a way of arranging geoms. Facet’s use R’s formula syntax. Read the ~ as “on” or “by”. p + geom_line( color = &quot;gray70&quot;, mapping = aes(group = country) ) + geom_smooth( size = 1.1, method = &quot;loess&quot;, se = FALSE ) + scale_y_log10(labels = scales::dollar) + facet_wrap(~continent, ncol = 5) + labs( x = &quot;Year&quot;, y = &quot;GDP per capita&quot;, title = &quot;GDP per capita on Five Continents&quot; ) The labs() function lets you name labels, title, subtitle, etc. 2.3 Geoms can transform data gss_sm ## # A tibble: 2,867 × 32 ## year id ballot age childs sibs degree race sex region income16 ## &lt;dbl&gt; &lt;dbl&gt; &lt;labelled&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;labe&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 2016 1 1 47 3 2 Bache… White Male New E… $170000… ## 2 2016 2 2 61 0 3 High … White Male New E… $50000 … ## 3 2016 3 3 72 2 3 Bache… White Male New E… $75000 … ## 4 2016 4 1 43 4 3 High … White Fema… New E… $170000… ## 5 2016 5 3 55 2 2 Gradu… White Fema… New E… $170000… ## 6 2016 6 2 53 2 2 Junio… White Fema… New E… $60000 … ## 7 2016 7 1 50 2 2 High … White Male New E… $170000… ## 8 2016 8 3 23 3 6 High … Other Fema… Middl… $30000 … ## 9 2016 9 1 45 3 5 High … Black Male Middl… $60000 … ## 10 2016 10 3 71 4 1 Junio… White Male Middl… $60000 … ## # … with 2,857 more rows, and 21 more variables: relig &lt;fct&gt;, marital &lt;fct&gt;, ## # padeg &lt;fct&gt;, madeg &lt;fct&gt;, partyid &lt;fct&gt;, polviews &lt;fct&gt;, happy &lt;fct&gt;, ## # partners &lt;fct&gt;, grass &lt;fct&gt;, zodiac &lt;fct&gt;, pres12 &lt;labelled&gt;, ## # wtssall &lt;dbl&gt;, income_rc &lt;fct&gt;, agegrp &lt;fct&gt;, ageq &lt;fct&gt;, siblings &lt;fct&gt;, ## # kids &lt;fct&gt;, religion &lt;fct&gt;, bigregion &lt;fct&gt;, partners_rc &lt;fct&gt;, obama &lt;dbl&gt; gss_sm contains a subset of General Social Survey questions from 2016. count(x = gss_sm, religion) ## # A tibble: 6 × 2 ## religion n ## &lt;fct&gt; &lt;int&gt; ## 1 Protestant 1371 ## 2 Catholic 649 ## 3 Jewish 51 ## 4 None 619 ## 5 Other 159 ## 6 &lt;NA&gt; 18 p &lt;- ggplot( data = gss_sm, mapping = aes(x = bigregion) ) p + geom_bar() The y-axis variable count is not in the data. Instead, ggplot has calculated it for us. It does this by using the default stat_*() function associated with geom_bar(), stat_count(). This function can compute two new variables, count and prop (short for proportion). The count statistic is the default one used. p &lt;- ggplot( data = gss_sm, mapping = aes(x = bigregion) ) p + geom_bar(mapping = aes(y = stat(prop))) p &lt;- ggplot( data = gss_sm, mapping = aes(x = bigregion) ) p + geom_bar(mapping = aes(y = stat(prop), group = 1)) By default stat(prop) is calculated within each bar. If you want to calculate the proportion of all the rows in the data frame, add group = 1. 2.4 geom_*() functions call their default stat_*() functions behind the scenes p + geom_bar() p + stat_count() 2.4.1 Color in a bar chart If you want to use color as an aesthetic to communicate additional information in a bar chart, pass it using the fill aesthetic. color defines the border of the bar. p &lt;- ggplot( data = gss_sm, mapping = aes(x = religion) ) p + geom_bar() p &lt;- ggplot( data = gss_sm, mapping = aes(x = religion, color = religion) ) p + geom_bar() p &lt;- ggplot( data = gss_sm, mapping = aes(x = religion, fill = religion) ) p + geom_bar() p &lt;- ggplot( data = gss_sm, mapping = aes(x = religion, fill = religion) ) p + geom_bar() + guides(fill = FALSE) 2.5 Histograms and kernel densities midwest ## # A tibble: 437 × 28 ## PID county state area poptotal popdensity popwhite popblack popamerindian ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 561 ADAMS IL 0.052 66090 1271. 63917 1702 98 ## 2 562 ALEXAN… IL 0.014 10626 759 7054 3496 19 ## 3 563 BOND IL 0.022 14991 681. 14477 429 35 ## 4 564 BOONE IL 0.017 30806 1812. 29344 127 46 ## 5 565 BROWN IL 0.018 5836 324. 5264 547 14 ## 6 566 BUREAU IL 0.05 35688 714. 35157 50 65 ## 7 567 CALHOUN IL 0.017 5322 313. 5298 1 8 ## 8 568 CARROLL IL 0.027 16805 622. 16519 111 30 ## 9 569 CASS IL 0.024 13437 560. 13384 16 8 ## 10 570 CHAMPA… IL 0.058 173025 2983. 146506 16559 331 ## # … with 427 more rows, and 19 more variables: popasian &lt;int&gt;, popother &lt;int&gt;, ## # percwhite &lt;dbl&gt;, percblack &lt;dbl&gt;, percamerindan &lt;dbl&gt;, percasian &lt;dbl&gt;, ## # percother &lt;dbl&gt;, popadults &lt;int&gt;, perchsd &lt;dbl&gt;, percollege &lt;dbl&gt;, ## # percprof &lt;dbl&gt;, poppovertyknown &lt;int&gt;, percpovertyknown &lt;dbl&gt;, ## # percbelowpoverty &lt;dbl&gt;, percchildbelowpovert &lt;dbl&gt;, percadultpoverty &lt;dbl&gt;, ## # percelderlypoverty &lt;dbl&gt;, inmetro &lt;int&gt;, category &lt;chr&gt; midwest contains county-level census data for Midwestern states. p &lt;- ggplot( data = midwest, mapping = aes(x = area) ) p + geom_histogram() The default stat for this geom has to make a choice, so the message is letting us know we might want to override it. p &lt;- ggplot( data = midwest, mapping = aes(x = area) ) p + geom_histogram(bins = 10) 2.5.1 Subsetting data on the fly oh_wi &lt;- c(&quot;OH&quot;, &quot;WI&quot;) p &lt;- ggplot( data = filter(midwest, state %in% oh_wi), mapping = aes(x = percollege, fill = state) ) p + geom_histogram( position = &quot;identity&quot;, alpha = 0.4, bins = 20 ) While this can be done, it is somewhat challenging to read intuitively. You must use alpha to incorporate transparency otherwise all hope at interpreting is lost. Alternatively, you could use a continuous counterpart, geom_density(). p &lt;- ggplot( data = midwest, mapping = aes(x = area) ) p + geom_density() p &lt;- ggplot( data = midwest, mapping = aes( x = area, fill = state, color = state ) ) p + geom_density(alpha = 0.3) 2.6 Avoiding transformations when necessary Sometimes no transformation is necessary. Consider the titanic dataset titanic ## fate sex n percent ## 1 perished male 1364 62.0 ## 2 perished female 126 5.7 ## 3 survived male 367 16.7 ## 4 survived female 344 15.6 Here the data has already been summarized. What if we want to make a bar chart? p &lt;- ggplot( data = titanic, mapping = aes( x = fate, y = percent, fill = sex ) ) p + geom_bar( stat = &quot;identity&quot;, position = &quot;dodge&quot; ) + theme(legend.position = &quot;top&quot;) Even more conveniently, use geom_col() p &lt;- ggplot(data = titanic, mapping = aes( x = fate, y = percent, fill = sex )) p + geom_col(position = &quot;dodge&quot;) + theme(legend.position = &quot;top&quot;) oecd_sum ## # A tibble: 57 × 5 ## # Groups: year [57] ## year other usa diff hi_lo ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1960 68.6 69.9 1.30 Below ## 2 1961 69.2 70.4 1.20 Below ## 3 1962 68.9 70.2 1.30 Below ## 4 1963 69.1 70 0.900 Below ## 5 1964 69.5 70.3 0.800 Below ## 6 1965 69.6 70.3 0.700 Below ## 7 1966 69.9 70.3 0.400 Below ## 8 1967 70.1 70.7 0.600 Below ## 9 1968 70.1 70.4 0.300 Below ## 10 1969 70.1 70.6 0.5 Below ## # … with 47 more rows p &lt;- ggplot( data = oecd_sum, mapping = aes(x = year, y = diff, fill = hi_lo) ) p + geom_col() + guides(fill = FALSE) + labs( x = NULL, y = &quot;Difference in Years&quot;, title = &quot;The US Life Expectancy Gap&quot;, subtitle = &quot;Difference between US and OECD average life expectancies, 1960-2015&quot;, caption = &quot;Data: OECD. After a chart by Christopher Ingraham, Washington Post, December 27th 2017.&quot; ) 2.7 Cross-tabulation the awkward way More commonly, we would add color to a bar graph to cross-classify two categorical variables. This is the graphical equivalent of a frequency table. We can do this directly within ggplot(), however it is also more convoluted. Consider examining religious preference by census region. ggplot( data = gss_sm, mapping = aes( x = bigregion, fill = religion ) ) + geom_bar() By default we get a stacked bar chart. If we want to make comparisons easier, we could convert this to a proportional bar chart. ggplot( data = gss_sm, mapping = aes( x = bigregion, fill = religion ) ) + geom_bar(position = &quot;fill&quot;) Now all the bars are the same height, but we lost the ability to see the relative size of each region with respect to the overall total. What if we wanted to show the proportion of religions within regions of the country, but instead of stacking the bars we want separate bars? The first attempt may use position = \"dodge\". ggplot( data = gss_sm, mapping = aes( x = bigregion, fill = religion ) ) + geom_bar(position = &quot;dodge&quot;) Good structure, but we’re back to counts. Let’s directly map the stat(prop) variable to the y aesthetic as well to preserve the proportion on the y-axis. ggplot( data = gss_sm, mapping = aes( x = bigregion, fill = religion ) ) + geom_bar(mapping = aes(y = stat(prop)), position = &quot;dodge&quot;) Still not correct. Same problem as before. Each individual bar sums to 1. If we want overall proportions for a single variable, we mapped group = 1. What if we do that here but with respect to religion? ggplot( data = gss_sm, mapping = aes( x = bigregion, fill = religion ) ) + geom_bar(mapping = aes( y = stat(prop), group = religion ), position = &quot;dodge&quot;) Looks better, but we still have a problem. Bars within a single region do not sum to 1. Instead, bars for any particular religion sum to 1. ggplot( data = gss_sm, mapping = aes(x = religion) ) + geom_bar(mapping = aes( y = stat(prop), group = bigregion ), position = &quot;dodge&quot;) + facet_wrap(~bigregion) The easiest approach is to use facet_wrap() and not force geom_bar() and stat_count() to do all the work in a single step. Instead, we can ask ggplot() to give us a proportional bar chart of religious affiliation, and then facet that by region. The proportions are calculated within each panel, which is the breakdown we wanted. This has the added advantage of not producing too many bars within each category. 2.7.1 Calculate manually Rather than doing all the summarizing in ggplot(), we could instead calculate the frequencies and proportions manually using dplyr functions first, then use the summarized data frame as the basis for the bar graph. glimpse(gss_sm) ## Rows: 2,867 ## Columns: 32 ## $ year &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016… ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ ballot &lt;labelled&gt; 1, 2, 3, 1, 3, 2, 1, 3, 1, 3, 2, 1, 2, 3, 2, 3, 3, 2,… ## $ age &lt;dbl&gt; 47, 61, 72, 43, 55, 53, 50, 23, 45, 71, 33, 86, 32, 60, 76… ## $ childs &lt;dbl&gt; 3, 0, 2, 4, 2, 2, 2, 3, 3, 4, 5, 4, 3, 5, 7, 2, 6, 5, 0, 2… ## $ sibs &lt;labelled&gt; 2, 3, 3, 3, 2, 2, 2, 6, 5, 1, 4, 4, 3, 6, 0, 1, 3, 8,… ## $ degree &lt;fct&gt; Bachelor, High School, Bachelor, High School, Graduate, Ju… ## $ race &lt;fct&gt; White, White, White, White, White, White, White, Other, Bl… ## $ sex &lt;fct&gt; Male, Male, Male, Female, Female, Female, Male, Female, Ma… ## $ region &lt;fct&gt; New England, New England, New England, New England, New En… ## $ income16 &lt;fct&gt; $170000 or over, $50000 to 59999, $75000 to $89999, $17000… ## $ relig &lt;fct&gt; None, None, Catholic, Catholic, None, None, None, Catholic… ## $ marital &lt;fct&gt; Married, Never Married, Married, Married, Married, Married… ## $ padeg &lt;fct&gt; Graduate, Lt High School, High School, NA, Bachelor, NA, H… ## $ madeg &lt;fct&gt; High School, High School, Lt High School, High School, Hig… ## $ partyid &lt;fct&gt; &quot;Independent&quot;, &quot;Ind,near Dem&quot;, &quot;Not Str Republican&quot;, &quot;Not … ## $ polviews &lt;fct&gt; Moderate, Liberal, Conservative, Moderate, Slightly Libera… ## $ happy &lt;fct&gt; Pretty Happy, Pretty Happy, Very Happy, Pretty Happy, Very… ## $ partners &lt;fct&gt; NA, &quot;1 Partner&quot;, &quot;1 Partner&quot;, NA, &quot;1 Partner&quot;, &quot;1 Partner&quot;… ## $ grass &lt;fct&gt; NA, Legal, Not Legal, NA, Legal, Legal, NA, Not Legal, NA,… ## $ zodiac &lt;fct&gt; Aquarius, Scorpio, Pisces, Cancer, Scorpio, Scorpio, Capri… ## $ pres12 &lt;labelled&gt; 3, 1, 2, 2, 1, 1, NA, NA, NA, 2, NA, NA, 1, 1, 2, 1, … ## $ wtssall &lt;dbl&gt; 0.957, 0.478, 0.957, 1.914, 1.435, 0.957, 1.435, 0.957, 0.… ## $ income_rc &lt;fct&gt; Gt $170000, Gt $50000, Gt $75000, Gt $170000, Gt $170000, … ## $ agegrp &lt;fct&gt; Age 45-55, Age 55-65, Age 65+, Age 35-45, Age 45-55, Age 4… ## $ ageq &lt;fct&gt; Age 34-49, Age 49-62, Age 62+, Age 34-49, Age 49-62, Age 4… ## $ siblings &lt;fct&gt; 2, 3, 3, 3, 2, 2, 2, 6+, 5, 1, 4, 4, 3, 6+, 0, 1, 3, 6+, 2… ## $ kids &lt;fct&gt; 3, 0, 2, 4+, 2, 2, 2, 3, 3, 4+, 4+, 4+, 3, 4+, 4+, 2, 4+, … ## $ religion &lt;fct&gt; None, None, Catholic, Catholic, None, None, None, Catholic… ## $ bigregion &lt;fct&gt; Northeast, Northeast, Northeast, Northeast, Northeast, Nor… ## $ partners_rc &lt;fct&gt; NA, 1, 1, NA, 1, 1, NA, 1, NA, 3, 1, NA, 1, NA, 0, 1, 0, N… ## $ obama &lt;dbl&gt; 0, 1, 0, 0, 1, 1, NA, NA, NA, 0, NA, NA, 1, 1, 0, 1, 0, 1,… (rel_by_region &lt;- gss_sm %&gt;% count(bigregion, religion) %&gt;% mutate( freq = n / sum(n), pct = round((freq * 100), 0) ) %&gt;% drop_na()) ## # A tibble: 20 × 5 ## bigregion religion n freq pct ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Northeast Protestant 158 0.0551 6 ## 2 Northeast Catholic 162 0.0565 6 ## 3 Northeast Jewish 27 0.00942 1 ## 4 Northeast None 112 0.0391 4 ## 5 Northeast Other 28 0.00977 1 ## 6 Midwest Protestant 325 0.113 11 ## 7 Midwest Catholic 172 0.0600 6 ## 8 Midwest Jewish 3 0.00105 0 ## 9 Midwest None 157 0.0548 5 ## 10 Midwest Other 33 0.0115 1 ## 11 South Protestant 650 0.227 23 ## 12 South Catholic 160 0.0558 6 ## 13 South Jewish 11 0.00384 0 ## 14 South None 170 0.0593 6 ## 15 South Other 50 0.0174 2 ## 16 West Protestant 238 0.0830 8 ## 17 West Catholic 155 0.0541 5 ## 18 West Jewish 10 0.00349 0 ## 19 West None 180 0.0628 6 ## 20 West Other 48 0.0167 2 Now this is easy to pass into ggplot() and draw the bar graph. ggplot( data = rel_by_region, mapping = aes( x = bigregion, y = pct, fill = religion ) ) + geom_col(position = &quot;dodge2&quot;) + labs(x = &quot;Region&quot;, y = &quot;Percent&quot;, fill = &quot;Religion&quot;) + theme(legend.position = &quot;top&quot;) Instead of using geom_bar(), we use geom_col() because we already summarized the data - we want stat_identity(), not stat_count(). While this figure works, it is not the best we can do. It is generally crowded. Instead, let’s convert it to a faceted plot: ggplot( data = rel_by_region, mapping = aes( x = religion, y = pct, fill = religion ) ) + geom_col(position = &quot;dodge2&quot;) + labs(x = &quot;Region&quot;, y = &quot;Percent&quot;, fill = &quot;Religion&quot;) + guides(fill = FALSE) + coord_flip() + facet_grid(~bigregion) 2.8 Continuous variables by group or category glimpse(organdata) ## Rows: 238 ## Columns: 21 ## $ country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;… ## $ year &lt;date&gt; NA, 1991-01-01, 1992-01-01, 1993-01-01, 1994-01-01, … ## $ donors &lt;dbl&gt; NA, 12.09, 12.35, 12.51, 10.25, 10.18, 10.59, 10.26, … ## $ pop &lt;int&gt; 17065, 17284, 17495, 17667, 17855, 18072, 18311, 1851… ## $ pop_dens &lt;dbl&gt; 0.220, 0.223, 0.226, 0.228, 0.231, 0.233, 0.237, 0.23… ## $ gdp &lt;int&gt; 16774, 17171, 17914, 18883, 19849, 21079, 21923, 2296… ## $ gdp_lag &lt;int&gt; 16591, 16774, 17171, 17914, 18883, 19849, 21079, 2192… ## $ health &lt;dbl&gt; 1300, 1379, 1455, 1540, 1626, 1737, 1846, 1948, 2077,… ## $ health_lag &lt;dbl&gt; 1224, 1300, 1379, 1455, 1540, 1626, 1737, 1846, 1948,… ## $ pubhealth &lt;dbl&gt; 4.8, 5.4, 5.4, 5.4, 5.4, 5.5, 5.6, 5.7, 5.9, 6.1, 6.2… ## $ roads &lt;dbl&gt; 136.6, 122.3, 112.8, 110.5, 108.0, 111.6, 107.6, 95.4… ## $ cerebvas &lt;int&gt; 682, 647, 630, 611, 631, 592, 576, 525, 516, 493, 474… ## $ assault &lt;int&gt; 21, 19, 17, 18, 17, 16, 17, 17, 16, 15, 16, 15, 14, N… ## $ external &lt;int&gt; 444, 425, 406, 376, 387, 371, 395, 385, 410, 409, 393… ## $ txp_pop &lt;dbl&gt; 0.938, 0.926, 0.915, 0.906, 0.896, 0.885, 0.874, 0.86… ## $ world &lt;chr&gt; &quot;Liberal&quot;, &quot;Liberal&quot;, &quot;Liberal&quot;, &quot;Liberal&quot;, &quot;Liberal&quot;… ## $ opt &lt;chr&gt; &quot;In&quot;, &quot;In&quot;, &quot;In&quot;, &quot;In&quot;, &quot;In&quot;, &quot;In&quot;, &quot;In&quot;, &quot;In&quot;, &quot;In&quot;,… ## $ consent_law &lt;chr&gt; &quot;Informed&quot;, &quot;Informed&quot;, &quot;Informed&quot;, &quot;Informed&quot;, &quot;Info… ## $ consent_practice &lt;chr&gt; &quot;Informed&quot;, &quot;Informed&quot;, &quot;Informed&quot;, &quot;Informed&quot;, &quot;Info… ## $ consistent &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes… ## $ ccode &lt;chr&gt; &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;, &quot;Oz&quot;,… 2.8.1 Boxplots ggplot( data = organdata, mapping = aes(x = country, y = donors) ) + geom_boxplot() Awkward to have country labels on x-axis. Switch to y-axis. 2.8.2 coord_flip() ggplot( data = organdata, mapping = aes(x = country, y = donors) ) + geom_boxplot() + coord_flip() Explicit use of a coordinate transformation system. 2.8.3 reorder() ggplot( data = organdata, mapping = aes( x = reorder(country, donors, na.rm = TRUE), y = donors ) ) + geom_boxplot() + labs(x = NULL) + coord_flip() Place on a more meaningful order. Add color aesthetic. ggplot( data = organdata, mapping = aes( x = reorder(country, donors, na.rm = TRUE), y = donors, fill = world ) ) + geom_boxplot() + labs(x = NULL) + coord_flip() + theme(legend.position = &quot;bottom&quot;) 2.8.4 Strip chart ggplot( data = organdata, mapping = aes( x = reorder(country, donors, na.rm = TRUE), y = donors, color = world ) ) + geom_point() + labs(x = NULL) + coord_flip() + theme(legend.position = &quot;bottom&quot;) Hard to see all the points. Add jitter. ggplot( data = organdata, mapping = aes( x = reorder(country, donors, na.rm = TRUE), y = donors, color = world ) ) + geom_jitter() + labs(x = NULL) + coord_flip() + theme(legend.position = &quot;bottom&quot;) 2.8.5 Cleveland dotplot 2.8.5.1 Calculate summary statistics (by_country &lt;- organdata %&gt;% group_by(consent_law, country) %&gt;% summarize( donors_mean = mean(donors, na.rm = TRUE), donors_sd = sd(donors, na.rm = TRUE), gdp_mean = mean(gdp, na.rm = TRUE), health_mean = mean(health, na.rm = TRUE), roads_mean = mean(roads, na.rm = TRUE), cerebvas_mean = mean(cerebvas, na.rm = TRUE) )) ## # A tibble: 17 × 8 ## # Groups: consent_law [2] ## consent_law country donors_mean donors_sd gdp_mean health_mean roads_mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Informed Australia 10.6 1.14 22179. 1958. 105. ## 2 Informed Canada 14.0 0.751 23711. 2272. 109. ## 3 Informed Denmark 13.1 1.47 23722. 2054. 102. ## 4 Informed Germany 13.0 0.611 22163. 2349. 113. ## 5 Informed Ireland 19.8 2.48 20824. 1480. 118. ## 6 Informed Netherlands 13.7 1.55 23013. 1993. 76.1 ## 7 Informed United Kin… 13.5 0.775 21359. 1561. 67.9 ## 8 Informed United Sta… 20.0 1.33 29212. 3988. 155. ## 9 Presumed Austria 23.5 2.42 23876. 1875. 150. ## 10 Presumed Belgium 21.9 1.94 22500. 1958. 155. ## 11 Presumed Finland 18.4 1.53 21019. 1615. 93.6 ## 12 Presumed France 16.8 1.60 22603. 2160. 156. ## 13 Presumed Italy 11.1 4.28 21554. 1757 122. ## 14 Presumed Norway 15.4 1.11 26448. 2217. 70.0 ## 15 Presumed Spain 28.1 4.96 16933 1289. 161. ## 16 Presumed Sweden 13.1 1.75 22415. 1951. 72.3 ## 17 Presumed Switzerland 14.2 1.71 27233 2776. 96.4 ## # … with 1 more variable: cerebvas_mean &lt;dbl&gt; Better approach using efficient code. (by_country &lt;- organdata %&gt;% group_by(consent_law, country) %&gt;% summarize(across(where(is.numeric), list(mean = mean, sd = sd), na.rm = TRUE)) %&gt;% ungroup()) ## # A tibble: 17 × 28 ## consent_law country donors_mean donors_sd pop_mean pop_sd pop_dens_mean ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Informed Australia 10.6 1.14 18318. 8.31e2 0.237 ## 2 Informed Canada 14.0 0.751 29608. 1.19e3 0.297 ## 3 Informed Denmark 13.1 1.47 5257. 8.06e1 12.2 ## 4 Informed Germany 13.0 0.611 80255. 5.16e3 22.5 ## 5 Informed Ireland 19.8 2.48 3674. 1.32e2 5.23 ## 6 Informed Netherlands 13.7 1.55 15548. 3.73e2 37.4 ## 7 Informed United Kingd… 13.5 0.775 58187. 6.26e2 24.0 ## 8 Informed United States 20.0 1.33 269330. 1.25e4 2.80 ## 9 Presumed Austria 23.5 2.42 7927. 1.09e2 9.45 ## 10 Presumed Belgium 21.9 1.94 10153. 1.09e2 30.7 ## 11 Presumed Finland 18.4 1.53 5112. 6.86e1 1.51 ## 12 Presumed France 16.8 1.60 58056. 8.51e2 10.5 ## 13 Presumed Italy 11.1 4.28 57360. 4.25e2 19.0 ## 14 Presumed Norway 15.4 1.11 4386. 9.73e1 1.35 ## 15 Presumed Spain 28.1 4.96 39666. 9.51e2 7.84 ## 16 Presumed Sweden 13.1 1.75 8789. 1.14e2 1.95 ## 17 Presumed Switzerland 14.2 1.71 7037. 1.70e2 17.0 ## # … with 21 more variables: pop_dens_sd &lt;dbl&gt;, gdp_mean &lt;dbl&gt;, gdp_sd &lt;dbl&gt;, ## # gdp_lag_mean &lt;dbl&gt;, gdp_lag_sd &lt;dbl&gt;, health_mean &lt;dbl&gt;, health_sd &lt;dbl&gt;, ## # health_lag_mean &lt;dbl&gt;, health_lag_sd &lt;dbl&gt;, pubhealth_mean &lt;dbl&gt;, ## # pubhealth_sd &lt;dbl&gt;, roads_mean &lt;dbl&gt;, roads_sd &lt;dbl&gt;, cerebvas_mean &lt;dbl&gt;, ## # cerebvas_sd &lt;dbl&gt;, assault_mean &lt;dbl&gt;, assault_sd &lt;dbl&gt;, ## # external_mean &lt;dbl&gt;, external_sd &lt;dbl&gt;, txp_pop_mean &lt;dbl&gt;, ## # txp_pop_sd &lt;dbl&gt; 2.8.5.2 Draw the plot ggplot( data = by_country, mapping = aes( x = donors_mean, y = reorder(country, donors_mean), color = consent_law ) ) + geom_point(size = 3) + labs( x = &quot;Donor Procurement Rate&quot;, y = &quot;&quot;, color = &quot;Consent Law&quot; ) + theme(legend.position = &quot;top&quot;) 2.8.5.3 Use facet instead of color ggplot( data = by_country, mapping = aes( x = donors_mean, y = reorder(country, donors_mean) ) ) + geom_point(size = 3) + facet_wrap(~consent_law, ncol = 1) + labs( x = &quot;Donor Procurement Rate&quot;, y = &quot;&quot;, color = &quot;Consent Law&quot; ) ggplot( data = by_country, mapping = aes( x = donors_mean, y = reorder(country, donors_mean) ) ) + geom_point(size = 3) + facet_wrap(~consent_law, scales = &quot;free_y&quot;, ncol = 1) + labs( x = &quot;Donor Procurement Rate&quot;, y = &quot;&quot;, color = &quot;Consent Law&quot; ) Allow the \\(y\\)-axis to vary and only include matching countries. 2.8.5.4 Add standard deviation ggplot( data = by_country, mapping = aes( x = reorder(country, donors_mean), y = donors_mean ) ) + geom_pointrange(mapping = aes( ymin = donors_mean - donors_sd, ymax = donors_mean + donors_sd )) + labs( x = &quot;&quot;, y = &quot;Donor Procurement Rate&quot; ) + coord_flip() 2.9 Plot text directly 2.9.1 geom_text() ggplot( data = by_country, mapping = aes( x = roads_mean, y = donors_mean ) ) + geom_point() + geom_text(mapping = aes(label = country)) ggplot( data = by_country, mapping = aes( x = roads_mean, y = donors_mean ) ) + geom_point() + geom_text(mapping = aes(label = country), hjust = 0) 2.10 ggrepel::geom_text_repel() elections_historic %&gt;% select(2:7) ## # A tibble: 49 × 6 ## year winner win_party ec_pct popular_pct popular_margin ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1824 John Quincy Adams D.-R. 0.322 0.309 -0.104 ## 2 1828 Andrew Jackson Dem. 0.682 0.559 0.122 ## 3 1832 Andrew Jackson Dem. 0.766 0.547 0.178 ## 4 1836 Martin Van Buren Dem. 0.578 0.508 0.142 ## 5 1840 William Henry Harrison Whig 0.796 0.529 0.0605 ## 6 1844 James Polk Dem. 0.618 0.495 0.0145 ## 7 1848 Zachary Taylor Whig 0.562 0.473 0.0479 ## 8 1852 Franklin Pierce Dem. 0.858 0.508 0.0695 ## 9 1856 James Buchanan Dem. 0.588 0.453 0.122 ## 10 1860 Abraham Lincoln Rep. 0.594 0.396 0.101 ## # … with 39 more rows p_title &lt;- &quot;Presidential Elections: Popular &amp; Electoral College Margins&quot; p_subtitle &lt;- &quot;1824-2016&quot; p_caption &lt;- &quot;Data for 2016 are provisional.&quot; x_label &lt;- &quot;Winner&#39;s share of Popular Vote&quot; y_label &lt;- &quot;Winner&#39;s share of Electoral College Votes&quot; library(ggrepel) ggplot(data = elections_historic, mapping = aes( x = popular_pct, y = ec_pct, label = winner_label )) + geom_hline(yintercept = 0.5, size = 1.4, color = &quot;gray80&quot;) + geom_vline(xintercept = 0.5, size = 1.4, color = &quot;gray80&quot;) + geom_point() + geom_text_repel() + scale_x_continuous(labels = scales::percent) + scale_y_continuous(labels = scales::percent) + labs( x = x_label, y = y_label, title = p_title, subtitle = p_subtitle, caption = p_caption ) 2.10.1 Label outliers only ggplot( data = by_country, mapping = aes(x = gdp_mean, y = health_mean) ) + geom_point() + geom_text_repel( data = filter(by_country, gdp_mean &gt; 25000), mapping = aes(label = country) ) ggplot( data = by_country, mapping = aes(x = gdp_mean, y = health_mean) ) + geom_point() + geom_text_repel( data = filter( by_country, gdp_mean &gt; 25000 | health_mean &lt; 1500 | country %in% &quot;Belgium&quot; ), mapping = aes(label = country) ) 2.11 Scales, guides, and themes p &lt;- ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent, fill = continent ) ) p + geom_point() + geom_smooth(method = &quot;loess&quot;) + scale_x_log10() Scale functions control scale mappings in geoms. Remember: not just x and y but also color, fill, shape, and size are scales. They visually represent quantities or categories in your data – thus, they have a scale associated with that representation. This means you control things like color schemes for data mappings through scale functions. scale_&lt;MAPPING&gt;_&lt;KIND&gt;() scale_x_continuous() scale_y_continuous() scale_x_discrete() scale_y_discrete() scale_x_log10() scale_x_sqrt() 2.11.1 Labels, breaks, and limits p &lt;- ggplot( data = organdata, mapping = aes( x = roads, y = donors, color = world ) ) p + geom_point() + scale_x_log10() + scale_y_continuous( breaks = c(5, 15, 25), labels = c(&quot;Five&quot;, &quot;Fifteen&quot;, &quot;Twenty Five&quot;) ) p &lt;- ggplot( data = organdata, mapping = aes( x = roads, y = donors, color = world ) ) p + geom_point() + scale_color_discrete( labels = c( &quot;Corporatist&quot;, &quot;Liberal&quot;, &quot;Social Democratic&quot;, &quot;Unclassified&quot; ) ) + labs( x = &quot;Road Deaths&quot;, y = &quot;Donor Procurement&quot;, color = &quot;Welfare State&quot; ) p &lt;- ggplot(data = organdata, mapping = aes( x = roads, y = donors, color = world )) p + geom_point() + labs( x = &quot;Road Deaths&quot;, y = &quot;Donor Procurement&quot; ) + guides(color = FALSE) 2.12 Mapping data to graphics Download the necessary data files for the following coding exercises using usethis::use_course(\"css-data-mining-viz/show-the-numbers\"). For this example, I’m going to use real world data to demonstrate the typical process for loading data, cleaning it up a bit, and mapping specific columns of the data onto the parts of a graph using the grammar of graphics and ggplot(). The data I’ll use comes from the BBC’s corporate charity, BBC Children in Need, which makes grants to smaller UK nonprofit organizations that work on issues related to childhood poverty. An organization in the UK named 360Giving helps nonprofits and foundations publish data about their grant giving activities in an open and standardized way, and (as of May 2020) they list data from 126 different charities, including BBC Children in Need. If you want to follow along with this example (highly recommended!), you can download the data directly from the website. 2.12.1 Load and clean data First, we need to load a few libraries: tidyverse (as always), along with readxl for reading Excel files and lubridate for working with dates: # Load libraries library(tidyverse) # For ggplot, dplyr, and friends library(readxl) # For reading Excel files library(lubridate) # For working with dates We’ll then load the original Excel file. I placed this file in a folder named data in my RStudio Project folder for this example. It’s also good practice to keep a pristine, untouched copy of your data. # Load the original Excel file bbc_raw &lt;- read_excel(&quot;data/360-giving-data.xlsx&quot;) There may be some errors reading the file – you can ignore those in this case. Next we’ll add a couple columns and clean up the data a little. We’ll extract the year from the Award Date column, rename some of the longer-named columns, and make a new column that shows the duration of grants. We’ll also get rid of 2015 since there are so few observations then. Note the strange use of `s around column names like `Award Date`. This is because R technically doesn’t allow special characters like spaces in column names. If there are spaces, you have to wrap the column names in backticks. Because typing backticks all the time gets tedious, we’ll use rename() to rename some of the columns: bbc &lt;- bbc_raw %&gt;% # Extract the year from the award date mutate(grant_year = year(`Award Date`)) %&gt;% # Rename some columns rename(grant_amount = `Amount Awarded`, grant_program = `Grant Programme:Title`, grant_duration = `Planned Dates:Duration (months)`) %&gt;% # Make a new text-based version of the duration column, recoding months # between 12-23, 23-35, and 36+. The case_when() function here lets us use # multiple if/else conditions at the same time. mutate(grant_duration_text = case_when( grant_duration &gt;= 12 &amp; grant_duration &lt; 24 ~ &quot;1 year&quot;, grant_duration &gt;= 24 &amp; grant_duration &lt; 36 ~ &quot;2 years&quot;, grant_duration &gt;= 36 ~ &quot;3 years&quot; )) %&gt;% # Get rid of anything before 2016 filter(grant_year &gt; 2015) %&gt;% # Make a categorical version of the year column mutate(grant_year_category = factor(grant_year)) 2.12.2 Histograms First let’s look at the distribution of grant amounts with a histogram. Map grant_amount to the x-axis and don’t map anything to the y-axis, since geom_histogram() will calculate the y-axis values for us: ggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram() Notice that ggplot warns you about bin widths. By default it will divide the data into 30 equally spaced bins, which will most likely not be the best for your data. You should always set your own bin width to something more appropriate. There are no rules for correct bin widths. Just don’t have them be too wide: ggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 100000) Or too small: ggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 500) £10,000 seems to fit well. It’s often helpful to add a white border to the histogram bars, too: ggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 10000, color = &quot;white&quot;) We can map other variables onto the plot, like mapping grant_year_category to the fill aesthetic: ggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) + geom_histogram(binwidth = 10000, color = &quot;white&quot;) That gets really hard to interpret though, so we can facet by year with facet_wrap(): ggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) + geom_histogram(binwidth = 10000, color = &quot;white&quot;) + facet_wrap(vars(grant_year)) Neat! 2.12.3 Points Next let’s look at the data using points, mapping year to the x-axis and grant amount to the y-axis: ggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point() We have some serious overplotting here, with dots so thick that it looks like lines. We can fix this a couple different ways. First, we can make the points semi-transparent using alpha, which ranges from 0 (completely invisible) to 1 (completely solid). ggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(alpha = 0.1) We can also randomly space the points to spread them out using position_jitter(): ggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(position = position_jitter()) One issue with this, though, is that the points are jittered along the x-axis (which is fine, since they’re all within the same year) and the y-axis (which is bad, since the amounts are actual numbers). We can tell ggplot to only jitter in one direction by specifying the height argument—we don’t want any up-and-down jittering: ggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(position = position_jitter(height = 0)) There are some weird clusters around £30,000 and below. Let’s map grant_program to the color aesthetic, which has two categories—regular grants and small grants—and see if that helps explain why: ggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) + geom_point(position = position_jitter(height = 0)) It does! We appear to have two different distributions of grants: small grants have a limit of £30,000, while regular grants have a much higher average amount. 2.12.4 Boxplots We can add summary information to the plot by only changing the geom we’re using. Switch from geom_point() to geom_boxplot(): ggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) + geom_boxplot() 2.12.5 Summaries We can also make smaller summarized datasets with dplyr functions like group_by() and summarize() and plot those. First let’s look at grant totals, averages, and counts over time: bbc_by_year &lt;- bbc %&gt;% group_by(grant_year) %&gt;% # Make invisible subgroups for each year summarize(total = sum(grant_amount), # Find the total awarded in each group avg = mean(grant_amount), # Find the average awarded in each group number = n()) # n() is a special function that shows the number of rows in each group # Look at our summarized data bbc_by_year ## # A tibble: 4 × 4 ## grant_year total avg number ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2016 17290488 78238. 221 ## 2 2017 62394278 59765. 1044 ## 3 2018 61349392 60205. 1019 ## 4 2019 41388816 61136. 677 Because we used summarize(), R shrank our data down significantly. We now only have a row for each of the subgroups we made: one for each year. We can plot this smaller data. We’ll use geom_col() for now. # Plot our summarized data ggplot(bbc_by_year, aes(x = grant_year, y = avg)) + geom_col() ggplot(bbc_by_year, aes(x = grant_year, y = total)) + geom_col() ggplot(bbc_by_year, aes(x = grant_year, y = number)) + geom_col() Based on these charts, it looks like 2016 saw the largest average grant amount. In all other years, grants averaged around £60,000, but in 2016 it jumped up to £80,000. If we look at total grants, though, we can see that there were far fewer grants awarded in 2016—only 221! 2017 and 2018 were much bigger years with far more money awarded. We can also use multiple aesthetics to reveal more information from the data. First we’ll make a new small summary dataset and group by both year and grant program. With those groups, we’ll again calculate the total, average, and number. bbc_year_size &lt;- bbc %&gt;% group_by(grant_year, grant_program) %&gt;% summarize(total = sum(grant_amount), avg = mean(grant_amount), number = n()) bbc_year_size ## # A tibble: 8 × 5 ## # Groups: grant_year [4] ## grant_year grant_program total avg number ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2016 Main Grants 16405586 86345. 190 ## 2 2016 Small Grants 884902 28545. 31 ## 3 2017 Main Grants 48502923 90154. 538 ## 4 2017 Small Grants 13891355 27453. 506 ## 5 2018 Main Grants 47347789 95652. 495 ## 6 2018 Small Grants 14001603 26721. 524 ## 7 2019 Main Grants 33019492 96267. 343 ## 8 2019 Small Grants 8369324 25058. 334 Next we’ll plot the data, mapping the grant_program column to the fill aesthetic: ggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() By default, ggplot will stack the different fill colors within the same bar, but this makes it a little hard to make comparisons. While we can see that the average small grant amount was a little bigger in 2017 than in 2019, it’s harder to compare average main grant amount, since the bottoms of those sections don’t align. To fix this, we can use position_dodge() to tell the columns to fit side-by-side: ggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col(position = position_dodge()) Instead of dodging, we can also facet by grant_program to separate the bars: ggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() + facet_wrap(vars(grant_program)) We can put these in one column if we want: ggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() + facet_wrap(vars(grant_program), ncol = 1) Finally, we can include even more variables! We have a lot of aesthetics we can work with (size, alpha, color, fill, linetype, etc.), as well as facets, so let’s add one more to show the duration of the awarded grant. First we’ll make another smaller summarized dataset, grouping by year, program, and duration and summarizing the total, average, and number of awards. bbc_year_size_duration &lt;- bbc %&gt;% group_by(grant_year, grant_program, grant_duration_text) %&gt;% summarize(total = sum(grant_amount), avg = mean(grant_amount), number = n()) bbc_year_size_duration ## # A tibble: 21 × 6 ## # Groups: grant_year, grant_program [8] ## grant_year grant_program grant_duration_text total avg number ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2016 Main Grants 2 years 97355 48678. 2 ## 2 2016 Main Grants 3 years 16308231 86746. 188 ## 3 2016 Small Grants 3 years 884902 28545. 31 ## 4 2017 Main Grants 1 year 59586 29793 2 ## 5 2017 Main Grants 2 years 825732 82573. 10 ## 6 2017 Main Grants 3 years 47617605 90528. 526 ## 7 2017 Small Grants 1 year 10000 10000 1 ## 8 2017 Small Grants 2 years 245227 18864. 13 ## 9 2017 Small Grants 3 years 13636128 27716. 492 ## 10 2018 Main Grants 1 year 118134 59067 2 ## # … with 11 more rows Next, we’ll fill by grant program and facet by duration and show the total number of grants awarded ggplot(bbc_year_size_duration, aes(x = grant_year, y = number, fill = grant_program)) + geom_col(position = position_dodge(preserve = &quot;single&quot;)) + facet_wrap(vars(grant_duration_text), ncol = 1) The vast majority of BBC Children in Need’s grants last for 3 years. Super neat. 2.13 Amounts and proportions For this example, we’re going to use real world data to demonstrate some different ways to visualize amounts and proportions. We’ll use data from the CDC and the Social Security Administration about the number of daily births in the United States from 1994–2014. FiveThirtyEight reported a story using this data in 2016 and they posted relatively CSV files on GitHub, so we can download and use those. If you want to follow along with this example, you can download the data directly from GitHub or by using these links (you’ll likely need to right click on these and choose “Save Link As…”): US_births_1994-2003_CDC_NCHS.csv US_births_2000-2014_SSA.csv 2.13.1 Load data There are two CSV files: US_births_1994-2003_CDC_NCHS.csv contains U.S. births data for the years 1994 to 2003, as provided by the Centers for Disease Control and Prevention’s National Center for Health Statistics. US_births_2000-2014_SSA.csv contains U.S. births data for the years 2000 to 2014, as provided by the Social Security Administration. Since the two datasets overlap in 2000–2003, we use Social Security Administration data for those years. We downloaded the data from GitHub and placed the CSV files in a folder named data. We’ll then load them with read_csv() and combine them into one data frame. library(tidyverse) library(scales) # For nice labels in charts births_1994_1999 &lt;- read_csv(&quot;data/US_births_1994-2003_CDC_NCHS.csv&quot;) %&gt;% # Ignore anything after 2000 filter(year &lt; 2000) births_2000_2014 &lt;- read_csv(&quot;data/US_births_2000-2014_SSA.csv&quot;) births_combined &lt;- bind_rows(births_1994_1999, births_2000_2014) 2.13.2 Wrangle data Let’s look at the first few rows of the data to see what we’re working with: births_combined ## # A tibble: 7,670 × 5 ## year month date_of_month day_of_week births ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1994 1 1 6 8096 ## 2 1994 1 2 7 7772 ## 3 1994 1 3 1 10142 ## 4 1994 1 4 2 11248 ## 5 1994 1 5 3 11053 ## 6 1994 1 6 4 11406 ## 7 1994 1 7 5 11251 ## 8 1994 1 8 6 8653 ## 9 1994 1 9 7 7910 ## 10 1994 1 10 1 10498 ## # … with 7,660 more rows The columns for year and births seem straightforward and ready to use. The columns for month and day of the week could be improved if we changed them to text (i.e. January instead of 1; Tuesday instead of 3). To fix this, we can convert these columns to categorical variables, or factors in R. We can also specify that these categories (or factors) are ordered, meaning that Feburary comes after January, etc. Without ordering, R will plot them alphabetically, which isn’t very helpful. We’ll make a new dataset named births that’s based on the combined births data, but with some new columns added: # The c() function lets us make a list of values month_names &lt;- c(&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot;) day_names &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;) births &lt;- births_combined %&gt;% # Make month an ordered factor, using the month_name list as labels mutate(month = factor(month, labels = month_names, ordered = TRUE)) %&gt;% mutate(day_of_week = factor(day_of_week, labels = day_names, ordered = TRUE), date_of_month_categorical = factor(date_of_month)) %&gt;% # Add a column indicating if the day is on a weekend mutate(weekend = ifelse(day_of_week %in% c(&quot;Saturday&quot;, &quot;Sunday&quot;), TRUE, FALSE)) head(births) ## # A tibble: 6 × 7 ## year month date_of_month day_of_week births date_of_month_categori… weekend ## &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt; &lt;lgl&gt; ## 1 1994 January 1 Saturday 8096 1 TRUE ## 2 1994 January 2 Sunday 7772 2 TRUE ## 3 1994 January 3 Monday 10142 3 FALSE ## 4 1994 January 4 Tuesday 11248 4 FALSE ## 5 1994 January 5 Wednesday 11053 5 FALSE ## 6 1994 January 6 Thursday 11406 6 FALSE If you look at the data now, you can see the columns are changed and have different types. year and date_of_month are still numbers, but month, and day_of_week are ordered factors (ord) and date_of_month_categorical is a regular factor (fct). Technically it’s also ordered, but because it’s already alphabetical (i.e. 2 naturally comes after 1), we don’t need to force it to be in the right order. Our births data is now clean and ready to go! 2.13.3 Bar plot First we can look at a bar chart showing the total number of births each day. We need to make a smaller summarized dataset and then we’ll plot it: total_births_weekday &lt;- births %&gt;% group_by(day_of_week) %&gt;% summarize(total = sum(births)) ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = day_of_week)) + geom_col() + # Turn off the fill legend because it&#39;s redundant guides(fill = FALSE) If we fill by day of the week, we get 7 different colors, which is fine (I guess), but doesn’t really help tell a story. The main story here is that there are far fewer births during weekends. If we create a new column that flags if a row is Saturday or Sunday, we can fill by that column instead: total_births_weekday &lt;- births %&gt;% group_by(day_of_week) %&gt;% summarize(total = sum(births)) %&gt;% mutate(weekend = day_of_week %in% c(&quot;Saturday&quot;, &quot;Sunday&quot;)) ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = weekend)) + geom_col() Neat! Those default colors are kinda ugly, though, so let’s use the principles of preattentive processing and contrast to highlight the weekend bars: ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = weekend)) + geom_col() + # Use grey and orange scale_fill_manual(values = c(&quot;grey70&quot;, &quot;#f2ad22&quot;)) + # Use commas instead of scientific notation scale_y_continuous(labels = comma) + # Turn off the legend since the title shows what the orange is guides(fill = FALSE) + labs(title = &quot;Weekends are unpopular times for giving birth&quot;, x = NULL, y = &quot;Total births&quot;) 2.13.4 Lollipop chart Since the ends of the bars are often the most important part of the graph, we can use a lollipop chart to emphasize them. We’ll keep all the same code from our bar chart and make a few changes: Color by weekend instead of fill by weekend, since points and lines are colored in ggplot, not filled Switch scale_fill_manual() to scale_color_manual() and turn off the color legend in the guides() layer Switch geom_col() to geom_pointrange(). The geom_pointrange() layer requires two additional aesthetics: ymin and ymax for the ends of the lines that come out of the point. Here we’ll set ymin to 0 so it starts at the x-axis, and we’ll set ymax to total so it ends at the point. ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, color = weekend)) + geom_pointrange(aes(ymin = 0, ymax = total), # Make the lines a little thicker and the dots a little bigger fatten = 5, size = 1.5) + # Use grey and orange scale_color_manual(values = c(&quot;grey70&quot;, &quot;#f2ad22&quot;)) + # Use commas instead of scientific notation scale_y_continuous(labels = comma) + # Turn off the legend since the title shows what the orange is guides(color = FALSE) + labs(title = &quot;Weekends are unpopular times for giving birth&quot;, x = NULL, y = &quot;Total births&quot;) 2.13.5 Strip plot Let’s show all the data with points. We’ll use the full dataset now, map x to weekday, y to births, and change geom_col() to geom_point(). We’ll tell geom_point() to jitter the points randomly. ggplot(data = births, mapping = aes(x = day_of_week, y = births, color = weekend)) + scale_color_manual(values = c(&quot;grey70&quot;, &quot;#f2ad22&quot;)) + geom_point(size = 0.5, position = position_jitter(height = 0)) + guides(color = FALSE) There are some interesting points in the low ends, likely because of holidays like Labor Day and Memorial Day (for the Mondays) and Thanksgiving (for the Thursday). If we had a column that indicated whether a day was a holiday, we could color by that and it would probably explain most of those low numbers. Unfortunately we don’t have that column, and it’d be hard to make. Some holidays are constant (Halloween is always October 31), but some aren’t (Thanksgiving is the fourth Thursday in November, so we’d need to find out which November 20-somethingth each year is the fourth Thursday, and good luck doing that at scale). 2.13.6 Beeswarm plot We can add some structure to these points if we use the ggbeeswarm package, with either geom_beeswarm() or geom_quasirandom(). geom_quasirandom() actually works better here since there are so many points – geom_beeswarm() makes the clusters of points way too wide. library(ggbeeswarm) ggplot(data = births, mapping = aes(x = day_of_week, y = births, color = weekend)) + scale_color_manual(values = c(&quot;grey70&quot;, &quot;#f2ad22&quot;)) + # Make these points suuuper tiny geom_quasirandom(size = 0.0001) + guides(color = FALSE) 2.13.7 Heatmap Finally, let’s use something non-traditional to show the average births by day in a somewhat proportional way. We can calculate the average number of births every day and then make a heatmap that fills each square by that average, thus showing the relative differences in births per day. To do this, we need to make a summarized data frame with group_by() %&gt;% summarize() to calculate the average number of births by month and day of the month (i.e. average for January 1, January 2, etc.). We’ll then make a sort of calendar with date of the month on the x axis, month on the y axis, with heat map squares filled by the daily average. We’ll use geom_tile() to add squares for each day, and then add some extra scale, coordinates, and theme layers to clean up the plot: avg_births_month_day &lt;- births %&gt;% group_by(month, date_of_month_categorical) %&gt;% summarize(avg_births = mean(births)) ggplot(data = avg_births_month_day, # By default, the y-axis will have December at the top, so use fct_rev() to reverse it mapping = aes(x = date_of_month_categorical, y = fct_rev(month), fill = avg_births)) + geom_tile() + # Add viridis colors scale_fill_viridis_c(option = &quot;inferno&quot;, labels = comma) + # Add nice labels labs(x = &quot;Day of the month&quot;, y = NULL, title = &quot;Average births per day&quot;, subtitle = &quot;1994-2014&quot;, fill = &quot;Average births&quot;) + # Force all the tiles to have equal widths and heights coord_equal() + # Use a cleaner theme theme_minimal() Neat! There are some really interesting trends here. Most obvious, probably, is that very few people are born on New Year’s Day, July 4th, Halloween, Thanksgiving, and Christmas. avg_births_month_day %&gt;% arrange(avg_births) ## # A tibble: 366 × 3 ## # Groups: month [12] ## month date_of_month_categorical avg_births ## &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 December 25 6601. ## 2 January 1 7827. ## 3 December 24 8103. ## 4 July 4 8825. ## 5 January 2 9356. ## 6 December 26 9599. ## 7 November 27 9770. ## 8 November 23 9919. ## 9 November 25 10001 ## 10 October 31 10030. ## # … with 356 more rows The days with the highest average are in mid-September, likely because that’s about 9 months after the first week of January. July 7th at #7 is odd and I have no idea why it might be so popular. avg_births_month_day %&gt;% arrange(desc(avg_births)) ## # A tibble: 366 × 3 ## # Groups: month [12] ## month date_of_month_categorical avg_births ## &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 September 9 12344. ## 2 September 19 12285. ## 3 September 12 12282. ## 4 September 17 12201. ## 5 September 10 12190. ## 6 September 20 12162. ## 7 July 7 12147. ## 8 September 15 12126. ## 9 September 16 12114. ## 10 September 18 12112. ## # … with 356 more rows The funniest trend is the very visible dark column for the 13th of every month. People really don’t want to give birth on the 13th. 2.14 Comparisons For this example, we’re going to use cross-national data, but instead of using the typical gapminder dataset, we’re going to collect data directly from the World Bank’s Open Data portal If you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”): wdi_raw.csv 2.14.1 Load and clean data First, we load the libraries we’ll be using: library(tidyverse) # For ggplot, dplyr, and friends library(WDI) # For getting data from the World Bank library(geofacet) # For map-shaped facets library(scales) # For helpful scale functions like dollar() library(ggrepel) # For non-overlapping labels The World Bank has a ton of country-level data at data.worldbank.org. We can use a package named WDI (world development indicators) to access their servers and download the data directly into R. To do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find this page. If you look at the end of the URL, you’ll see a cryptic code: EG.ELC.ACCS.ZS. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator. We can feed a list of ID codes to the WDI() function to download data for those specific indicators. We want data from 1995-2015, so we set the start and end years accordingly. The extra=TRUE argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed. indicators &lt;- c(&quot;SP.DYN.LE00.IN&quot;, # Life expectancy &quot;EG.ELC.ACCS.ZS&quot;, # Access to electricity &quot;EN.ATM.CO2E.PC&quot;, # CO2 emissions &quot;NY.GDP.PCAP.KD&quot;) # GDP per capita wdi_raw &lt;- WDI(country = &quot;all&quot;, indicators, extra = TRUE, start = 1995, end = 2015) head(wdi_raw) Downloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that. write_csv(wdi_raw, &quot;data/wdi_raw.csv&quot;) Then we clean up the data a little, filtering out rows that aren’t actually countries and renaming the ugly World Bank code columns to actual words: wdi_clean &lt;- wdi_raw %&gt;% filter(region != &quot;Aggregates&quot;) %&gt;% select(iso2c, country, year, life_expectancy = SP.DYN.LE00.IN, access_to_electricity = EG.ELC.ACCS.ZS, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income) head(wdi_clean) ## # A tibble: 6 × 9 ## iso2c country year life_expectancy access_to_elect… co2_emissions gdp_per_cap ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD Andorra 2015 NA 100 NA 41768. ## 2 AD Andorra 2004 NA 100 7.36 47033. ## 3 AD Andorra 2001 NA 100 7.79 41421. ## 4 AD Andorra 2002 NA 100 7.59 42396. ## 5 AD Andorra 2014 NA 100 5.83 40790. ## 6 AD Andorra 1995 NA 100 6.66 32918. ## # … with 2 more variables: region &lt;chr&gt;, income &lt;chr&gt; 2.14.2 Small multiples First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country. life_expectancy_small &lt;- wdi_clean %&gt;% filter(country %in% c(&quot;Afghanistan&quot;, &quot;Belarus&quot;, &quot;India&quot;, &quot;Mexico&quot;, &quot;New Zealand&quot;, &quot;Spain&quot;)) ggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country)) Small multiples! That’s all we need to do. We can do some fancier things, though. We can make this plot hyper minimalist: ggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = &quot;free_y&quot;) + theme_void() + theme(strip.text = element_text(face = &quot;bold&quot;)) We can do a whole part of a continent (poor Iraq and Syria) life_expectancy_mena &lt;- wdi_clean %&gt;% filter(region == &quot;Middle East &amp; North Africa&quot;) ggplot(data = life_expectancy_mena, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = &quot;free_y&quot;, nrow = 3) + theme_void() + theme(strip.text = element_text(face = &quot;bold&quot;)) We can use the geofacet package to arrange these facets by geography: life_expectancy_eu &lt;- wdi_clean %&gt;% filter(region == &quot;Europe &amp; Central Asia&quot;) ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_geo(vars(country), grid = &quot;eu_grid1&quot;, scales = &quot;free_y&quot;) + labs(x = NULL, y = NULL, title = &quot;Life expectancy from 1995–2015&quot;, caption = &quot;Source: The World Bank (SP.DYN.LE00.IN)&quot;) + theme_minimal() + theme(strip.text = element_text(face = &quot;bold&quot;), plot.title = element_text(face = &quot;bold&quot;), axis.text.x = element_text(angle = 45, hjust = 1)) Neat! 2.14.3 Sparklines Sparklines are just line charts (or bar charts) that are really really small. india_co2 &lt;- wdi_clean %&gt;% filter(country == &quot;India&quot;) plot_india &lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_india ggsave(&quot;india_co2.pdf&quot;, plot_india, width = 1, height = 0.15, units = &quot;in&quot;) ggsave(&quot;india_co2.png&quot;, plot_india, width = 1, height = 0.15, units = &quot;in&quot;) china_co2 &lt;- wdi_clean %&gt;% filter(country == &quot;China&quot;) plot_china &lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_china ggsave(&quot;china_co2.pdf&quot;, plot_china, width = 1, height = 0.15, units = &quot;in&quot;) ggsave(&quot;china_co2.png&quot;, plot_china, width = 1, height = 0.15, units = &quot;in&quot;) You can then use those saved tiny plots in your text. Both India and China have seen increased CO2 emissions over the past 20 years. 2.14.4 Slopegraphs We can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The group_by(...) %&gt;% filter(...) %&gt;% ungroup() pipeline does this, with the !any(is.na(gdp_per_cap)) test keeping any rows where any of the gdp_per_cap values are not missing for the whole country. We then add a couple special columns for labels. The paste0() function concatenates strings and variables together, so that paste0(\"2 + 2 = \", 2 + 2) would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year. gdp_south_asia &lt;- wdi_clean %&gt;% filter(region == &quot;South Asia&quot;) %&gt;% filter(year %in% c(1995, 2015)) %&gt;% # Look at each country individually group_by(country) %&gt;% # Remove the country if any of its gdp_per_cap values are missing filter(!any(is.na(gdp_per_cap))) %&gt;% ungroup() %&gt;% # Make year a factor mutate(year = factor(year)) %&gt;% # Make some nice label columns # If the year is 1995, format it like &quot;Country name: $GDP&quot;. If the year is # 2015, format it like &quot;$GDP&quot; mutate(label_first = ifelse(year == 1995, str_c(country, &quot;: &quot;, dollar(round(gdp_per_cap))), NA), label_last = ifelse(year == 2015, dollar(round(gdp_per_cap, 0)), NA)) With the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the group aesthetic. ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) Cool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with geom_text(): ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = country)) + guides(color = FALSE) That gets us a little closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those label_first and label_last columns we made? Let’s use those instead: ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = label_first)) + geom_text(aes(label = label_last)) + guides(color = FALSE) Now we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The ggrepel package lets us do this with geom_text_repel() ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first)) + geom_text_repel(aes(label = label_last)) + guides(color = FALSE) Now none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the direction = \"y\" argument, and we can move all the labels to the left or right with the nudge_x argument. The seed argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number. ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = &quot;y&quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = &quot;y&quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) That’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect. ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = &quot;y&quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = &quot;y&quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) + scale_color_viridis_d(option = &quot;magma&quot;, end = 0.9) + theme_void() 2.14.5 Bump charts Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO2 emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the co2_emissions column. sa_co2 &lt;- wdi_clean %&gt;% filter(region == &quot;South Asia&quot;) %&gt;% filter(year &gt;= 2004, year &lt; 2015) %&gt;% group_by(year) %&gt;% mutate(rank = rank(co2_emissions)) We then plot this with points and lines, reversing the y-axis so 1 is at the top: ggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line() + geom_point() + scale_y_reverse(breaks = 1:8) Afghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan. As with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner. ggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line(size = 2) + geom_point(size = 4) + geom_text(data = filter(sa_co2, year == 2004), aes(label = iso2c, x = 2003.25), fontface = &quot;bold&quot;) + geom_text(data = filter(sa_co2, year == 2014), aes(label = iso2c, x = 2014.75), fontface = &quot;bold&quot;) + guides(color = FALSE) + scale_y_reverse(breaks = 1:8) + scale_x_continuous(breaks = 2004:2014) + scale_color_viridis_d(option = &quot;magma&quot;, begin = 0.2, end = 0.9) + labs(x = NULL, y = &quot;Rank&quot;) + theme_minimal() + theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank()) If you want to be super fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the ggflags package. See here for an example. Acknowledgments Coding examples from Andrew Heiss Session info devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.2 (2021-11-01) ## os macOS Monterey 12.2.1 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Chicago ## date 2022-03-04 ## pandoc 2.17.1.1 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.1.1) ## beeswarm 0.4.0 2021-06-01 [1] CRAN (R 4.1.0) ## bit 4.0.4 2020-08-04 [1] CRAN (R 4.1.1) ## bit64 4.0.5 2020-08-30 [1] CRAN (R 4.1.0) ## bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.1) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.1.1) ## broom * 0.7.12 2022-01-28 [1] CRAN (R 4.1.1) ## bslib 0.3.1 2021-10-06 [1] CRAN (R 4.1.1) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.1.1) ## callr 3.7.0 2021-04-20 [1] CRAN (R 4.1.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.1.0) ## class 7.3-20 2022-01-13 [1] CRAN (R 4.1.1) ## classInt 0.4-3 2020-04-07 [1] CRAN (R 4.1.0) ## cli 3.2.0 2022-02-14 [1] CRAN (R 4.1.1) ## codetools 0.2-18 2020-11-04 [1] CRAN (R 4.1.2) ## colorspace 2.0-3 2022-02-21 [1] CRAN (R 4.1.1) ## crayon 1.5.0 2022-02-14 [1] CRAN (R 4.1.1) ## DBI 1.1.2 2021-12-20 [1] CRAN (R 4.1.1) ## dbplyr 2.1.1 2021-04-06 [1] CRAN (R 4.1.0) ## desc 1.4.0 2021-09-28 [1] CRAN (R 4.1.1) ## devtools 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.1.1) ## dplyr * 1.0.8 2022-02-08 [1] CRAN (R 4.1.1) ## e1071 1.7-9 2021-09-16 [1] CRAN (R 4.1.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) ## evaluate 0.15 2022-02-18 [1] CRAN (R 4.1.1) ## fansi 1.0.2 2022-01-14 [1] CRAN (R 4.1.1) ## farver 2.1.0 2021-02-28 [1] CRAN (R 4.1.0) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.1.1) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.1.1) ## gapminder * 0.3.0 2017-10-31 [1] CRAN (R 4.1.0) ## generics 0.1.2 2022-01-31 [1] CRAN (R 4.1.1) ## geofacet * 0.2.0 2020-05-26 [1] CRAN (R 4.1.1) ## geogrid 0.1.1 2018-12-11 [1] CRAN (R 4.1.0) ## ggbeeswarm * 0.6.0 2017-08-07 [1] CRAN (R 4.1.0) ## ggplot2 * 3.3.5 2021-06-25 [1] CRAN (R 4.1.1) ## ggrepel * 0.9.1 2021-01-15 [1] CRAN (R 4.1.1) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.1.1) ## gridExtra 2.3 2017-09-09 [1] CRAN (R 4.1.1) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.1.1) ## haven 2.4.3 2021-08-04 [1] CRAN (R 4.1.1) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.1.0) ## highr 0.9 2021-04-16 [1] CRAN (R 4.1.0) ## hms 1.1.1 2021-09-26 [1] CRAN (R 4.1.1) ## htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.1) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.1.0) ## imguR 1.0.3 2016-03-29 [1] CRAN (R 4.1.0) ## jpeg 0.1-9 2021-07-24 [1] CRAN (R 4.1.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) ## jsonlite 1.8.0 2022-02-22 [1] CRAN (R 4.1.1) ## KernSmooth 2.23-20 2021-05-03 [1] CRAN (R 4.1.2) ## knitr * 1.37 2021-12-16 [1] CRAN (R 4.1.1) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.1.0) ## lattice 0.20-45 2021-09-22 [1] CRAN (R 4.1.2) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.1.1) ## lubridate * 1.8.0 2021-10-07 [1] CRAN (R 4.1.1) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.1.1) ## Matrix 1.4-0 2021-12-08 [1] CRAN (R 4.1.1) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.1.1) ## mgcv 1.8-38 2021-10-06 [1] CRAN (R 4.1.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.1.0) ## nlme 3.1-155 2022-01-13 [1] CRAN (R 4.1.1) ## pillar 1.7.0 2022-02-01 [1] CRAN (R 4.1.1) ## pkgbuild 1.3.1 2021-12-20 [1] CRAN (R 4.1.1) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.1.0) ## pkgload 1.2.4 2021-11-30 [1] CRAN (R 4.1.1) ## png 0.1-7 2013-12-03 [1] CRAN (R 4.1.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.1.0) ## processx 3.5.2 2021-04-30 [1] CRAN (R 4.1.0) ## proxy 0.4-26 2021-06-07 [1] CRAN (R 4.1.0) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.1.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.1) ## Rcpp 1.0.8 2022-01-13 [1] CRAN (R 4.1.1) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.1.1) ## readxl * 1.3.1 2019-03-13 [1] CRAN (R 4.1.0) ## remotes 2.4.2 2021-11-30 [1] CRAN (R 4.1.1) ## reprex 2.0.1 2021-08-05 [1] CRAN (R 4.1.1) ## rgdal 1.5-28 2021-12-15 [1] CRAN (R 4.1.1) ## rgeos 0.5-9 2021-12-15 [1] CRAN (R 4.1.1) ## RJSONIO 1.3-1.6 2021-09-16 [1] CRAN (R 4.1.1) ## rlang 1.0.1 2022-02-03 [1] CRAN (R 4.1.1) ## rmarkdown 2.11 2021-09-14 [1] CRAN (R 4.1.1) ## rnaturalearth 0.1.0 2017-03-21 [1] CRAN (R 4.1.0) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.1.0) ## rvest 1.0.2 2021-10-16 [1] CRAN (R 4.1.1) ## sass 0.4.0 2021-05-12 [1] CRAN (R 4.1.0) ## scales * 1.1.1 2020-05-11 [1] CRAN (R 4.1.0) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.1.1) ## sf 1.0-6 2022-02-04 [1] CRAN (R 4.1.1) ## socviz * 1.2 2020-06-10 [1] CRAN (R 4.1.0) ## sp 1.4-6 2021-11-14 [1] CRAN (R 4.1.1) ## stringi 1.7.6 2021-11-29 [1] CRAN (R 4.1.1) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.1.1) ## testthat 3.1.2 2022-01-20 [1] CRAN (R 4.1.1) ## tibble * 3.1.6 2021-11-07 [1] CRAN (R 4.1.1) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.1.1) ## tidyselect 1.1.2 2022-02-21 [1] CRAN (R 4.1.1) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.1.0) ## tzdb 0.2.0 2021-10-27 [1] CRAN (R 4.1.1) ## units 0.8-0 2022-02-05 [1] CRAN (R 4.1.1) ## usethis 2.1.5 2021-12-09 [1] CRAN (R 4.1.1) ## utf8 1.2.2 2021-07-24 [1] CRAN (R 4.1.0) ## vctrs 0.3.8 2021-04-29 [1] CRAN (R 4.1.0) ## vipor 0.4.5 2017-03-22 [1] CRAN (R 4.1.0) ## viridisLite 0.4.0 2021-04-13 [1] CRAN (R 4.1.0) ## vroom 1.5.7 2021-11-30 [1] CRAN (R 4.1.1) ## WDI * 2.7.6 2022-02-25 [1] CRAN (R 4.1.1) ## withr 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## xfun 0.29 2021-12-14 [1] CRAN (R 4.1.1) ## xml2 1.3.3 2021-11-30 [1] CRAN (R 4.1.1) ## yaml 2.3.5 2022-02-21 [1] CRAN (R 4.1.1) ## ## [1] /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── References "],["science-polish.html", "Day 3 Making plots pretty and clean Learning objectives Assigned readings 3.1 Tufte’s world 3.2 Chart junk 3.3 Compare Tufte minimal graphs to traditional graphs using ggplot2 3.4 Reconsidering Tufte 3.5 Visualizing uncertainty 3.6 Building a theme() 3.7 Annotations Acknowledgments Session info", " Day 3 Making plots pretty and clean library(tidyverse) library(ggthemes) library(knitr) library(broom) library(stringr) library(socviz) library(patchwork) library(RColorBrewer) library(colorspace) library(dichromat) library(ggrepel) library(ggthemes) library(scales) library(gapminder) library(here) Learning objectives 3.0.1 Morning Define Tufte’s theory of data graphics, data-ink ratio, and chartjunk Present and compare examples of minimalistic graphics to their original form Assess visualizations under the engineer/designers philosophy 3.0.2 Afternoon Demonstrate methods for visualizing uncertainty Generate layered plots to highlight specific attributes of data Adjust themes Assigned readings Chapter 8, Healy (2018) - accessible via the book’s website 3.1 Tufte’s world Core purpose of visualization is to communicate quantitative information Art is secondary “Above all else show the data” Goal is to maximize the data-ink ratio \\[\\text{Data-ink ratio} = \\frac{\\text{data-ink}}{\\text{total ink used to print the graphic}}\\] Data-ink - non-erasable core of a graphic This is what Tufte says we should most care about Minimize all extraneous fluff What should we consider to be part of the “data-ink”? Is this literally just the data? Don’t we need gridlines or axes? What else can be considered integral to the graph? He never offers proof of his hypothesis that less is better 3.1.1 What is integral? Data points Axis ticks Axis tick labels Axis labels Background Grid lines What happens if we strip away everything except the data? Hmm, so what do we actually need to keep? What should we consider “integral”? What if we remove the background color? Remove panel box Remove minor grid lines Remove all grid lines Remove tick marks Use serif font What have we lost? Is this easier to interpret? Harder? 3.2 Chart junk Vibrating moire effects Hard to produce in ggplot2 - no support for them Eye junk Makes the graph harder to decode/interpret The grid Minimize/reduce the thickness of grid lines to ease interpretation Less visual clutter to weed through Add some compare/contrast with ggplot The duck Tufte concludes that forgoing chartjunk enables functionality and insight (as Cairo would describe it). Do you agree? 3.3 Compare Tufte minimal graphs to traditional graphs using ggplot2 ggthemes Compare other themes for the same basic plot11 The goal of Tufte’s minimalism is to maximize the data-ink ratio, so we want to modify traditional or default graphs in R and ggplot2 to minimize use of extraneous ink. 3.3.1 Minimal line plot We use geom_point() to draw the data points and geom_line() to connect the points What is the extraneous ink on this graph? Background Title of graph and y-axis labels - redundant x-axis label - year is obvious/self-explanatory Missing context - how is this expansion meaningful over time? Remove axis and graph titles Adds text annotation within graph Highlights a 5% increase in per capital expandures Changes font to be more aesthetically pleasing, not so blockish 3.3.2 Minimal boxplot Key features of a boxplot Lines to indicate: Maximum of IQR 3rd quartile Median 1st quartile Minimum of IQR Dots for outliers How many different line strokes do we use? 8 for each graph \\(8 \\times 22 = 176\\) This is extraneous ink Now we use only 22 verticals to show the same data. It could easily be drawn by hand with a single vertical for each category on the x-axis Doesn’t show outlier info, but is this really necessary? Also removes the background color and gridlines Here we use offsetting lines to indicate the middle half of the data rather than using a gap Is this prettier? Easier to interpret? 3.3.3 Minimal barchart Again, the background is the main culprit Erases the box/grid background Removes vertical axis Use a white grid to show coordinate lines through the absence of ink, rather than adding ink Allows us to remove tick marks as well 3.3.4 Range-frame scatterplot A standard bivariate scatterplot Use the frame/axis lines of the graph to communicate important information Extends only to minimum/maximum values in the data, rather than arbitrary points Explicitly identifies the minimum and maximum values 3.3.4.1 With a quartile plot Combine with info on the quartiles of the data to show case this info as well Thicker bar indicates inner two quartiles Median is explicitly labeled 3.4 Reconsidering Tufte 3.4.1 When is redundancy better? Figure 1.1: Double-time bar chart of crime in the city of San Francisco, 2009-10. Source: Visualizing Time with the Double-Time Bar Chart Each set of 24 bars show the same data. The top bars run from midnight to 11pm. The bottom bars run from noon to 11am. Highlighted regions represent 6-5 (6am-5pm; 6pm-5am) Colors represent (roughly) day and night (yellow for day, blue for night) Enables representing trends over a 24 hour period without breaking arbitrarily at midnight Figure 1.2: Double-time bar chart of crime in the city of San Francisco, 2009-10. Source: Visualizing Time with the Double-Time Bar Chart The second graph is incredibly redundant, but which is easier to interpret? Does it pass Tufte’s test? What does it mean to be “integral”? 3.4.2 Does minimalism really help here? Accompanying an article declaring that student progress on NAEP tests has come to a virtual standstill Figure 3.1: Chart from Harvard magazine. Source: Involuntary head-shaking is probably not an intended consequence of data visualization What is the comparison we should make? Is this too much color? Meets Tufte’s minimalist standards, probably has a decent data-ink ratio Note Grade 4 math scores for whites in 2009-2015 - does this mean no progress or unknown scores? Figure 3.2: Redesigned chart from Harvard magazine. Source: Involuntary head-shaking is probably not an intended consequence of data visualization This version is much clearer - specifically tells us how to compare the scores Removes color as a channel, using linetype instead In this situation, is that better or worse? Title for the graph makes clear the point trying to be made 3.4.3 Experimental tests of Tufte’s claims How do we know Tufte’s claims are true? We can test them with experiments! Figure 3.3: Source: Figure 2 from Bateman, Scott, et al. “Useful junk?: the effects of visual embellishment on comprehension and memorability of charts.” Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2010. 3.4.3.1 Protocol Compared chartjunk versions of graphs to standard/minimalist versions of graphs Tested individuals on chart description and recall 20 subjects split into short and long-term recall groups Quite a small sample of convenience (university population) Collected measures Response scores - did the individual correctly read/interpret the chart? Preferences - which type of chart did the individual prefer? Standard or embellished? Gaze data - where did the subject look during the experiment? At data regions or embellishment regions? 3.4.3.2 Results Figure 3.4: Source: Figures 4-6 from Bateman, Scott, et al. “Useful junk?: the effects of visual embellishment on comprehension and memorability of charts.” Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2010. No difference for description No difference for immediate recall Embellished images slightly better for long-term recall (12-22 days after treatment) 3.4.4 Discussing the results Why did the chartjunk not lead to worse description and recall? Chartjunk was related to the topic of the chart “Gets to the point quicker” Why would the embellished images produce better long-term recall? Very vivid image Value message - individual believes author is trying to communicate a set of values Embellished images produced more value messages Should visualizations be “objective”? Tufte seems to think so: minimalism leads to the data speaking for itself - do we buy this? 3.4.5 Rethinking Tufte’s definition of visual excellence Too many of Tufte’s claims are based on nothing - no evidence to support his minimalistic approach to graphical design Think of the hockey stick chart vs. xkcd on Earth’s temperature According to Tufte, both probably have a lot of chartjunk (xkcd more so) But if asked to remember the importance and story of the graph weeks later, which one do you think the average reader would recall better? 3.4.6 Testing this theory Design an experiment to test the impact/effectiveness of chartjunk vs. minimalism What protocols/features could we use? How could we deploy the experiment? If deployed on a platform such as Amazon MTurk, what are the benefits and drawbacks? 3.5 Visualizing uncertainty Download the necessary data files for the following coding exercises using usethis::use_course(\"css-data-mining-viz/science-polish\"). For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package. atl-weather-2019.csv 3.5.1 Load and clean data First, we load the libraries we’ll be using: library(tidyverse) library(lubridate) library(ggridges) library(gghalves) Then we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data: weather_atl_raw &lt;- read_csv(&quot;data/atl-weather-2019.csv&quot;) We’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date: weather_atl &lt;- weather_atl_raw %&gt;% mutate(Month = month(time, label = TRUE, abbr = FALSE), Day = wday(time, label = TRUE, abbr = FALSE)) Now we’re ready to go! 3.5.2 Histograms We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white: ggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5: ggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) We can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic: ggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant. ggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) + guides(fill = FALSE) + facet_wrap(vars(Month)) Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less. 3.5.3 Density plots The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer: ggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = &quot;grey20&quot;, fill = &quot;grey50&quot;) If we want, we can mess with some of the calculus options like the kernel and bandwidth: ggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = &quot;grey20&quot;, fill = &quot;grey50&quot;, bw = 0.1, kernel = &quot;epanechnikov&quot;) We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack: ggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms: ggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) + guides(fill = FALSE) + facet_wrap(vars(Month)) Or we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function: ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges() + guides(fill = FALSE) We can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half: ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) Now that we have good working code, we can easily substitute in other variables by changing the x mapping: ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷: ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = &quot;plasma&quot;) + labs(x = &quot;High temperature&quot;, y = NULL, color = &quot;Temp&quot;) And finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather(): weather_atl_long &lt;- weather_atl %&gt;% pivot_longer(cols = c(temperatureLow, temperatureHigh), names_to = &quot;temp_type&quot;, values_to = &quot;temp&quot;) %&gt;% # Clean up the new temp_type column so that &quot;temperatureHigh&quot; becomes &quot;High&quot;, etc. mutate(temp_type = recode(temp_type, temperatureHigh = &quot;High&quot;, temperatureLow = &quot;Low&quot;)) %&gt;% # This is optional—just select a handful of columns select(time, temp_type, temp, Month) # Show the first few rows head(weather_atl_long) ## # A tibble: 6 × 4 ## time temp_type temp Month ## &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;ord&gt; ## 1 2019-01-01 05:00:00 Low 50.6 January ## 2 2019-01-01 05:00:00 High 63.9 January ## 3 2019-01-02 05:00:00 Low 49.0 January ## 4 2019-01-02 05:00:00 High 57.4 January ## 5 2019-01-03 05:00:00 Low 53.1 January ## 6 2019-01-03 05:00:00 High 55.3 January Now we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots: ggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month), fill = ..x.., linetype = temp_type)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = &quot;plasma&quot;) + labs(x = &quot;High temperature&quot;, y = NULL, color = &quot;Temp&quot;) Super neat! We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter. 3.5.4 Box, violin, and rain cloud plots Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday: ggplot(weather_atl, aes(y = windSpeed, fill = Day)) + geom_boxplot() We can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis: ggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot: ggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result: ggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = &quot;point&quot;, fun = &quot;mean&quot;, size = 5, color = &quot;white&quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also show the mean and confidence interval at the same time by changing the summary function: ggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = &quot;pointrange&quot;, fun.data = &quot;mean_se&quot;, size = 1, color = &quot;white&quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so: ggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = &quot;l&quot;, size = 0.5) + geom_half_boxplot(aes(fill = Day), side = &quot;r&quot;) + guides(color = FALSE, fill = FALSE) Note the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin(): ggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = &quot;l&quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = &quot;r&quot;) + guides(color = FALSE, fill = FALSE) If we flip the plot, we can make a rain cloud plot: ggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_boxplot(aes(fill = Day), side = &quot;l&quot;, width = 0.5, nudge = 0.1) + geom_half_point(aes(color = Day), side = &quot;l&quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = &quot;r&quot;) + guides(color = FALSE, fill = FALSE) + coord_flip() Neat! 3.6 Building a theme() Consider this example using gapminder: gapminder_filtered &lt;- gapminder %&gt;% filter(year &gt; 2000) base_plot &lt;- ggplot( data = gapminder_filtered, mapping = aes( x = gdpPercap, y = lifeExp, color = continent, size = pop ) ) + geom_point() + # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00) scale_x_log10(labels = dollar_format(accuracy = 1)) + # Format with commas scale_size_continuous(labels = comma) + # Use viridis scale_color_viridis_d(option = &quot;plasma&quot;, end = 0.9) + labs( x = &quot;GDP per capita&quot;, y = &quot;Life expectancy&quot;, color = &quot;Continent&quot;, size = &quot;Population&quot;, title = &quot;Here&#39;s a cool title&quot;, subtitle = &quot;And here&#39;s a neat subtitle&quot;, caption = &quot;Source: The Gapminder Project&quot; ) + facet_wrap(vars(year)) base_plot Now we have base_plot to work with. Here’s what it looks like with theme_minimal() applied to it: base_plot + theme_minimal() That gets rid of the grey background and is a good start, but we can make lots of improvements. First let’s deal with the gridlines. There are too many. We can get rid of the minor gridlines with by setting them to element_blank(): base_plot + theme_minimal() + theme(panel.grid.minor = element_blank()) Next let’s add some typographic contrast. We’ll use Roboto Condensed Regular as the base font. Before trying this, make sure you do the following: On macOS: Run capabilities() in your console and verify that TRUE shows up under cairo If not, download and install XQuartz On Windows: Run windowsFonts() in your console and you’ll see a list of all the fonts you can use with R. It’s not a very big list. #&gt; $serif #&gt; [1] &quot;TT Times New Roman&quot; #&gt; #&gt; $sans #&gt; [1] &quot;TT Arial&quot; #&gt; #&gt; $mono #&gt; [1] &quot;TT Courier New&quot; You can add Roboto Condensed to your current R session by running this in your console: windowsFonts(`Roboto Condensed` = windowsFont(&quot;Roboto Condensed&quot;)) Now if you run windowsFonts(), you’ll see it in the list: #&gt; $serif #&gt; [1] &quot;TT Times New Roman&quot; #&gt; #&gt; $sans #&gt; [1] &quot;TT Arial&quot; #&gt; #&gt; $mono #&gt; [1] &quot;TT Courier New&quot; #&gt; #&gt; $`Roboto Condensed` #&gt; [1] &quot;Roboto Condensed&quot; This only takes effect for your current R session, so if you are knitting a document or if you ever plan on closing RStudio, you’ll need to incorporate this font creation code into your script. We’ll use the font as the base_family argument. Note how I make it bold with face and change the size with rel(). Instead of manually setting some arbitrary size, I use rel() to resize the text in relation to the base_size argument. Using rel(1.7) means 1.7 × base_size, or 20.4 That will rescale according to whatever base_size is—if I shrink it to base_size = 8, the title will scale down accordingly. plot_with_good_typography &lt;- base_plot + theme_minimal(base_family = &quot;Roboto Condensed&quot;, base_size = 12) + theme( panel.grid.minor = element_blank(), # Bold, bigger title plot.title = element_text(face = &quot;bold&quot;, size = rel(1.7)), # Plain, slightly bigger subtitle that is grey plot.subtitle = element_text(face = &quot;plain&quot;, size = rel(1.3), color = &quot;grey70&quot;), # Italic, smaller, grey caption that is left-aligned plot.caption = element_text( face = &quot;italic&quot;, size = rel(0.7), color = &quot;grey70&quot;, hjust = 0 ), # Bold legend titles legend.title = element_text(face = &quot;bold&quot;), # Bold, slightly larger facet titles that are left-aligned for the sake of repetition strip.text = element_text(face = &quot;bold&quot;, size = rel(1.1), hjust = 0), # Bold axis titles axis.title = element_text(face = &quot;bold&quot;), # Add some space above the x-axis title and make it left-aligned axis.title.x = element_text(margin = margin(t = 10), hjust = 0), # Add some space to the right of the y-axis title and make it top-aligned axis.title.y = element_text(margin = margin(r = 10), hjust = 1) ) plot_with_good_typography Whoa. That gets us most of the way there! We have good contrast with the typography, with the strong bold and the lighter regular font (contrast). Everything is aligned left (alignment and repetition). By moving the axis titles a little bit away from the labels, we’ve enhanced proximity, since they were too close together (proximity). We repeat grey in both the caption and the subtitle (repetition). The only thing I don’t like is that the 2002 isn’t quite aligned with the title and subtitle. This is because the facet labels are in boxes along the top of each plot, and in some themes (like theme_grey() and theme_bw()) those facet labels have grey backgrounds. We can turn off the margin in those boxes, or we can add a background, which will then be perfectly aligned with the title and subtitle. plot_with_good_typography + # Add a light grey background to the facet titles, with no borders theme( strip.background = element_rect(fill = &quot;grey90&quot;, color = NA), # Add a thin grey border around all the plots to tie in the facet titles panel.border = element_rect(color = &quot;grey90&quot;, fill = NA) ) That looks great! To save ourselves time in the future, we can store this whole thing as an object that we can then reuse on other plots: my_pretty_theme &lt;- theme_minimal(base_family = &quot;Roboto Condensed&quot;, base_size = 12) + theme( panel.grid.minor = element_blank(), # Bold, bigger title plot.title = element_text(face = &quot;bold&quot;, size = rel(1.7)), # Plain, slightly bigger subtitle that is grey plot.subtitle = element_text(face = &quot;plain&quot;, size = rel(1.3), color = &quot;grey70&quot;), # Italic, smaller, grey caption that is left-aligned plot.caption = element_text( face = &quot;italic&quot;, size = rel(0.7), color = &quot;grey70&quot;, hjust = 0 ), # Bold legend titles legend.title = element_text(face = &quot;bold&quot;), # Bold, slightly larger facet titles that are left-aligned for the sake of repetition strip.text = element_text(face = &quot;bold&quot;, size = rel(1.1), hjust = 0), # Bold axis titles axis.title = element_text(face = &quot;bold&quot;), # Add some space above the x-axis title and make it left-aligned axis.title.x = element_text(margin = margin(t = 10), hjust = 0), # Add some space to the right of the y-axis title and make it top-aligned axis.title.y = element_text(margin = margin(r = 10), hjust = 1), # Add a light grey background to the facet titles, with no borders strip.background = element_rect(fill = &quot;grey90&quot;, color = NA), # Add a thin grey border around all the plots to tie in the facet titles panel.border = element_rect(color = &quot;grey90&quot;, fill = NA) ) Now we can use it on any plot. mpg_example &lt;- ggplot( data = mpg, mapping = aes(x = displ, y = hwy, color = class) ) + geom_point(size = 3) + scale_color_viridis_d() + facet_wrap(vars(drv)) + labs( x = &quot;Displacement&quot;, y = &quot;Highway MPG&quot;, color = &quot;Car class&quot;, title = &quot;Heavier cars get worse mileage&quot;, subtitle = &quot;Except two-seaters?&quot;, caption = &quot;Here&#39;s a caption&quot; ) + my_pretty_theme mpg_example Super neat! 3.7 Annotations For this example, we’re again going to use cross-national data from the World Bank’s Open Data portal. We’ll download the data with the WDI package. wdi_co2.csv 3.7.1 Load data First, we load the libraries we’ll be using: library(tidyverse) # For ggplot, dplyr, and friends library(WDI) # Get data from the World Bank library(ggrepel) # For non-overlapping labels # You need to install ggtext from GitHub. Follow the instructions at # https://github.com/wilkelab/ggtext library(ggtext) # For fancier text handling indicators &lt;- c(&quot;SP.POP.TOTL&quot;, # Population &quot;EN.ATM.CO2E.PC&quot;, # CO2 emissions &quot;NY.GDP.PCAP.KD&quot;) # GDP per capita wdi_co2_raw &lt;- WDI(country = &quot;all&quot;, indicators, extra = TRUE, start = 1995, end = 2015) wdi_co2_raw &lt;- read_csv(here::here(&quot;data&quot;, &quot;wdi_co2.csv&quot;)) Then we clean the data by removing non-country countries and renaming some of the columns. wdi_clean &lt;- wdi_co2_raw %&gt;% filter(region != &quot;Aggregates&quot;) %&gt;% select(iso2c, iso3c, country, year, population = SP.POP.TOTL, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income) 3.7.2 Clean and reshape data Next we’ll do some substantial filtering and reshaping so that we can end up with the rankings of CO2 emissions in 1995 and 2014. I annotate as much as possible below so you can see what’s happening in each step. co2_rankings &lt;- wdi_clean %&gt;% # Get rid of smaller countries filter(population &gt; 200000) %&gt;% # Only look at two years filter(year %in% c(1995, 2014)) %&gt;% # Get rid of all the rows that have missing values in co2_emissions drop_na(co2_emissions) %&gt;% # Look at each year individually and rank countries based on their emissions that year group_by(year) %&gt;% mutate(ranking = rank(co2_emissions)) %&gt;% ungroup() %&gt;% # Only select a handful of columns, mostly just the newly created &quot;ranking&quot; # column and some country identifiers select(iso3c, country, year, region, income, ranking) %&gt;% # Right now the data is tidy and long, but we want to widen it and create # separate columns for emissions in 1995 and in 2014. pivot_wider() will make # new columns based on the existing &quot;year&quot; column (that&#39;s what `names_from` # does), and it will add &quot;rank_&quot; as the prefix, so that the new columns will # be &quot;rank_1995&quot; and &quot;rank_2014&quot;. The values that go in those new columns will # come from the existing &quot;ranking&quot; column pivot_wider(names_from = year, names_prefix = &quot;rank_&quot;, values_from = ranking) %&gt;% # Find the difference in ranking between 2014 and 1995 mutate(rank_diff = rank_2014 - rank_1995) %&gt;% # Remove all rows where there&#39;s a missing value in the rank_diff column drop_na(rank_diff) %&gt;% # Make an indicator variable that is true of the absolute value of the # difference in rankings is greater than 25. 25 is arbitrary here—that just # felt like a big change in rankings mutate(big_change = ifelse(abs(rank_diff) &gt;= 25, TRUE, FALSE)) %&gt;% # Make another indicator variable that indicates if the rank improved by a # lot, worsened by a lot, or didn&#39;t change much. We use the case_when() # function, which is like a fancy version of ifelse() that takes multiple # conditions. This is how it generally works: # # case_when( # some_test ~ value_if_true, # some_other_test ~ value_if_true, # TRUE ~ value_otherwise #) mutate(better_big_change = case_when( rank_diff &lt;= -25 ~ &quot;Rank improved&quot;, rank_diff &gt;= 25 ~ &quot;Rank worsened&quot;, TRUE ~ &quot;Rank changed a little&quot; )) Here’s what that reshaped data looked like before: head(wdi_clean) ## # A tibble: 6 × 9 ## iso2c iso3c country year population co2_emissions gdp_per_cap region income ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AD AND Andorra 1995 63850 6.66 32577. Europe … High … ## 2 AD AND Andorra 1996 64360 7.07 33822. Europe … High … ## 3 AD AND Andorra 1997 64327 7.24 36907. Europe … High … ## 4 AD AND Andorra 1999 64370 7.98 39621. Europe … High … ## 5 AD AND Andorra 2000 65390 8.02 40379. Europe … High … ## 6 AD AND Andorra 2001 67341 7.79 42393. Europe … High … And here’s what it looks like now: head(co2_rankings) ## # A tibble: 6 × 9 ## iso3c country region income rank_1995 rank_2014 rank_diff big_change ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 ARE United Arab Emir… Middl… High … 170 176 6 FALSE ## 2 AFG Afghanistan South… Low i… 9 21 12 FALSE ## 3 ALB Albania Europ… Upper… 54 79 25 TRUE ## 4 ARM Armenia Europ… Upper… 71 77 6 FALSE ## 5 AGO Angola Sub-S… Lower… 59 69 10 FALSE ## 6 ARG Argentina Latin… Upper… 104 121 17 FALSE ## # … with 1 more variable: better_big_change &lt;chr&gt; 3.7.3 Plot the data and annotate I use IBM Plex Sans in this plot. You can download it from Google Fonts. # These three functions make it so all geoms that use text, label, and # label_repel will use IBM Plex Sans as the font. Those layers are *not* # influenced by whatever you include in the base_family argument in something # like theme_bw(), so ordinarily you&#39;d need to specify the font in each # individual annotate(geom = &quot;text&quot;) layer or geom_label() layer, and that&#39;s # tedious! This removes that tediousness. update_geom_defaults(&quot;text&quot;, list(family = &quot;IBM Plex Sans&quot;)) update_geom_defaults(&quot;label&quot;, list(family = &quot;IBM Plex Sans&quot;)) update_geom_defaults(&quot;label_repel&quot;, list(family = &quot;IBM Plex Sans&quot;)) ggplot(co2_rankings, aes(x = rank_1995, y = rank_2014)) + # Add a reference line that goes from the bottom corner to the top corner annotate(geom = &quot;segment&quot;, x = 0, xend = 172, y = 0, yend = 178) + # Add points and color them by the type of change in rankings geom_point(aes(color = better_big_change)) + # Add repelled labels. Only use data where big_change is TRUE. Fill them by # the type of change (so they match the color in geom_point() above) and use # white text geom_label_repel(data = filter(co2_rankings, big_change == TRUE), aes(label = country, fill = better_big_change), color = &quot;white&quot;) + # Add notes about what the outliers mean in the bottom left and top right # corners. These are italicized and light grey. The text in the bottom corner # is justified to the right with hjust = 1, and the text in the top corner is # justified to the left with hjust = 0 annotate(geom = &quot;text&quot;, x = 170, y = 6, label = &quot;Outliers improving&quot;, fontface = &quot;italic&quot;, hjust = 1, color = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = 2, y = 170, label = &quot;Outliers worsening&quot;, fontface = &quot;italic&quot;, hjust = 0, color = &quot;grey50&quot;) + # Add mostly transparent rectangles in the bottom right and top left corners annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 25, ymin = 0, ymax = 25, fill = &quot;#2ECC40&quot;, alpha = 0.25) + annotate(geom = &quot;rect&quot;, xmin = 150, xmax = 178, ymin = 150, ymax = 178, fill = &quot;#FF851B&quot;, alpha = 0.25) + # Add text to define what the rectangles abovee actually mean. The \\n in # &quot;highest\\nemitters&quot; will put a line break in the label annotate(geom = &quot;text&quot;, x = 40, y = 6, label = &quot;Lowest emitters&quot;, hjust = 0, color = &quot;#2ECC40&quot;) + annotate(geom = &quot;text&quot;, x = 162.5, y = 135, label = &quot;Highest\\nemitters&quot;, hjust = 0.5, vjust = 1, lineheight = 1, color = &quot;#FF851B&quot;) + # Add arrows between the text and the rectangles. These use the segment geom, # and the arrows are added with the arrow() function, which lets us define the # angle of the arrowhead and the length of the arrowhead pieces. Here we use # 0.5 lines, which is a unit of measurement that ggplot uses internally (think # of how many lines of text fit in the plot). We could also use unit(1, &quot;cm&quot;) # or unit(0.25, &quot;in&quot;) or anything else annotate(geom = &quot;segment&quot;, x = 38, xend = 20, y = 6, yend = 6, color = &quot;#2ECC40&quot;, arrow = arrow(angle = 15, length = unit(0.5, &quot;lines&quot;))) + annotate(geom = &quot;segment&quot;, x = 162.5, xend = 162.5, y = 140, yend = 155, color = &quot;#FF851B&quot;, arrow = arrow(angle = 15, length = unit(0.5, &quot;lines&quot;))) + # Use three different colors for the points scale_color_manual(values = c(&quot;grey50&quot;, &quot;#0074D9&quot;, &quot;#FF4136&quot;)) + # Use two different colors for the filled labels. There are no grey labels, so # we don&#39;t have to specify that color scale_fill_manual(values = c(&quot;#0074D9&quot;, &quot;#FF4136&quot;)) + # Make the x and y axes expand all the way to the edges of the plot area and # add breaks every 25 units from 0 to 175 scale_x_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) + scale_y_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) + # Add labels! There are a couple fancy things here. # 1. In the title we wrap the 2 of CO2 in the HTML &lt;sub&gt;&lt;/sub&gt; tag so that the # number gets subscripted. The only way this will actually get parsed as # HTML is if we tell the plot.title to use element_markdown() in the # theme() function, and element_markdown() comes from the ggtext package. # 2. In the subtitle we bold the two words **improved** and **worsened** using # Markdown asterisks. We also wrap these words with HTML span tags with # inline CSS to specify the color of the text. Like the title, this will # only be processed and parsed as HTML and Markdown if we tell the p # lot.subtitle to use element_markdown() in the theme() function. labs(x = &quot;Rank in 1995&quot;, y = &quot;Rank in 2014&quot;, title = &quot;Changes in CO&lt;sub&gt;2&lt;/sub&gt; emission rankings between 1995 and 2014&quot;, subtitle = &quot;Countries that &lt;span style=&#39;color: #0074D9&#39;&gt;**improved**&lt;/span&gt; or &lt;span style=&#39;color: #FF4136&#39;&gt;**worsened**&lt;/span&gt; more than 25 positions in the rankings highlighted&quot;, caption = &quot;Source: The World Bank.\\nCountries with populations of less than 200,000 excluded.&quot;) + # Turn off the legends for color and fill, since the subtitle includes that guides(color = FALSE, fill = FALSE) + # Use theme_bw() with IBM Plex Sans theme_bw(base_family = &quot;IBM Plex Sans&quot;) + # Tell the title and subtitle to be treated as Markdown/HTML, make the title # 1.6x the size of the base font, and make the subtitle 1.3x the size of the # base font. Also add a little larger margin on the right of the plot so that # the 175 doesn&#39;t get cut off. theme(plot.title = element_markdown(face = &quot;bold&quot;, size = rel(1.6)), plot.subtitle = element_markdown(size = rel(1.3)), plot.margin = unit(c(0.5, 1, 0.5, 0.5), units = &quot;lines&quot;)) Acknowledgments Coding examples from Andrew Heiss Session info devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.2 (2021-11-01) ## os macOS Monterey 12.2.1 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Chicago ## date 2022-03-04 ## pandoc 2.17.1.1 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.1.1) ## bit 4.0.4 2020-08-04 [1] CRAN (R 4.1.1) ## bit64 4.0.5 2020-08-30 [1] CRAN (R 4.1.0) ## bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.1) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.1.1) ## broom * 0.7.12 2022-01-28 [1] CRAN (R 4.1.1) ## bslib 0.3.1 2021-10-06 [1] CRAN (R 4.1.1) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.1.1) ## callr 3.7.0 2021-04-20 [1] CRAN (R 4.1.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.1.0) ## cli 3.2.0 2022-02-14 [1] CRAN (R 4.1.1) ## codetools 0.2-18 2020-11-04 [1] CRAN (R 4.1.2) ## colorspace * 2.0-3 2022-02-21 [1] CRAN (R 4.1.1) ## crayon 1.5.0 2022-02-14 [1] CRAN (R 4.1.1) ## curl 4.3.2 2021-06-23 [1] CRAN (R 4.1.0) ## DBI 1.1.2 2021-12-20 [1] CRAN (R 4.1.1) ## dbplyr 2.1.1 2021-04-06 [1] CRAN (R 4.1.0) ## desc 1.4.0 2021-09-28 [1] CRAN (R 4.1.1) ## devtools * 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## dichromat * 2.0-0 2013-01-24 [1] CRAN (R 4.1.0) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.1.1) ## dplyr * 1.0.8 2022-02-08 [1] CRAN (R 4.1.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) ## emo 0.0.0.9000 2022-01-06 [1] Github (hadley/emo@3f03b11) ## evaluate 0.15 2022-02-18 [1] CRAN (R 4.1.1) ## fansi 1.0.2 2022-01-14 [1] CRAN (R 4.1.1) ## farver 2.1.0 2021-02-28 [1] CRAN (R 4.1.0) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.1.1) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.1.1) ## gapminder * 0.3.0 2017-10-31 [1] CRAN (R 4.1.0) ## generics 0.1.2 2022-01-31 [1] CRAN (R 4.1.1) ## gghalves * 0.1.1 2020-11-08 [1] CRAN (R 4.1.1) ## ggplot2 * 3.3.5 2021-06-25 [1] CRAN (R 4.1.1) ## ggrepel * 0.9.1 2021-01-15 [1] CRAN (R 4.1.1) ## ggridges * 0.5.3 2021-01-08 [1] CRAN (R 4.1.1) ## ggtext * 0.1.1 2020-12-17 [1] CRAN (R 4.1.1) ## ggthemes * 4.2.4 2021-01-20 [1] CRAN (R 4.1.0) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.1.1) ## gridtext 0.1.4 2020-12-10 [1] CRAN (R 4.1.0) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.1.1) ## haven 2.4.3 2021-08-04 [1] CRAN (R 4.1.1) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.1.0) ## highr 0.9 2021-04-16 [1] CRAN (R 4.1.0) ## hms 1.1.1 2021-09-26 [1] CRAN (R 4.1.1) ## htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.1) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.1.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) ## jsonlite 1.8.0 2022-02-22 [1] CRAN (R 4.1.1) ## knitr * 1.37 2021-12-16 [1] CRAN (R 4.1.1) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.1.0) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.1.1) ## lubridate * 1.8.0 2021-10-07 [1] CRAN (R 4.1.1) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.1.1) ## markdown 1.1 2019-08-07 [1] CRAN (R 4.1.0) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.1.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.1.0) ## patchwork * 1.1.1 2020-12-17 [1] CRAN (R 4.1.1) ## pillar 1.7.0 2022-02-01 [1] CRAN (R 4.1.1) ## pkgbuild 1.3.1 2021-12-20 [1] CRAN (R 4.1.1) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.1.0) ## pkgload 1.2.4 2021-11-30 [1] CRAN (R 4.1.1) ## plyr 1.8.6 2020-03-03 [1] CRAN (R 4.1.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.1.0) ## processx 3.5.2 2021-04-30 [1] CRAN (R 4.1.0) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.1.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.1) ## RColorBrewer * 1.1-2 2014-12-07 [1] CRAN (R 4.1.0) ## Rcpp 1.0.8 2022-01-13 [1] CRAN (R 4.1.1) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.1.1) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.1.0) ## remotes 2.4.2 2021-11-30 [1] CRAN (R 4.1.1) ## reprex 2.0.1 2021-08-05 [1] CRAN (R 4.1.1) ## RJSONIO 1.3-1.6 2021-09-16 [1] CRAN (R 4.1.1) ## rlang 1.0.1 2022-02-03 [1] CRAN (R 4.1.1) ## rmarkdown 2.11 2021-09-14 [1] CRAN (R 4.1.1) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.1.0) ## rvest 1.0.2 2021-10-16 [1] CRAN (R 4.1.1) ## sass 0.4.0 2021-05-12 [1] CRAN (R 4.1.0) ## scales * 1.1.1 2020-05-11 [1] CRAN (R 4.1.0) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.1.1) ## socviz * 1.2 2020-06-10 [1] CRAN (R 4.1.0) ## stringi 1.7.6 2021-11-29 [1] CRAN (R 4.1.1) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.1.1) ## testthat 3.1.2 2022-01-20 [1] CRAN (R 4.1.1) ## tibble * 3.1.6 2021-11-07 [1] CRAN (R 4.1.1) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.1.1) ## tidyselect 1.1.2 2022-02-21 [1] CRAN (R 4.1.1) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.1.0) ## tzdb 0.2.0 2021-10-27 [1] CRAN (R 4.1.1) ## usethis * 2.1.5 2021-12-09 [1] CRAN (R 4.1.1) ## utf8 1.2.2 2021-07-24 [1] CRAN (R 4.1.0) ## vctrs 0.3.8 2021-04-29 [1] CRAN (R 4.1.0) ## viridisLite 0.4.0 2021-04-13 [1] CRAN (R 4.1.0) ## vroom 1.5.7 2021-11-30 [1] CRAN (R 4.1.1) ## WDI * 2.7.6 2022-02-25 [1] CRAN (R 4.1.1) ## withr 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## xfun 0.29 2021-12-14 [1] CRAN (R 4.1.1) ## xml2 1.3.3 2021-11-30 [1] CRAN (R 4.1.1) ## yaml 2.3.5 2022-02-21 [1] CRAN (R 4.1.1) ## ## [1] /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── References "],["geoviz.html", "Day 4 Geospatial visualizations Learning objectives Assigned readings 4.1 Introduction to geospatial visualization 4.2 Drawing raster maps with ggmap 4.3 Exercise: Chicago 311 data 4.4 Importing spatial data files using sf 4.5 File formats 4.6 Simple features 4.7 Simple features in R 4.8 Drawing vector maps with sf and ggplot2 4.9 Practice drawing vector maps 4.10 Selecting optimal color palettes Session info", " Day 4 Geospatial visualizations library(tidyverse) library(sf) library(ggmap) library(rnaturalearth) library(RColorBrewer) library(patchwork) library(tidycensus) library(viridis) library(here) # useful on MacOS to speed up rendering of geom_sf() objects if (!identical(getOption(&quot;bitmapType&quot;), &quot;cairo&quot;) &amp;&amp; isTRUE(capabilities()[[&quot;cairo&quot;]])) { options(bitmapType = &quot;cairo&quot;) } Learning objectives 4.0.1 Morning Introduce the major components of a geospatial visualization Identify how to draw raster maps using ggmaps and get_map() Practice generating raster maps 4.0.2 Afternoon Define shapefiles and import spatial data using the sf package Draw maps using ggplot2 and geom_sf() Change coordinate systems Generate appropriate color palettes to visualize additional dimensions of data Assigned readings Chapter 7, Healy (2018) - accessible via the book’s website 4.1 Introduction to geospatial visualization Geospatial visualizations are one of the earliest forms of information visualizations. They were used historically for navigation and were essential tools before the modern technological era of humanity. Data maps were first popularized in the seventeenth century and have grown in complexity and detail since then. Consider Google Maps, the sheer volume of data depicted, and the analytical pathways available to its users. Of course geospatial data visualizations do not require computational skills to generate. 4.1.1 John Snow and the Broad Street water pump Figure 1.6: Original map made by John Snow in 1854. Cholera cases are highlighted in black. Source: Wikipedia. In the nineteenth century the theory of bacteria was not widely accepted by the medical community or the public.12 A mother washed her baby’s diaper in a well in 1854 in London, sparking an outbreak of cholera, an intestinal disease that causes vomiting, diarrhea, and eventually death. This disease had presented itself previously in London but its cause was still unknown. Dr. John Snow lived in Soho, the suburb of London where the disease manifested in 1854, and wanted to understand how cholera spreads through a population (an early day epidemiologist). Snow recorded the location of individuals who contracted cholera, including their places of residence and employment. He used this information to draw a map of the region, recording the location of individuals who contracted the disease. They seemed to be clustered around the well pump along Broad Street. Snow used this map to deduce the source of the outbreak was the well, observing that almost all of the infected individuals lived near, and drank from, the well. Based on this information, the government removed the handle from the well pump so the public could not draw water from it. As a result, the cholera epidemic ended. 4.1.2 Carte figurative des pertes successives en hommes de l’Armée Française dans la campagne de Russie 1812-1813) Figure 1.7: Charles Minard’s 1869 chart showing the number of men in Napoleon’s 1812 Russian campaign army, their movements, as well as the temperature they encountered on the return path. Source: Wikipedia. Figure 1.8: English translation of Minard’s map. Source: Wikipedia. This illustration is identifed in Edward Tufte’s The Visual Display of Quantitative Information as one of “the best statistical drawings ever created”. It also demonstrates a very important rule of warfare: never invade Russia in the winter. In 1812, Napoleon ruled most of Europe. He wanted to seize control of the British islands, but could not overcome the UK defenses. He decided to impose an embargo to weaken the nation in preparation for invasion, but Russia refused to participate. Angered at this decision, Napoleon launched an invasion of Russia with over 400,000 troops in the summer of 1812. Russia was unable to defeat Napoleon in battle, but instead waged a war of attrition. The Russian army was in near constant retreat, burning or destroying anything of value along the way to deny France usable resources. While Napoleon’s army maintained the military advantage, his lack of food and the emerging European winter decimated his forces. He left France with an army of approximately 422,000 soldiers; he returned to France with just 10,000. Charles Minard’s map is a stunning achievement for his era. It incorporates data across six dimensions to tell the story of Napoleon’s failure. The graph depicts: Size of the army Location in two physical dimensions (latitude and longitude) Direction of the army’s movement Temperature on dates during Napoleon’s retreat What makes this such an effective visualization?13 Forces visual comparisons (colored bands for advancing and retreating) Shows causality (temperature chart) Captures multivariate complexity Integrates text and graphic into a coherent whole (perhaps the first infographic, and done well!) Illustrates high quality content (based on reliable data) Places comparisons adjacent to each other (all on the same page, no jumping back and forth between pages) Mimimalistic in nature (avoids what we will later term “chart junk”) 4.1.3 Designing modern maps Geometric visualizations are used to depict spatial features, and with the incorporation of data reveal additional attributes and information. The main features of a map are defined by its scale (the proportion between distances and sizes on the map), its projection (how the three-dimensional Earth is represented on a two-dimensional surface), and its symbols (how data is depicted and visualized on the map). 4.1.3.1 Scale Scale defines the proportion between distances and sizes on a map and their actual distances and sizes on Earth. Depending on the total geographic area for which you have data to visualize, you could create a small-scale map or a large-scale map. So for instance, a map of the United States would be considered large-scale: Whereas a map of Hyde Park would be small-scale: The smaller the scale, the easier it is to include additional details in the map. 4.1.3.2 Projection Projection is the process of taking a globe (i.e. a three-dimensional object)14 and visualizing it on a two-dimensional picture. There is no 100% perfect method for doing this, as any projection method will have to distort some features of the map to achieve a two-dimensional representation. There are five properties to consider when defining a projection method: Shape Area Angles Distance Direction Projection methods typically maximize the accuracy of one or two of these properties, but no more. For instance, conformal projections such as the mercator projection preserves shape and local angles and is very useful for sea navigation, but distorts the area of landmasses. The farther away from the equator one travels, the more distorted the size of the region. Another family of projections called equal-area projections preserves area ratios, so that the relative size of areas on a map are proportional to their areas on the Earth. The downside is that equal-area projections tend to distory shapes heavily, so shapes of areas can become distorted. No method can be both conformal and equal-area simultaneously, but some methods such as the Mollweide projection achieve a trade-off between these sets of characteristics. 4.1.3.3 Symbols Different types of symbols are used to denote different types of information on a spatial visualization. For instance, consider the following map of Hyde Park: Line are used to indicate roadways Fill is used to indicate type of land (grassland, water, urban, etc.) Symbols/shapes are used to locate buildings Text labels are used to indicate geographic locations Data maps do not just encode geographic features on the visualization. They also plot quantitative and qualitative data on the mapping surface itself. Minard’s drawing was not just of geographic coordinates and features - it also visualizes quantitative data such as troop deaths and temperature. Different symbols are used depending on the type of data you seek to visualize. 4.2 Drawing raster maps with ggmap ggmap is a package for R that retrieves raster map tiles from online mapping services like Google Maps and plots them using the ggplot2 framework. The map tiles are raster because they are static image files generated previously by the mapping service. You do not need any data files containing information on things like scale, projection, boundaries, etc. because that information is already created by the map tile. This severely limits your ability to redraw or change the appearance of the geographic map, however the tradeoff means you can immediately focus on incorporating additional data into the map. Google has changed its API requirements, and ggmap users are now required to provide an API key and enable billing. I would not recommend trying to use Google Maps to obtain map images. The code below would work for you, but Google now charges you each time you obtain a map image. Stick to the other providers such as Stamen Maps. 4.2.1 Obtain map images ggmap supports open-source map providers such as OpenStreetMap and Stamen Maps, as well as the proprietary Google Maps. Obtaining map tiles requires use of the get_map() function. There are two formats for specifying the mapping region you wish to obtain: Bounding box Center/zoom 4.2.2 Specifying map regions 4.2.2.1 Bounding box Bounding box requires the user to specify the four corners of the box defining the map region. For instance, to obtain a map of Chicago using Stamen Maps: # store bounding box coordinates chi_bb &lt;- c(left = -87.936287, bottom = 41.679835, right = -87.447052, top = 42.000835) chicago_stamen &lt;- get_stamenmap(bbox = chi_bb, zoom = 11) chicago_stamen ## 627x712 terrain map image from Stamen Maps. ## See ?ggmap to plot it. To view the map, use ggmap(): ggmap(chicago_stamen) The zoom argument in get_stamenmap() controls the level of detail in the map. The larger the number, the greater the detail. get_stamenmap(bbox = chi_bb, zoom = 12) %&gt;% ggmap() The smaller the number, the lesser the detail. get_stamenmap(bbox = chi_bb, zoom = 10) %&gt;% ggmap() Trial and error will help you decide on the appropriate level of detail depending on what data you need to visualize on the map. Use bboxfinder.com to determine the exact longitude/latitude coordinates for the bounding box you wish to obtain. 4.2.2.2 Center/zoom While Stamen Maps and OpenStreetMap require the bounding box format for obtaining map tiles and allow you to increase or decrease the level of detail within a single bounding box, Google Maps requires specifying the center coordinate of the map (a single longitude/latitude location) and the level of zoom or detail. zoom is an integer value from 3 (continent) to 21 (building). This means the level of detail is hardcoded to the size of the mapping region. The default zoom level is 10. # store center coordinate chi_center &lt;- c(lon = -87.65, lat = 41.855) chicago_google &lt;- get_googlemap(center = chi_center) ggmap(chicago_google) get_googlemap(center = chi_center, zoom = 12) %&gt;% ggmap() get_googlemap(center = chi_center, zoom = 8) %&gt;% ggmap() Use Find Latitude and Longitude to get the exact GPS coordinates of the center location. 4.2.3 Types of map tiles Each map tile provider offers a range of different types of maps depending on the background you want for the map. Stamen Maps offers several different types: Google Maps is a bit more limited, but still offers a few major types: See the documentation for the get_*map() function for the exact code necessary to get each type of map. get_map() is a wrapper that automatically queries Google Maps, OpenStreetMap, or Stamen Maps depending on the function arguments and inputs. While useful, it also combines all the different arguments of get_googlemap(), get_stamenmap(), and getopenstreetmap() and can become a bit jumbled. Use at your own risk. 4.2.4 Import crime data Now that we can obtain map tiles and draw them using ggmap(), let’s explore how to add data to the map. The city of Chicago has an excellent data portal publishing a large volume of public records. Here we’ll look at crime data from 2017.15 I previously downloaded a .csv file containing all the records, which I import using read_csv(): If you are copying-and-pasting code from this demonstration, change this line of code to crimes &lt;- read_csv(\"https://cfss.uchicago.edu/data/Crimes_-_2017.csv\") to download the file from the course website. crimes &lt;- here(&quot;data&quot;, &quot;Crimes_-_2017.csv&quot;) %&gt;% read_csv() glimpse(crimes) ## Rows: 267,345 ## Columns: 22 ## $ ID &lt;dbl&gt; 11094370, 11118031, 11134189, 11156462, 1116487… ## $ `Case Number` &lt;chr&gt; &quot;JA440032&quot;, &quot;JA470589&quot;, &quot;JA491697&quot;, &quot;JA521389&quot;,… ## $ Date &lt;chr&gt; &quot;09/21/2017 12:15:00 AM&quot;, &quot;10/12/2017 07:14:00 … ## $ Block &lt;chr&gt; &quot;072XX N CALIFORNIA AVE&quot;, &quot;055XX W GRAND AVE&quot;, … ## $ IUCR &lt;chr&gt; &quot;1122&quot;, &quot;1345&quot;, &quot;4651&quot;, &quot;1110&quot;, &quot;0265&quot;, &quot;143A&quot;,… ## $ `Primary Type` &lt;chr&gt; &quot;DECEPTIVE PRACTICE&quot;, &quot;CRIMINAL DAMAGE&quot;, &quot;OTHER… ## $ Description &lt;chr&gt; &quot;COUNTERFEIT CHECK&quot;, &quot;TO CITY OF CHICAGO PROPER… ## $ `Location Description` &lt;chr&gt; &quot;CURRENCY EXCHANGE&quot;, &quot;JAIL / LOCK-UP FACILITY&quot;,… ## $ Arrest &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… ## $ Domestic &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ Beat &lt;chr&gt; &quot;2411&quot;, &quot;2515&quot;, &quot;0922&quot;, &quot;2514&quot;, &quot;1221&quot;, &quot;0232&quot;,… ## $ District &lt;chr&gt; &quot;024&quot;, &quot;025&quot;, &quot;009&quot;, &quot;025&quot;, &quot;012&quot;, &quot;002&quot;, &quot;005&quot;… ## $ Ward &lt;dbl&gt; 50, 29, 12, 30, 32, 20, 9, 12, 12, 27, 32, 17, … ## $ `Community Area` &lt;dbl&gt; 2, 19, 58, 19, 24, 40, 49, 30, 30, 23, 24, 44, … ## $ `FBI Code` &lt;chr&gt; &quot;10&quot;, &quot;14&quot;, &quot;26&quot;, &quot;11&quot;, &quot;02&quot;, &quot;15&quot;, &quot;03&quot;, &quot;06&quot;,… ## $ `X Coordinate` &lt;dbl&gt; 1156443, 1138788, 1159425, 1138653, 1161264, 11… ## $ `Y Coordinate` &lt;dbl&gt; 1947707, 1913480, 1875711, 1920720, 1905292, 18… ## $ Year &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017,… ## $ `Updated On` &lt;chr&gt; &quot;03/01/2018 03:52:35 PM&quot;, &quot;03/01/2018 03:52:35 … ## $ Latitude &lt;dbl&gt; 42.0, 41.9, 41.8, 41.9, 41.9, 41.8, 41.7, 41.8,… ## $ Longitude &lt;dbl&gt; -87.7, -87.8, -87.7, -87.8, -87.7, -87.6, -87.6… ## $ Location &lt;chr&gt; &quot;(42.012293397, -87.699714109)&quot;, &quot;(41.918711651… Each row of the data frame is a single reported incident of crime. Geographic location is encoded in several ways, though most importantly for us the exact longitude and latitude of the incident is encoded in the Longitude and Latitude columns respectively. 4.2.5 Plot high-level map of crime Let’s start with a simple high-level overview of reported crime in Chicago. First we need a map for the entire city. chicago &lt;- chicago_stamen ggmap(chicago) 4.2.6 Using geom_point() Since each row is a single reported incident of crime, we could use geom_point() to map the location of every crime in the dataset. Because ggmap() uses the map tiles (here, defined by chicago) as the basic input, we specify data and mapping inside of geom_point(), rather than inside ggplot(): ggmap(chicago) + geom_point(data = crimes, mapping = aes(x = Longitude, y = Latitude)) What went wrong? All we get is a sea of black. nrow(crimes) ## [1] 267345 Oh yeah. There were 267345 reported incidents of crime in the city. Each incident is represented by a dot on the map. How can we make this map more usable? One option is to decrease the size and increase the transparancy of each data point so dense clusters of crime become apparent: ggmap(chicago) + geom_point(data = crimes, aes(x = Longitude, y = Latitude), size = .25, alpha = .01) Better, but still not quite as useful as it could be. 4.2.7 Using stat_density_2d() Instead of relying on geom_point() and plotting the raw data, a better approach is to create a heatmap. More precisely, this will be a two-dimensional kernel density estimation (KDE). In this context, KDE will take all the raw data (i.e. reported incidents of crime) and convert it into a smoothed plot showing geographic concentrations of crime. The core function in ggplot2 to generate this kind of plot is geom_density_2d(): ggmap(chicago) + geom_density_2d(data = crimes, aes(x = Longitude, y = Latitude)) By default, geom_density_2d() draws a contour plot with lines of constant value. That is, each line represents approximately the same frequency of crime all along that specific line. Contour plots are frequently used in maps (known as topographic maps) to denote elevation. Figure 4.1: The Cadillac Mountains. Source: US Geological Survey. Rather than drawing lines, instead we can fill in the graph so that we use the fill aesthetic to draw bands of crime density. To do that, we use the related function stat_density_2d(): ggmap(chicago) + stat_density_2d(data = crimes, aes(x = Longitude, y = Latitude, fill = stat(level)), geom = &quot;polygon&quot;) Note the two new arguments: geom = \"polygon\" - change the geometric object to be drawn from a density_2d geom to a polygon geom fill = stat(level) - the value for the fill aesthetic is the level calculated within stat_density_2d(), which we access using the stat() notation. This is an improvement, but we can adjust some additional settings to make the graph visually more useful. Specifically, Increase the number of bins, or unique bands of color allowed on the graph Make the heatmap semi-transparent using alpha so we can still view the underlying map Change the color palette to better distinguish between high and low crime areas. Here I use brewer.pal() from the RColorBrewer package to create a custom color palette using reds and yellows. ggmap(chicago) + stat_density_2d(data = crimes, aes(x = Longitude, y = Latitude, fill = stat(level)), alpha = .2, bins = 25, geom = &quot;polygon&quot;) + scale_fill_gradientn(colors = brewer.pal(7, &quot;YlOrRd&quot;)) From this map, a couple trends are noticeable: The downtown region has the highest crime incidence rate. Not surprising given its population density during the workday. There are clusters of crime on the south and west sides. Also not surprising if you know anything about the city of Chicago. 4.2.8 Looking for variation Because ggmap is built on ggplot2, we can use the core features of ggplot2 to modify the graph. One major feature is faceting. Let’s focus our analysis on four types of crimes with similar frequency of reported incidents16 and facet by type of crime: ggmap(chicago) + stat_density_2d(data = crimes %&gt;% filter(`Primary Type` %in% c(&quot;BURGLARY&quot;, &quot;MOTOR VEHICLE THEFT&quot;, &quot;NARCOTICS&quot;, &quot;ROBBERY&quot;)), aes(x = Longitude, y = Latitude, fill = stat(level)), alpha = .4, bins = 10, geom = &quot;polygon&quot;) + scale_fill_gradientn(colors = brewer.pal(7, &quot;YlOrRd&quot;)) + facet_wrap(~ `Primary Type`) There is a large difference in the geographic density of narcotics crimes relative to the other catgories. While burglaries, motor vehicle thefts, and robberies are reasonably prevalent all across the city, the vast majority of narcotics crimes occur in the west and south sides of the city. 4.2.9 Locations of murders While geom_point() was not appropriate for graphing a large number of observations in a dense geographic location, it does work rather well for less dense areas. Now let’s limit our analysis strictly to reported incidents of homicide in 2017. (homicides &lt;- crimes %&gt;% filter(`Primary Type` == &quot;HOMICIDE&quot;)) ## # A tibble: 671 × 22 ## ID `Case Number` Date Block IUCR `Primary Type` Description ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 23128 JA149608 02/11/2017 07:… 001X… 0110 HOMICIDE FIRST DEGR… ## 2 23851 JA530946 11/30/2017 11:… 088X… 0110 HOMICIDE FIRST DEGR… ## 3 23355 JA302423 06/11/2017 06:… 047X… 0110 HOMICIDE FIRST DEGR… ## 4 23379 JA312425 06/18/2017 04:… 006X… 0110 HOMICIDE FIRST DEGR… ## 5 23673 JA490016 10/28/2017 10:… 048X… 0110 HOMICIDE FIRST DEGR… ## 6 23224 JA210752 04/03/2017 12:… 013X… 0110 HOMICIDE FIRST DEGR… ## 7 23627 JA461918 10/07/2017 11:… 018X… 0110 HOMICIDE FIRST DEGR… ## 8 23628 JA461918 10/07/2017 11:… 018X… 0110 HOMICIDE FIRST DEGR… ## 9 10836558 JA138326 02/01/2017 06:… 013X… 0142 HOMICIDE RECKLESS H… ## 10 23477 JA364517 07/26/2017 05:… 047X… 0110 HOMICIDE FIRST DEGR… ## # … with 661 more rows, and 15 more variables: `Location Description` &lt;chr&gt;, ## # Arrest &lt;lgl&gt;, Domestic &lt;lgl&gt;, Beat &lt;chr&gt;, District &lt;chr&gt;, Ward &lt;dbl&gt;, ## # `Community Area` &lt;dbl&gt;, `FBI Code` &lt;chr&gt;, `X Coordinate` &lt;dbl&gt;, ## # `Y Coordinate` &lt;dbl&gt;, Year &lt;dbl&gt;, `Updated On` &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, Location &lt;chr&gt; We can draw a map of the city with all homicides indicated on the map using geom_point(): ggmap(chicago) + geom_point(data = homicides, mapping = aes(x = Longitude, y = Latitude), size = 1) Compared to our previous overviews, few if any homicides are reported downtown. We can also narrow down the geographic location to map specific neighborhoods in Chicago. First we obtain map tiles for those specific regions. Here we’ll examine North Lawndale and Kenwood. # North Lawndale is the highest homicides in 2017 # Compare to Kenwood north_lawndale_bb &lt;- c( left = -87.749047, bottom = 41.840185, right = -87.687893, top = 41.879850 ) north_lawndale &lt;- get_stamenmap(bbox = north_lawndale_bb, zoom = 14) kenwood_bb &lt;- c( left = -87.613113, bottom = 41.799215, right = -87.582536, top = 41.819064 ) kenwood &lt;- get_stamenmap(bbox = kenwood_bb, zoom = 15) ggmap(north_lawndale) ggmap(kenwood) To plot homicides specifically in these neighborhoods, change ggmap(chicago) to the appropriate map tile: ggmap(north_lawndale) + geom_point(data = homicides, aes(x = Longitude, y = Latitude)) ggmap(kenwood) + geom_point(data = homicides, aes(x = Longitude, y = Latitude)) North Lawndale had the most reported homicides in 2017, whereas Kenwood had only a handful. And even though homicides contained data for homicides across the entire city, ggmap() automatically cropped the graph to keep just the homicides that occurred within the bounding box. All the other aesthetic customizations of geom_point() work with ggmap. So we could expand these neighborhood maps to include all violent crime categories17 and distinguish each type by color: (violent &lt;- crimes %&gt;% filter(`Primary Type` %in% c(&quot;HOMICIDE&quot;, &quot;CRIM SEXUAL ASSAULT&quot;, &quot;ROBBERY&quot;))) ## # A tibble: 14,146 × 22 ## ID `Case Number` Date Block IUCR `Primary Type` Description ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 11164874 JA531910 12/01/2017 06:… 022X… 0265 CRIM SEXUAL A… AGGRAVATED… ## 2 10995008 JA322389 06/25/2017 07:… 003X… 031A ROBBERY ARMED: HAN… ## 3 11175304 JA545986 12/11/2017 07:… 007X… 031A ROBBERY ARMED: HAN… ## 4 11175934 JA546734 12/12/2017 06:… 007X… 031A ROBBERY ARMED: HAN… ## 5 11227287 JB147188 10/08/2017 03:… 092X… 0281 CRIM SEXUAL A… NON-AGGRAV… ## 6 11227634 JB147599 08/26/2017 10:… 001X… 0281 CRIM SEXUAL A… NON-AGGRAV… ## 7 23128 JA149608 02/11/2017 07:… 001X… 0110 HOMICIDE FIRST DEGR… ## 8 11043709 JA378592 08/05/2017 03:… 038X… 0313 ROBBERY ARMED: OTH… ## 9 11170225 JA538651 12/06/2017 09:… 092X… 031A ROBBERY ARMED: HAN… ## 10 11228964 JB149656 12/24/2017 02:… 005X… 0330 ROBBERY AGGRAVATED ## # … with 14,136 more rows, and 15 more variables: `Location Description` &lt;chr&gt;, ## # Arrest &lt;lgl&gt;, Domestic &lt;lgl&gt;, Beat &lt;chr&gt;, District &lt;chr&gt;, Ward &lt;dbl&gt;, ## # `Community Area` &lt;dbl&gt;, `FBI Code` &lt;chr&gt;, `X Coordinate` &lt;dbl&gt;, ## # `Y Coordinate` &lt;dbl&gt;, Year &lt;dbl&gt;, `Updated On` &lt;chr&gt;, Latitude &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, Location &lt;chr&gt; ggmap(north_lawndale) + geom_point(data = violent, aes(x = Longitude, y = Latitude, color = `Primary Type`)) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) ggmap(kenwood) + geom_point(data = violent, aes(x = Longitude, y = Latitude, color = `Primary Type`)) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Dark2&quot;) 4.3 Exercise: Chicago 311 data The city of Chicago has an excellent data portal publishing a large volume of public records. Here we’ll look at a subset of the 311 service requests. I used RSocrata and the data portal’s API to retrieve a portion of the data set. Download the necessary data files for the following coding exercises using usethis::use_course(\"css-data-mining-viz/geoviz\"). chi_311 &lt;- read_csv(&quot;data/chi-311.csv&quot;) glimpse(chi_311) ## Rows: 167,552 ## Columns: 8 ## $ sr_number &lt;chr&gt; &quot;SR19-01209373&quot;, &quot;SR19-01129184&quot;, &quot;SR19-01130159&quot;, &quot;SR1… ## $ sr_type &lt;chr&gt; &quot;Dead Animal Pick-Up Request&quot;, &quot;Dead Animal Pick-Up Req… ## $ sr_short_code &lt;chr&gt; &quot;SGQ&quot;, &quot;SGQ&quot;, &quot;SGQ&quot;, &quot;SGQ&quot;, &quot;SGQ&quot;, &quot;SGQ&quot;, &quot;SGQ&quot;, &quot;SGQ&quot;,… ## $ created_date &lt;dttm&gt; 2019-03-23 17:13:05, 2019-03-09 01:37:26, 2019-03-09 1… ## $ community_area &lt;dbl&gt; 58, 40, 40, 67, 59, 59, 2, 59, 59, 64, 59, 25, 25, 59, … ## $ ward &lt;dbl&gt; 12, 20, 20, 17, 12, 12, 40, 12, 12, 13, 12, 29, 28, 12,… ## $ latitude &lt;dbl&gt; 41.8, 41.8, 41.8, 41.8, 41.8, 41.8, 42.0, 41.8, 41.8, 4… ## $ longitude &lt;dbl&gt; -87.7, -87.6, -87.6, -87.7, -87.7, -87.7, -87.7, -87.7,… 4.3.1 Visualize the 311 data Obtain map tiles using ggmap for the city of Chicago. Click for the solution # store bounding box coordinates chi_bb &lt;- c(left = -87.936287, bottom = 41.679835, right = -87.447052, top = 42.000835) # retrieve bounding box chicago &lt;- get_stamenmap(bbox = chi_bb, zoom = 11) # plot the raster map ggmap(chicago) Generate a scatterplot of complaints about potholes in streets. Click for the solution # initialize map ggmap(chicago) + # add layer with scatterplot # use alpha to show density of points geom_point(data = filter(chi_311, sr_type == &quot;Pothole in Street Complaint&quot;), mapping = aes(x = longitude, y = latitude), size = .25, alpha = .05) Generate a heatmap of complaints about potholes in streets. Do you see any unusual patterns or clusterings? Click for the solution # initialize the map ggmap(chicago) + # add the heatmap stat_density_2d(data = filter(chi_311, sr_type == &quot;Pothole in Street Complaint&quot;), mapping = aes(x = longitude, y = latitude, fill = stat(level)), alpha = .1, bins = 50, geom = &quot;polygon&quot;) + # customize the color gradient scale_fill_gradientn(colors = brewer.pal(9, &quot;YlOrRd&quot;)) Seems to be clustered on the north side. Also looks to occur along major arterial routes for commuting traffic. Makes sense because they receive the most wear and tear. Obtain map tiles for Hyde Park. Click for the solution # store bounding box coordinates hp_bb &lt;- c(left = -87.608221, bottom = 41.783249, right = -87.577643, top = 41.803038) # retrieve bounding box hyde_park &lt;- get_stamenmap(bbox = hp_bb, zoom = 15) # plot the raster map ggmap(hyde_park) Generate a scatterplot of requests to pick up dead animals in Hyde Park. Click for the solution # initialize the map ggmap(hyde_park) + # add a scatterplot layer geom_point(data = filter(chi_311, sr_type == &quot;Dead Animal Pick-Up Request&quot;), mapping = aes(x = longitude, y = latitude)) 4.4 Importing spatial data files using sf Rather than storing spatial data as raster image files which are not easily modifiable, we can instead store spatial data as vector files. Vector files store the underlying geographical features (e.g. points, lines, polygons) as numerical data which software such as R can import and use to draw a map. There are many popular file formats for storing spatial data. Here we will look at two common file types, shapefiles and GeoJSON. 4.5 File formats 4.5.1 Shapefile Shapefiles are a commonly supported file type for spatial data dating back to the early 1990s. Proprietary software for geographic information systems (GIS) such as ArcGIS pioneered this format and helps maintain its continued usage. A shapefile encodes points, lines, and polygons in geographic space, and is actually a set of files. Shapefiles appear with a .shp extension, sometimes with accompanying files ending in .dbf and .prj. .shp stores the geographic coordinates of the geographic features (e.g. country, state, county) .dbf stores data associated with the geographic features (e.g. unemployment rate, crime rates, percentage of votes cast for Donald Trump) .prj stores information about the projection of the coordinates in the shapefile When importing a shapefile, you need to ensure all the files are in the same folder. For example, here is the structure of the Census Bureau’s 2013 state boundaries shapefile: ## -- cb_2013_us_county_20m.dbf ## -- cb_2013_us_county_20m.prj ## -- cb_2013_us_county_20m.shp ## -- cb_2013_us_county_20m.shp.iso.xml ## -- cb_2013_us_county_20m.shp.xml ## -- cb_2013_us_county_20m.shx ## -- county_20m.ea.iso.xml This is the complete shapefile. If any of these files are missing, you will get an error importing your shapefile: ## Error in CPL_read_ogr(dsn, layer, query, as.character(options), quiet, : Open failed. 4.5.2 GeoJSON GeoJSON is a newer format for encoding a variety of geographical data structures using the JavaScript Object Notation (JSON) file format. JSON formatted data is frequently used in web development and services. We will explore it in more detail when we get to collecting data from the web. An example of a GeoJSON file is below: { &quot;type&quot;: &quot;Feature&quot;, &quot;geometry&quot;: { &quot;type&quot;: &quot;Point&quot;, &quot;coordinates&quot;: [125.6, 10.1] }, &quot;properties&quot;: { &quot;name&quot;: &quot;Dinagat Islands&quot; } } GeoJSON files are plain text files and can contain many different types of geometric features. 4.6 Simple features There are a crap ton of packages for R that allow you to interact with shapefiles and spatial data. Here we will focus on a modern package for reading and transforming spatial data in a tidy format. Simple features or simple feature access refers to a formal standard that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them. The standard is widely implemented in spatial databases (such as PostGIS), commercial GIS (e.g., ESRI ArcGIS) and forms the vector data basis for libraries such as GDAL. A subset of simple features forms the GeoJSON standard. R has well-supported classes for storing spatial data (sp) and interfacing to the above mentioned environments (rgdal, rgeos), but has so far lacked a complete implementation of simple features, making conversions at times convoluted, inefficient or incomplete. The sf package tries to fill this gap. 4.6.1 What is a feature? A feature is a thing or an object in the real world. Often features will consist of a set of features. For instance, a tree can be a feature but a set of trees can form a forest which is itself a feature. Features have geometry describing where on Earth the feature is located. They also have attributes, which describe other properties of the feature. 4.6.2 Dimensions All geometries are composed of points. Points are coordinates in a 2-, 3- or 4-dimensional space. All points in a geometry have the same dimensionality. In addition to X and Y coordinates, there are two optional additional dimensions: a Z coordinate, denoting altitude an M coordinate (rarely used), denoting some measure that is associated with the point, rather than with the feature as a whole (in which case it would be a feature attribute); examples could be time of measurement, or measurement error of the coordinates The four possible cases then are: two-dimensional points refer to x and y, easting and northing, or longitude and latitude, we refer to them as XY three-dimensional points as XYZ three-dimensional points as XYM four-dimensional points as XYZM (the third axis is Z, fourth M) 4.6.3 Simple feature geometry types The following seven simple feature types are the most common, and are for instance the only ones used for GeoJSON: type description POINT zero-dimensional geometry containing a single point LINESTRING sequence of points connected by straight, non-self intersecting line pieces; one-dimensional geometry POLYGON geometry with a positive area (two-dimensional); sequence of points form a closed, non-self intersecting ring; the first ring denotes the exterior ring, zero or more subsequent rings denote holes in this exterior ring MULTIPOINT set of points; a MULTIPOINT is simple if no two Points in the MULTIPOINT are equal MULTILINESTRING set of linestrings MULTIPOLYGON set of polygons GEOMETRYCOLLECTION set of geometries of any type except GEOMETRYCOLLECTION 4.6.4 Coordinate reference system Coordinates can only be placed on the Earth’s surface when their coordinate reference system (CRS) is known; this may be an spheroid CRS such as WGS84, a projected, two-dimensional (Cartesian) CRS such as a UTM zone or Web Mercator, or a CRS in three-dimensions, or including time. Similarly, M-coordinates need an attribute reference system, e.g. a measurement unit. 4.7 Simple features in R sf stores simple features as basic R data structures (lists, matrix, vectors, etc.). The typical data structure stores geometric and feature attributes as a data frame with one row per feature. However since feature geometries are not single-valued, they are put in a list-column with each list element holding the simple feature geometry of that feature. 4.7.1 Importing spatial data using sf st_read() imports a spatial data file and converts it to a simple feature data frame. Here we import a shapefile containing the spatial boundaries of each community area in the city of Chicago. chi_shape &lt;- here(&quot;data/Boundaries - Community Areas (current)/geo_export_328cdcbf-33ba-4997-8ce8-90953c6fec19.shp&quot;) %&gt;% st_read() ## Reading layer `geo_export_328cdcbf-33ba-4997-8ce8-90953c6fec19&#39; from data source `/Users/soltoffbc/Projects/Data Visualization/course-notes/data/Boundaries - Community Areas (current)/geo_export_328cdcbf-33ba-4997-8ce8-90953c6fec19.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 77 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.9 ymin: 41.6 xmax: -87.5 ymax: 42 ## Geodetic CRS: WGS84(DD) The short report printed gives the file name, mentions that there are 77 features (records, represented as rows) and 10 fields (attributes, represented as columns), states that the spatial data file is a MULTIPOLYGON, provides the bounding box coordinates, and identifies the projection method (which we will discuss later). If we print the first rows of chi_shape: chi_shape ## Simple feature collection with 77 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.9 ymin: 41.6 xmax: -87.5 ymax: 42 ## Geodetic CRS: WGS84(DD) ## First 10 features: ## perimeter community shape_len shape_area area comarea area_numbe ## 1 0 DOUGLAS 31027 46004621 0 0 35 ## 2 0 OAKLAND 19566 16913961 0 0 36 ## 3 0 FULLER PARK 25339 19916705 0 0 37 ## 4 0 GRAND BOULEVARD 28197 48492503 0 0 38 ## 5 0 KENWOOD 23325 29071742 0 0 39 ## 6 0 LINCOLN SQUARE 36625 71352328 0 0 4 ## 7 0 WASHINGTON PARK 28175 42373881 0 0 40 ## 8 0 HYDE PARK 29747 45105380 0 0 41 ## 9 0 WOODLAWN 46937 57815180 0 0 42 ## 10 0 ROGERS PARK 34052 51259902 0 0 1 ## area_num_1 comarea_id geometry ## 1 35 0 MULTIPOLYGON (((-87.6 41.8,... ## 2 36 0 MULTIPOLYGON (((-87.6 41.8,... ## 3 37 0 MULTIPOLYGON (((-87.6 41.8,... ## 4 38 0 MULTIPOLYGON (((-87.6 41.8,... ## 5 39 0 MULTIPOLYGON (((-87.6 41.8,... ## 6 4 0 MULTIPOLYGON (((-87.7 42, -... ## 7 40 0 MULTIPOLYGON (((-87.6 41.8,... ## 8 41 0 MULTIPOLYGON (((-87.6 41.8,... ## 9 42 0 MULTIPOLYGON (((-87.6 41.8,... ## 10 1 0 MULTIPOLYGON (((-87.7 42, -... In the output we see: Each row is a simple feature: a single record, or data.frame row, consisting of attributes and geometry The geometry column is a simple feature list-column (an object of class sfc, which is a column in the data.frame) Each value in geometry is a single simple feature geometry (an object of class sfg) We start to recognize the data frame structure. Substantively, community defines the name of the community area for each row. st_read() also works with GeoJSON files. chi_json &lt;- here(&quot;data/Boundaries - Community Areas (current).geojson&quot;) %&gt;% st_read() ## Reading layer `Boundaries - Community Areas (current)&#39; from data source ## `/Users/soltoffbc/Projects/Data Visualization/course-notes/data/Boundaries - Community Areas (current).geojson&#39; ## using driver `GeoJSON&#39; ## Simple feature collection with 77 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.9 ymin: 41.6 xmax: -87.5 ymax: 42 ## Geodetic CRS: WGS 84 chi_json ## Simple feature collection with 77 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.9 ymin: 41.6 xmax: -87.5 ymax: 42 ## Geodetic CRS: WGS 84 ## First 10 features: ## community area shape_area perimeter area_num_1 area_numbe ## 1 DOUGLAS 0 46004621.1581 0 35 35 ## 2 OAKLAND 0 16913961.0408 0 36 36 ## 3 FULLER PARK 0 19916704.8692 0 37 37 ## 4 GRAND BOULEVARD 0 48492503.1554 0 38 38 ## 5 KENWOOD 0 29071741.9283 0 39 39 ## 6 LINCOLN SQUARE 0 71352328.2399 0 4 4 ## 7 WASHINGTON PARK 0 42373881.4842 0 40 40 ## 8 HYDE PARK 0 45105380.1732 0 41 41 ## 9 WOODLAWN 0 57815179.512 0 42 42 ## 10 ROGERS PARK 0 51259902.4506 0 1 1 ## comarea_id comarea shape_len geometry ## 1 0 0 31027.0545098 MULTIPOLYGON (((-87.6 41.8,... ## 2 0 0 19565.5061533 MULTIPOLYGON (((-87.6 41.8,... ## 3 0 0 25339.0897503 MULTIPOLYGON (((-87.6 41.8,... ## 4 0 0 28196.8371573 MULTIPOLYGON (((-87.6 41.8,... ## 5 0 0 23325.1679062 MULTIPOLYGON (((-87.6 41.8,... ## 6 0 0 36624.6030848 MULTIPOLYGON (((-87.7 42, -... ## 7 0 0 28175.3160866 MULTIPOLYGON (((-87.6 41.8,... ## 8 0 0 29746.7082016 MULTIPOLYGON (((-87.6 41.8,... ## 9 0 0 46936.9592443 MULTIPOLYGON (((-87.6 41.8,... ## 10 0 0 34052.3975757 MULTIPOLYGON (((-87.7 42, -... 4.8 Drawing vector maps with sf and ggplot2 Unlike raster image maps, vector maps require you to obtain spatial data files which contain detailed information necessary to draw all the components of a map (e.g. points, lines, polygons). Once you successfully import that data into R, ggplot2 works with simple features data frames to easily generate geospatial visualizations using all the core elements and approaches of ggplot(). 4.8.1 Import USA state boundaries First we will import a spatial data file containing the boundaries of all 50 states in the United States18 using sf::st_read(): usa &lt;- here(&quot;data&quot;, &quot;census_bureau&quot;, &quot;cb_2013_us_state_20m&quot;, &quot;cb_2013_us_state_20m.shp&quot;) %&gt;% st_read() ## Reading layer `cb_2013_us_state_20m&#39; from data source ## `/Users/soltoffbc/Projects/Data Visualization/course-notes/data/census_bureau/cb_2013_us_state_20m/cb_2013_us_state_20m.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 52 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -179 ymin: 17.9 xmax: 180 ymax: 71.4 ## Geodetic CRS: NAD83 4.8.2 Draw the boundaries ggplot2 contains a geometric object specifically for simple feature objects called geom_sf(). This works reasonably well when you need to draw polygons, like our state boundaries. Support for simple features in ggplot2 is under active development, so you may not find adequate support for plotting line or point features. To draw the map, we pass the simple features data frame as the data argument. ggplot(data = usa) + geom_sf() Because simple features data frames are standardized with the geometry column always containing information on the geographic coordinates of the features, we do not need to specify additional parameters for aes(). Notice a problem with the map above: it wastes a lot of space. This is caused by the presence of Alaska and Hawaii in the dataset. The Aleutian Islands cross the the 180th meridian, requiring the map to show the Eastern hemisphere. Likewise, Hawaii is substantially distant from the continental United States. 4.8.2.1 Plot a subset of a map One solution is to plot just the lower 48 states. That is, exclude Alaska and Hawaii, as well as DC and Puerto Rico.19 Because simple features data frames contain one row per feature and in this example a feature is defined as a state, we can use filter() from dplyr to exclude these four states/territories. (usa_48 &lt;- usa %&gt;% filter(!(NAME %in% c(&quot;Alaska&quot;, &quot;District of Columbia&quot;, &quot;Hawaii&quot;, &quot;Puerto Rico&quot;)))) ## Simple feature collection with 48 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -125 ymin: 24.5 xmax: -66.9 ymax: 49.4 ## Geodetic CRS: NAD83 ## First 10 features: ## STATEFP STATENS AFFGEOID GEOID STUSPS NAME LSAD ALAND AWATER ## 1 01 01779775 0400000US01 01 AL Alabama 00 1.31e+11 4.59e+09 ## 2 05 00068085 0400000US05 05 AR Arkansas 00 1.35e+11 2.96e+09 ## 3 06 01779778 0400000US06 06 CA California 00 4.03e+11 2.05e+10 ## 4 09 01779780 0400000US09 09 CT Connecticut 00 1.25e+10 1.82e+09 ## 5 12 00294478 0400000US12 12 FL Florida 00 1.39e+11 3.14e+10 ## 6 13 01705317 0400000US13 13 GA Georgia 00 1.49e+11 4.95e+09 ## 7 16 01779783 0400000US16 16 ID Idaho 00 2.14e+11 2.40e+09 ## 8 17 01779784 0400000US17 17 IL Illinois 00 1.44e+11 6.20e+09 ## 9 18 00448508 0400000US18 18 IN Indiana 00 9.28e+10 1.54e+09 ## 10 20 00481813 0400000US20 20 KS Kansas 00 2.12e+11 1.35e+09 ## geometry ## 1 MULTIPOLYGON (((-88.3 30.2,... ## 2 MULTIPOLYGON (((-94.6 36.5,... ## 3 MULTIPOLYGON (((-119 33.5, ... ## 4 MULTIPOLYGON (((-73.7 41.1,... ## 5 MULTIPOLYGON (((-80.7 24.9,... ## 6 MULTIPOLYGON (((-85.6 35, -... ## 7 MULTIPOLYGON (((-117 44.4, ... ## 8 MULTIPOLYGON (((-91.5 40.2,... ## 9 MULTIPOLYGON (((-88.1 37.9,... ## 10 MULTIPOLYGON (((-102 40, -1... ggplot(data = usa_48) + geom_sf() Since the map is a ggplot() object, it can easily be modified like any other ggplot() graph. We could change the color of the map and the borders: ggplot(data = usa_48) + geom_sf(fill = &quot;palegreen&quot;, color = &quot;black&quot;) 4.8.2.2 albersusa Rather than excluding them entirely, most maps of the United States place Alaska and Hawaii as insets to the south of California. Until recently, in R this was an extremely tedious task that required manually changing the latitude and longitude coordinates for these states to place them in the correct location. Fortunately several packages are now available that have already done the work for you. albersusa includes the usa_sf() function which returns a simple features data frame which contains adjusted coordinates for Alaska and Hawaii to plot them with the mainland. It can be installed from GitHub using devtools::install_github(\"hrbrmstr/albersusa\"). library(albersusa) usa_sf() ## Simple feature collection with 51 features and 13 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -125 ymin: 20.6 xmax: -66.9 ymax: 49.4 ## Geodetic CRS: WGS 84 ## First 10 features: ## geo_id fips_state name lsad census_area iso_3166_2 ## 1 0400000US04 04 Arizona 113594 AZ ## 2 0400000US05 05 Arkansas 52035 AR ## 3 0400000US06 06 California 155779 CA ## 4 0400000US08 08 Colorado 103642 CO ## 5 0400000US09 09 Connecticut 4842 CT ## 6 0400000US11 11 District of Columbia 61 DC ## 7 0400000US13 13 Georgia 57513 GA ## 8 0400000US17 17 Illinois 55519 IL ## 9 0400000US18 18 Indiana 35826 IN ## 10 0400000US22 22 Louisiana 43204 LA ## census pop_estimataes_base pop_2010 pop_2011 pop_2012 pop_2013 pop_2014 ## 1 6392017 6392310 6411999 6472867 6556236 6634997 6731484 ## 2 2915918 2915958 2922297 2938430 2949300 2958765 2966369 ## 3 37253956 37254503 37336011 37701901 38062780 38431393 38802500 ## 4 5029196 5029324 5048575 5119661 5191709 5272086 5355866 ## 5 3574097 3574096 3579345 3590537 3594362 3599341 3596677 ## 6 601723 601767 605210 620427 635040 649111 658893 ## 7 9687653 9688681 9714464 9813201 9919000 9994759 10097343 ## 8 12830632 12831587 12840097 12858725 12873763 12890552 12880580 ## 9 6483802 6484192 6490308 6516560 6537632 6570713 6596855 ## 10 4533372 4533479 4545581 4575972 4604744 4629284 4649676 ## geometry ## 1 MULTIPOLYGON (((-113 37, -1... ## 2 MULTIPOLYGON (((-94 33, -94... ## 3 MULTIPOLYGON (((-120 34, -1... ## 4 MULTIPOLYGON (((-107 41, -1... ## 5 MULTIPOLYGON (((-72.4 42, -... ## 6 MULTIPOLYGON (((-77 38.8, -... ## 7 MULTIPOLYGON (((-84.8 35, -... ## 8 MULTIPOLYGON (((-89.4 42.5,... ## 9 MULTIPOLYGON (((-84.8 40.4,... ## 10 MULTIPOLYGON (((-88.9 29.8,... ggplot(data = usa_sf()) + geom_sf() 4.8.3 Add data to the map Region boundaries serve as the background in geospatial data visualization - so now we need to add data. Some types of geographic data (points and symbols) are overlaid on top of the boundaries, whereas other data (fill) are incorporated into the region layer itself. 4.8.3.1 Points Let’s use our usa_48 map data to add some points. The airports data frame in the nycflights13 package includes geographic info on airports in the United States. library(nycflights13) airports ## # A tibble: 1,458 × 8 ## faa name lat lon alt tz dst tzone ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 04G Lansdowne Airport 41.1 -80.6 1044 -5 A America/… ## 2 06A Moton Field Municipal Airport 32.5 -85.7 264 -6 A America/… ## 3 06C Schaumburg Regional 42.0 -88.1 801 -6 A America/… ## 4 06N Randall Airport 41.4 -74.4 523 -5 A America/… ## 5 09J Jekyll Island Airport 31.1 -81.4 11 -5 A America/… ## 6 0A9 Elizabethton Municipal Airport 36.4 -82.2 1593 -5 A America/… ## 7 0G6 Williams County Airport 41.5 -84.5 730 -5 A America/… ## 8 0G7 Finger Lakes Regional Airport 42.9 -76.8 492 -5 A America/… ## 9 0P2 Shoestring Aviation Airfield 39.8 -76.6 1000 -5 U America/… ## 10 0S9 Jefferson County Intl 48.1 -123. 108 -8 A America/… ## # … with 1,448 more rows Each airport has it’s geographic location encoded through lat and lon. To draw these points on the map, basically we draw a scatterplot with x = lon and y = lat. In fact we could simply do that: ggplot(airports, aes(lon, lat)) + geom_point() Let’s overlay it with the mapped state borders: ggplot(data = usa_48) + geom_sf() + geom_point(data = airports, aes(x = lon, y = lat), shape = 1) Slight problem. We have airports listed outside of the continental United States. There are a couple ways to rectify this. Unfortunately airports does not include a variable identifying state so the filter() operation is not that simple. The easiest solution is to crop the limits of the graph using coord_sf() to only show the mainland: ggplot(data = usa_48) + geom_sf() + geom_point(data = airports, aes(x = lon, y = lat), shape = 1) + coord_sf(xlim = c(-130, -60), ylim = c(20, 50)) Alternatively, we can use st_as_sf() to convert airports to a simple features data frame. airports_sf &lt;- st_as_sf(airports, coords = c(&quot;lon&quot;, &quot;lat&quot;)) st_crs(airports_sf) &lt;- 4326 # set the coordinate reference system airports_sf ## Simple feature collection with 1458 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -177 ymin: 19.7 xmax: 174 ymax: 72.3 ## Geodetic CRS: WGS 84 ## # A tibble: 1,458 × 7 ## faa name alt tz dst tzone geometry ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;POINT [°]&gt; ## 1 04G Lansdowne Airport 1044 -5 A Amer… (-80.6 41.1) ## 2 06A Moton Field Municipa… 264 -6 A Amer… (-85.7 32.5) ## 3 06C Schaumburg Regional 801 -6 A Amer… (-88.1 42) ## 4 06N Randall Airport 523 -5 A Amer… (-74.4 41.4) ## 5 09J Jekyll Island Airport 11 -5 A Amer… (-81.4 31.1) ## 6 0A9 Elizabethton Municip… 1593 -5 A Amer… (-82.2 36.4) ## 7 0G6 Williams County Airp… 730 -5 A Amer… (-84.5 41.5) ## 8 0G7 Finger Lakes Regiona… 492 -5 A Amer… (-76.8 42.9) ## 9 0P2 Shoestring Aviation … 1000 -5 U Amer… (-76.6 39.8) ## 10 0S9 Jefferson County Intl 108 -8 A Amer… (-123 48.1) ## # … with 1,448 more rows coords tells st_as_sf() which columns contain the geographic coordinates of each airport. To graph the points on the map, we use a second geom_sf(): ggplot() + geom_sf(data = usa_48) + geom_sf(data = airports_sf, shape = 1) + coord_sf(xlim = c(-130, -60), ylim = c(20, 50)) 4.8.3.2 Symbols We can change the size or type of symbols on the map. For instance, we can draw a bubble plot (also known as a proportional symbol map) and encode the altitude of the airport through the size channel: ggplot(data = usa_48) + geom_sf() + geom_point(data = airports, aes(x = lon, y = lat, size = alt), fill = &quot;grey&quot;, color = &quot;black&quot;, alpha = .2) + coord_sf(xlim = c(-130, -60), ylim = c(20, 50)) + scale_size_area(guide = FALSE) Circle area is proportional to the airport’s altitude (in feet). Or we could scale it based on the number of arriving flights in flights: airports_n &lt;- flights %&gt;% count(dest) %&gt;% left_join(airports, by = c(&quot;dest&quot; = &quot;faa&quot;)) ggplot(data = usa_48) + geom_sf() + geom_point(data = airports_n, aes(x = lon, y = lat, size = n), fill = &quot;grey&quot;, color = &quot;black&quot;, alpha = .2) + coord_sf(xlim = c(-130, -60), ylim = c(20, 50)) + scale_size_area(guide = FALSE) airports contains a list of virtually all commercial airports in the United States. However flights only contains data on flights departing from New York City airports (JFK, LaGuardia, or Newark) and only services a few airports around the country. 4.8.3.3 Fill (choropleths) Choropleth maps encode information by assigning shades of colors to defined areas on a map (e.g. countries, states, counties, zip codes). There are lots of ways to tweak and customize these graphs, which is generally a good idea because remember that color is one of the harder-to-decode channels. We will continue to use the usa_48 simple features data frame and draw a choropleth for the number of foreign-born individuals in each state. We get those files from the census_bureau folder. Let’s also normalize our measure by the total population to get the rate of foreign-born individuals in the population: (fb_state &lt;- here(&quot;data&quot;, &quot;census_bureau&quot;, &quot;ACS_13_5YR_B05012_state&quot;, &quot;ACS_13_5YR_B05012.csv&quot;) %&gt;% read_csv() %&gt;% mutate(rate = HD01_VD03 / HD01_VD01)) ## # A tibble: 51 × 10 ## GEO.id GEO.id2 `GEO.display-la…` HD01_VD01 HD02_VD01 HD01_VD02 HD02_VD02 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0400000US01 01 Alabama 4799277 NA 4631045 2881 ## 2 0400000US02 02 Alaska 720316 NA 669941 1262 ## 3 0400000US04 04 Arizona 6479703 NA 5609835 7725 ## 4 0400000US05 05 Arkansas 2933369 NA 2799972 2568 ## 5 0400000US06 06 California 37659181 NA 27483342 30666 ## 6 0400000US08 08 Colorado 5119329 NA 4623809 5778 ## 7 0400000US09 09 Connecticut 3583561 NA 3096374 5553 ## 8 0400000US10 10 Delaware 908446 NA 831683 2039 ## 9 0400000US11 11 District of Colu… 619371 NA 534142 2017 ## 10 0400000US12 12 Florida 19091156 NA 15392410 16848 ## # … with 41 more rows, and 3 more variables: HD01_VD03 &lt;dbl&gt;, HD02_VD03 &lt;dbl&gt;, ## # rate &lt;dbl&gt; 4.8.3.3.1 Join the data Now that we have our data, we want to draw it on the map. fb_state contains one row per state, as does usa_48. Since there is a one-to-one match between the data frames, we join the data frames together first, then use that single data frame to draw the map. This differs from the approach above for drawing points because a point feature is not the same thing as a polygon feature. That is, there were more airports then there were states. Because the spatial data is stored in a data frame with one row per state, all we need to do is merge the data frames together on a column that uniquely identifies each row in each data frame. (usa_fb &lt;- usa_48 %&gt;% left_join(fb_state, by = c(&quot;STATEFP&quot; = &quot;GEO.id2&quot;))) ## Simple feature collection with 48 features and 18 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -125 ymin: 24.5 xmax: -66.9 ymax: 49.4 ## Geodetic CRS: NAD83 ## First 10 features: ## STATEFP STATENS AFFGEOID GEOID STUSPS NAME LSAD ALAND AWATER ## 1 01 01779775 0400000US01 01 AL Alabama 00 1.31e+11 4.59e+09 ## 2 05 00068085 0400000US05 05 AR Arkansas 00 1.35e+11 2.96e+09 ## 3 06 01779778 0400000US06 06 CA California 00 4.03e+11 2.05e+10 ## 4 09 01779780 0400000US09 09 CT Connecticut 00 1.25e+10 1.82e+09 ## 5 12 00294478 0400000US12 12 FL Florida 00 1.39e+11 3.14e+10 ## 6 13 01705317 0400000US13 13 GA Georgia 00 1.49e+11 4.95e+09 ## 7 16 01779783 0400000US16 16 ID Idaho 00 2.14e+11 2.40e+09 ## 8 17 01779784 0400000US17 17 IL Illinois 00 1.44e+11 6.20e+09 ## 9 18 00448508 0400000US18 18 IN Indiana 00 9.28e+10 1.54e+09 ## 10 20 00481813 0400000US20 20 KS Kansas 00 2.12e+11 1.35e+09 ## GEO.id GEO.display-label HD01_VD01 HD02_VD01 HD01_VD02 HD02_VD02 ## 1 0400000US01 Alabama 4799277 NA 4631045 2881 ## 2 0400000US05 Arkansas 2933369 NA 2799972 2568 ## 3 0400000US06 California 37659181 NA 27483342 30666 ## 4 0400000US09 Connecticut 3583561 NA 3096374 5553 ## 5 0400000US12 Florida 19091156 NA 15392410 16848 ## 6 0400000US13 Georgia 9810417 NA 8859747 7988 ## 7 0400000US16 Idaho 1583364 NA 1489560 2528 ## 8 0400000US17 Illinois 12848554 NA 11073828 10091 ## 9 0400000US18 Indiana 6514861 NA 6206801 4499 ## 10 0400000US20 Kansas 2868107 NA 2677007 3095 ## HD01_VD03 HD02_VD03 rate geometry ## 1 168232 2881 0.0351 MULTIPOLYGON (((-88.3 30.2,... ## 2 133397 2568 0.0455 MULTIPOLYGON (((-94.6 36.5,... ## 3 10175839 30666 0.2702 MULTIPOLYGON (((-119 33.5, ... ## 4 487187 5553 0.1360 MULTIPOLYGON (((-73.7 41.1,... ## 5 3698746 16848 0.1937 MULTIPOLYGON (((-80.7 24.9,... ## 6 950670 7988 0.0969 MULTIPOLYGON (((-85.6 35, -... ## 7 93804 2528 0.0592 MULTIPOLYGON (((-117 44.4, ... ## 8 1774726 10093 0.1381 MULTIPOLYGON (((-91.5 40.2,... ## 9 308060 4500 0.0473 MULTIPOLYGON (((-88.1 37.9,... ## 10 191100 3100 0.0666 MULTIPOLYGON (((-102 40, -1... 4.8.3.3.2 Draw the map With the newly combined data frame, use geom_sf() and define the fill aesthetic based on the column in usa_fb you want to visualize. ggplot(data = usa_fb) + geom_sf(aes(fill = rate)) 4.8.3.4 Bin data to discrete intervals When creating a heatmap with a continuous variable, one must decide whether to keep the variable as continuous or collapse it into a series of bins with discrete colors. While keep the variable continuous is technically more precise, the human eye cannot usually distinguish between two colors which are very similar to one another. By converting the variable to a discrete variable, you easily distinguish between the different levels. If you decide to convert a continuous variable to a discrete variable, you will need to decide how to do this. While cut() is a base R function for converting continuous variables into discrete values, ggplot2 offers two functions that explicitly define how we want to bin the numeric vector (column). cut_interval() makes n groups with equal range: usa_fb %&gt;% mutate(rate_cut = cut_interval(rate, n = 6)) %&gt;% ggplot() + geom_sf(aes(fill = rate_cut)) Whereas cut_number() makes n groups with (approximately) equal numbers of observations: usa_fb %&gt;% mutate(rate_cut = cut_number(rate, n = 6)) %&gt;% ggplot() + geom_sf(aes(fill = rate_cut)) See this StackOverflow thread for a more in-depth discussion on the merits of bucketizing a continuous variable and whether to use cut_interval() or cut_number(). 4.8.4 Changing map projection Figure 4.2: Mercator Projection Representing portions of the globe on a flat surface can be challenging. Depending on how you project the map, you can distort or emphasize certain features of the map. Fortunately, ggplot() includes the coord_sf() function which allows us to easily implement different projection methods. In order to implement coordinate transformations, you need to know the coordinate reference system that defines the projection method. The “easiest” approach is to provide what is known as the proj4string that defines the projection method. PROJ4 is a generic coordinate transformation software that allows you to convert between projection methods. If you get really into geospatial analysis and visualization, it is helpful to learn this system. For our purposes here, proj4string is a character string in R that defines the coordinate system and includes parameters specific to a given coordinate transformation. PROJ4 includes some documentation on common projection methods that can get you started. Some projection methods are relatively simple and require just the name of the projection, like for a Mercator projection (\"+proj=merc\"): map_proj_base &lt;- ggplot(data = usa_48) + geom_sf() map_proj_base + coord_sf(crs = &quot;+proj=merc&quot;) + ggtitle(&quot;Mercator projection&quot;) Other coordinate systems require specification of the standard lines, or lines that define areas of the surface of the map that are tangent to the globe. These include Gall-Peters, Albers equal-area, and Lambert azimuthal. map_proj_base + coord_sf(crs = &quot;+proj=cea +lon_0=0 +lat_ts=45&quot;) + ggtitle(&quot;Gall-Peters projection&quot;) map_proj_base + coord_sf(crs = &quot;+proj=aea +lat_1=25 +lat_2=50 +lon_0=-100&quot;) + ggtitle(&quot;Albers equal-area projection&quot;) map_proj_base + coord_sf(crs = &quot;+proj=laea +lat_0=35 +lon_0=-100&quot;) + ggtitle(&quot;Lambert azimuthal projection&quot;) 4.9 Practice drawing vector maps 4.9.1 American Community Survey The U.S. Census Bureau conducts the American Community Survey which gathers detailed information on topics such as demographics, employment, educational attainment, etc. They make a vast portion of their data available through an application programming interface (API), which can be accessed intuitively through R via the tidycensus package. We previously discussed how to use this package to obtain statistical data from the decennial census. However the Census Bureau also has detailed information on political and geographic boundaries which we can combine with their statistical measures to easily construct geospatial visualizations. If you have not already, obtain an API key and store it securely on your computer. 4.9.2 Exercise: Visualize income data Obtain information on median household income in 2017 for Cook County, IL at the tract-level using the ACS. To retrieve the geographic features for each tract, set geometry = TRUE in your function. You can use load_variables(year = 2017, dataset = \"acs5\") to retrieve the list of variables available and search to find the correct variable name. Click for the solution cook_inc &lt;- get_acs(state = &quot;IL&quot;, county = &quot;Cook&quot;, geography = &quot;tract&quot;, variables = c(medincome = &quot;B19013_001&quot;), year = 2017, geometry = TRUE) cook_inc ## Simple feature collection with 1319 features and 5 fields (with 1 geometry empty) ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -88.3 ymin: 41.5 xmax: -87.5 ymax: 42.2 ## Geodetic CRS: NAD83 ## First 10 features: ## GEOID NAME variable estimate ## 1 17031010201 Census Tract 102.01, Cook County, Illinois medincome 40841 ## 2 17031030200 Census Tract 302, Cook County, Illinois medincome 64089 ## 3 17031031700 Census Tract 317, Cook County, Illinois medincome 44555 ## 4 17031031900 Census Tract 319, Cook County, Illinois medincome 61211 ## 5 17031050200 Census Tract 502, Cook County, Illinois medincome 74375 ## 6 17031051300 Census Tract 513, Cook County, Illinois medincome 149271 ## 7 17031061500 Census Tract 615, Cook County, Illinois medincome 117656 ## 8 17031062600 Census Tract 626, Cook County, Illinois medincome 144211 ## 9 17031063400 Census Tract 634, Cook County, Illinois medincome 95488 ## 10 17031070600 Census Tract 706, Cook County, Illinois medincome 151250 ## moe geometry ## 1 7069 MULTIPOLYGON (((-87.7 42, -... ## 2 12931 MULTIPOLYGON (((-87.7 42, -... ## 3 12220 MULTIPOLYGON (((-87.7 42, -... ## 4 6343 MULTIPOLYGON (((-87.7 42, -... ## 5 18773 MULTIPOLYGON (((-87.7 42, -... ## 6 26389 MULTIPOLYGON (((-87.7 41.9,... ## 7 11416 MULTIPOLYGON (((-87.7 41.9,... ## 8 22537 MULTIPOLYGON (((-87.7 41.9,... ## 9 4904 MULTIPOLYGON (((-87.6 41.9,... ## 10 47800 MULTIPOLYGON (((-87.7 41.9,... Draw a choropleth using the median household income data. Use a continuous color gradient to identify each tract’s median household income. Click for the solution ggplot(data = cook_inc) + # use fill and color to avoid gray boundary lines geom_sf(aes(fill = estimate, color = estimate)) + # increase interpretability of graph scale_color_continuous(labels = scales::dollar) + scale_fill_continuous(labels = scales::dollar) + labs(title = &quot;Median household income in Cook County, IL&quot;, subtitle = &quot;In 2017&quot;, color = NULL, fill = NULL, caption = &quot;Source: American Community Survey&quot;) 4.9.3 Exercise: Customize your maps Draw the same choropleth for Cook County, but convert median household income into a discrete variable with 6 levels. Click for the solution Using cut_interval(): cook_inc %&gt;% mutate(inc_cut = cut_interval(estimate, n = 6)) %&gt;% ggplot() + # use fill and color to avoid gray boundary lines geom_sf(aes(fill = inc_cut, color = inc_cut)) + # increase interpretability of graph labs(title = &quot;Median household income in Cook County, IL&quot;, subtitle = &quot;In 2017&quot;, color = NULL, fill = NULL, caption = &quot;Source: American Community Survey&quot;) Using cut_number(): cook_inc %&gt;% mutate(inc_cut = cut_number(estimate, n = 6)) %&gt;% ggplot() + # use fill and color to avoid gray boundary lines geom_sf(aes(fill = inc_cut, color = inc_cut)) + # increase interpretability of graph labs(title = &quot;Median household income in Cook County, IL&quot;, subtitle = &quot;In 2017&quot;, color = NULL, fill = NULL, caption = &quot;Source: American Community Survey&quot;) Draw the same choropleth for Cook County using the discrete variable, but select an appropriate color palette using Color Brewer. Click for the solution Using cut_interval() and the Blue-Green palette: cook_inc %&gt;% mutate(inc_cut = cut_interval(estimate, n = 6)) %&gt;% ggplot() + # use fill and color to avoid gray boundary lines geom_sf(aes(fill = inc_cut, color = inc_cut)) + scale_fill_brewer(type = &quot;seq&quot;, palette = &quot;BuGn&quot;) + scale_color_brewer(type = &quot;seq&quot;, palette = &quot;BuGn&quot;) + # increase interpretability of graph labs(title = &quot;Median household income in Cook County, IL&quot;, subtitle = &quot;In 2017&quot;, color = NULL, fill = NULL, caption = &quot;Source: American Community Survey&quot;) Using cut_number() and the Blue-Green palette: cook_inc %&gt;% mutate(inc_cut = cut_number(estimate, n = 6)) %&gt;% ggplot() + # use fill and color to avoid gray boundary lines geom_sf(aes(fill = inc_cut, color = inc_cut)) + scale_fill_brewer(type = &quot;seq&quot;, palette = &quot;BuGn&quot;) + scale_color_brewer(type = &quot;seq&quot;, palette = &quot;BuGn&quot;) + # increase interpretability of graph labs(title = &quot;Median household income in Cook County, IL&quot;, subtitle = &quot;In 2017&quot;, color = NULL, fill = NULL, caption = &quot;Source: American Community Survey&quot;) You can choose any palette that is for sequential data. Use the viridis color palette for the Cook County map drawn using the continuous measure. Click for the solution ggplot(data = cook_inc) + # use fill and color to avoid gray boundary lines geom_sf(aes(fill = estimate, color = estimate)) + # increase interpretability of graph scale_color_viridis(labels = scales::dollar) + scale_fill_viridis(labels = scales::dollar) + labs(title = &quot;Median household income in Cook County, IL&quot;, subtitle = &quot;In 2017&quot;, color = NULL, fill = NULL, caption = &quot;Source: American Community Survey&quot;) 4.10 Selecting optimal color palettes Selection of your color palette is perhaps the most important decision to make when drawing a choropleth. By default, ggplot2 picks evenly spaced hues around the Hue-Chroma-Luminance (HCL) color space:20 ggplot2 gives you many different ways of defining and customizing your scale_color_ and scale_fill_ palettes, but will not tell you if they are optimal for your specific usage in the graph. 4.10.1 Color Brewer Color Brewer is a diagnostic tool for selecting optimal color palettes for maps with discrete variables. The authors have generated different color palettes designed to make differentiating between categories easy depending on the scaling of your variable. All you need to do is define the number of categories in the variable, the nature of your data (sequential, diverging, or qualitative), and a color scheme. There are also options to select palettes that are colorblind safe, print friendly, and photocopy safe. Depending on the combination of options, you may not find any color palette that matches your criteria. In such a case, consider reducing the number of data classes. 4.10.1.1 Sequential Sequential palettes work best with ordered data that progresses from a low to high value. display.brewer.all(type = &quot;seq&quot;) 4.10.1.2 Diverging Diverging palettes work for variables with meaningful mid-range values, as well as extreme low and high values. display.brewer.all(type = &quot;div&quot;) 4.10.1.3 Qualitative Qualitative palettes are best used for nominal data where there is no inherent ordering to the categories. display.brewer.all(type = &quot;qual&quot;) 4.10.2 Viridis The viridis package imports several color palettes for continuous variables from the matplotlib package in Python. These palettes have been tested to be colorful, perceptually uniform, robust to colorblindness, and pretty. To use these with ggplot2, use scale_color_viridis() and scale_fill_viridis(): library(viridis) viridis_base &lt;- ggplot(state_inc) + geom_sf(aes(fill = estimate)) + labs(title = &quot;Median household income, 2016&quot;, subtitle = &quot;Palette: viridis&quot;, caption = &quot;Source: 2016 American Community Survey&quot;, fill = NULL) + scale_fill_viridis(labels = scales::dollar) viridis_base viridis_base + scale_fill_viridis(option = &quot;cividis&quot;, labels = scales::dollar) + labs(subtitle = &quot;Palette: cividis&quot;) viridis_base + scale_fill_viridis(option = &quot;inferno&quot;, labels = scales::dollar) + labs(subtitle = &quot;Palette: inferno&quot;) viridis_base + scale_fill_viridis(option = &quot;magma&quot;, labels = scales::dollar) + labs(subtitle = &quot;Palette: magma&quot;) viridis_base + scale_fill_viridis(option = &quot;plasma&quot;, labels = scales::dollar) + labs(subtitle = &quot;Palette: plasma&quot;) Session info devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.2 (2021-11-01) ## os macOS Monterey 12.2.1 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Chicago ## date 2022-03-04 ## pandoc 2.17.1.1 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## albersusa * 0.4.1 2022-01-06 [1] Github (hrbrmstr/albersusa@07aa87f) ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.1.1) ## bit 4.0.4 2020-08-04 [1] CRAN (R 4.1.1) ## bit64 4.0.5 2020-08-30 [1] CRAN (R 4.1.0) ## bitops 1.0-7 2021-04-24 [1] CRAN (R 4.1.0) ## bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.1) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.1.1) ## broom 0.7.12 2022-01-28 [1] CRAN (R 4.1.1) ## bslib 0.3.1 2021-10-06 [1] CRAN (R 4.1.1) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.1.1) ## callr 3.7.0 2021-04-20 [1] CRAN (R 4.1.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.1.0) ## class 7.3-20 2022-01-13 [1] CRAN (R 4.1.1) ## classInt 0.4-3 2020-04-07 [1] CRAN (R 4.1.0) ## cli 3.2.0 2022-02-14 [1] CRAN (R 4.1.1) ## codetools 0.2-18 2020-11-04 [1] CRAN (R 4.1.2) ## colorspace 2.0-3 2022-02-21 [1] CRAN (R 4.1.1) ## crayon 1.5.0 2022-02-14 [1] CRAN (R 4.1.1) ## curl 4.3.2 2021-06-23 [1] CRAN (R 4.1.0) ## DBI 1.1.2 2021-12-20 [1] CRAN (R 4.1.1) ## dbplyr 2.1.1 2021-04-06 [1] CRAN (R 4.1.0) ## desc 1.4.0 2021-09-28 [1] CRAN (R 4.1.1) ## devtools 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.1.1) ## dplyr * 1.0.8 2022-02-08 [1] CRAN (R 4.1.1) ## e1071 1.7-9 2021-09-16 [1] CRAN (R 4.1.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) ## evaluate 0.15 2022-02-18 [1] CRAN (R 4.1.1) ## fansi 1.0.2 2022-01-14 [1] CRAN (R 4.1.1) ## farver 2.1.0 2021-02-28 [1] CRAN (R 4.1.0) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.1.1) ## foreign 0.8-82 2022-01-13 [1] CRAN (R 4.1.1) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.1.1) ## generics 0.1.2 2022-01-31 [1] CRAN (R 4.1.1) ## ggmap * 3.0.0 2019-02-05 [1] CRAN (R 4.1.1) ## ggplot2 * 3.3.5 2021-06-25 [1] CRAN (R 4.1.1) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.1.1) ## gridExtra 2.3 2017-09-09 [1] CRAN (R 4.1.1) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.1.1) ## haven 2.4.3 2021-08-04 [1] CRAN (R 4.1.1) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.1.0) ## highr 0.9 2021-04-16 [1] CRAN (R 4.1.0) ## hms 1.1.1 2021-09-26 [1] CRAN (R 4.1.1) ## htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.1) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.1.0) ## isoband 0.2.5 2021-07-13 [1] CRAN (R 4.1.0) ## jpeg 0.1-9 2021-07-24 [1] CRAN (R 4.1.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) ## jsonlite 1.8.0 2022-02-22 [1] CRAN (R 4.1.1) ## KernSmooth 2.23-20 2021-05-03 [1] CRAN (R 4.1.2) ## kimisc 0.4 2017-12-18 [1] CRAN (R 4.1.0) ## knitr 1.37 2021-12-16 [1] CRAN (R 4.1.1) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.1.0) ## lattice 0.20-45 2021-09-22 [1] CRAN (R 4.1.2) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.1.1) ## lubridate 1.8.0 2021-10-07 [1] CRAN (R 4.1.1) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.1.1) ## maptools 1.1-2 2021-09-07 [1] CRAN (R 4.1.1) ## MASS 7.3-55 2022-01-13 [1] CRAN (R 4.1.1) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.1.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.1.0) ## nycflights13 * 1.0.2 2021-04-12 [1] CRAN (R 4.1.0) ## patchwork * 1.1.1 2020-12-17 [1] CRAN (R 4.1.1) ## pillar 1.7.0 2022-02-01 [1] CRAN (R 4.1.1) ## pkgbuild 1.3.1 2021-12-20 [1] CRAN (R 4.1.1) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.1.0) ## pkgload 1.2.4 2021-11-30 [1] CRAN (R 4.1.1) ## plyr 1.8.6 2020-03-03 [1] CRAN (R 4.1.0) ## png 0.1-7 2013-12-03 [1] CRAN (R 4.1.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.1.0) ## processx 3.5.2 2021-04-30 [1] CRAN (R 4.1.0) ## proxy 0.4-26 2021-06-07 [1] CRAN (R 4.1.0) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.1.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.1) ## rappdirs 0.3.3 2021-01-31 [1] CRAN (R 4.1.0) ## RColorBrewer * 1.1-2 2014-12-07 [1] CRAN (R 4.1.0) ## Rcpp 1.0.8 2022-01-13 [1] CRAN (R 4.1.1) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.1.1) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.1.0) ## remotes 2.4.2 2021-11-30 [1] CRAN (R 4.1.1) ## reprex 2.0.1 2021-08-05 [1] CRAN (R 4.1.1) ## rgdal 1.5-28 2021-12-15 [1] CRAN (R 4.1.1) ## rgeos 0.5-9 2021-12-15 [1] CRAN (R 4.1.1) ## RgoogleMaps 1.4.5.3 2020-02-12 [1] CRAN (R 4.1.0) ## rjson 0.2.21 2022-01-09 [1] CRAN (R 4.1.1) ## rlang 1.0.1 2022-02-03 [1] CRAN (R 4.1.1) ## rmarkdown 2.11 2021-09-14 [1] CRAN (R 4.1.1) ## rnaturalearth * 0.1.0 2017-03-21 [1] CRAN (R 4.1.0) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.1.0) ## rvest 1.0.2 2021-10-16 [1] CRAN (R 4.1.1) ## s2 1.0.7 2021-09-28 [1] CRAN (R 4.1.1) ## sass 0.4.0 2021-05-12 [1] CRAN (R 4.1.0) ## scales 1.1.1 2020-05-11 [1] CRAN (R 4.1.0) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.1.1) ## sf * 1.0-6 2022-02-04 [1] CRAN (R 4.1.1) ## sp 1.4-6 2021-11-14 [1] CRAN (R 4.1.1) ## stringi 1.7.6 2021-11-29 [1] CRAN (R 4.1.1) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.1.1) ## testthat 3.1.2 2022-01-20 [1] CRAN (R 4.1.1) ## tibble * 3.1.6 2021-11-07 [1] CRAN (R 4.1.1) ## tidycensus * 1.1.0.9000 2022-01-25 [1] Github (walkerke/tidycensus@8b8e38a) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.1.1) ## tidyselect 1.1.2 2022-02-21 [1] CRAN (R 4.1.1) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.1.0) ## tigris 1.6 2022-02-22 [1] CRAN (R 4.1.1) ## tzdb 0.2.0 2021-10-27 [1] CRAN (R 4.1.1) ## units 0.8-0 2022-02-05 [1] CRAN (R 4.1.1) ## usethis 2.1.5 2021-12-09 [1] CRAN (R 4.1.1) ## utf8 1.2.2 2021-07-24 [1] CRAN (R 4.1.0) ## uuid 1.0-3 2021-11-01 [1] CRAN (R 4.1.1) ## vctrs 0.3.8 2021-04-29 [1] CRAN (R 4.1.0) ## viridis * 0.6.2 2021-10-13 [1] CRAN (R 4.1.1) ## viridisLite * 0.4.0 2021-04-13 [1] CRAN (R 4.1.0) ## vroom 1.5.7 2021-11-30 [1] CRAN (R 4.1.1) ## withr 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## wk 0.6.0 2022-01-03 [1] CRAN (R 4.1.2) ## xfun 0.29 2021-12-14 [1] CRAN (R 4.1.1) ## xml2 1.3.3 2021-11-30 [1] CRAN (R 4.1.1) ## yaml 2.3.5 2022-02-21 [1] CRAN (R 4.1.1) ## ## [1] /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── References "],["shiny.html", "Day 5 Shiny applications Learning objectives Assigned readings 5.1 What is Shiny? 5.2 Before we begin 5.3 Shiny app basics 5.4 Create an empty Shiny app 5.5 Load the dataset 5.6 Build the basic UI 5.7 All UI functions are simply HTML wrappers 5.8 Add inputs to the UI 5.9 Add placeholders for outputs 5.10 Checkpoint: what our app looks like after implementing the UI 5.11 Implement server logic to create outputs 5.12 Reactivity 101 5.13 Using uiOutput() to create UI elements dynamically 5.14 Final Shiny app code 5.15 Share your app with the world 5.16 More Shiny features to check out 5.17 Awesome add-on packages to Shiny 5.18 Resources Acknowledgments Session info", " Day 5 Shiny applications Learning objectives 5.0.1 Morning Introduce Shiny apps Demonstrate each element of a Shiny app Distinguish UI from server components Build the UI component of a Shiny app 5.0.2 Afternoon Define reactivity Build the server component of a Shiny app Assigned readings None. 5.1 What is Shiny? Shiny is a package from RStudio that can be used to build interactive web pages with R. While that may sound scary because of the words “web pages”, it’s geared to R users who have no experience with web development, and you do not need to know any HTML/CSS/JavaScript. You can do quite a lot with Shiny: think of it as an easy way to make an interactive web page, and that web page can seamlessly interact with R and display R objects (plots, tables, of anything else you do in R). To get a sense of the wide range of things you can do with Shiny, you can visit the Shiny gallery, which hosts examples of basic (and complex) Shiny apps. In this lesson, we’ll walk through all the steps of building a Shiny app using a subset of the city of Chicago’s current employee data set. The city annually releases an updated file of all employees of the city government, including information on department, job title, and salary/wage. We will build an app to report information specifically for wage employees. The final version of the app can be seen here. Any activity deemed as an exercise throughout this tutorial is not mandatory for building our app, but they are good for getting more practice with Shiny. If you want even more practice, another great tutorial is the official Shiny tutorial. RStudio also provides a handy cheatsheet to remember all the little details after you already learned the basics. 5.2 Before we begin Download the necessary files for the Shiny app lesson using usethis::use_course(\"uc-cfss/shiny-demo\"). You’ll need to have the shiny package, so install it. install.packages(&quot;shiny&quot;) To ensure you successfully installed Shiny, try running one of the demo apps. library(shiny) runExample(&quot;01_hello&quot;) If the example app is running, press Escape to close the app, and you are ready to build your first Shiny app! 5.3 Shiny app basics Every Shiny app is composed of a two parts: a web page that shows the app to the user, and a computer that powers the app. The computer that runs the app can either be your own laptop (such as when you’re running an app from RStudio) or a server somewhere else. You, as the Shiny app developer, need to write these two parts (you’re not going to write a computer, but rather the code that powers the app). In Shiny terminology, they are called UI (user interface) and server. UI is just a web document that the user gets to see, it’s HTML that you write using Shiny’s functions. The UI is responsible for creating the layout of the app and telling Shiny exactly where things go. The server is responsible for the logic of the app; it’s the set of instructions that tell the web page what to show when the user interacts with the page. If you look at the app we will be building, the page that you see is built with the UI code. You’ll notice there are some controls that you, as the user, can manipulate. If you adjust the price or choose a type of alcohol, you’ll notice that the plot and the table get updated. The UI is responsible for creating these controls and telling Shiny where to place the controls and where to place the plot and table, while the server is responsible for creating the actual plot or the data in the table. 5.4 Create an empty Shiny app All Shiny apps follow the same template: library(shiny) ui &lt;- fluidPage() server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) This template is by itself a working minimal Shiny app that doesn’t do much. It initializes an empty UI and an empty server, and runs an app using these empty parts. Copy this template into a new file named app.R in a new folder. It is very important that the name of the file is app.R, otherwise it would not be recognized as a Shiny app. It is also very important that you place this app in its own folder, and not in a folder that already has other R scripts or files, unless those other files are used by your app. After saving the file, RStudio should recognize that this is a Shiny app, and you should see the usual Run button at the top change to Run App. Figure 5.1: Shiny Run App If you don’t see the Run App button, it means you either have a very old version of RStudio, don’t have Shiny installed, or didn’t follow the file naming conventions. Click the Run App button, and now your app should run. You won’t see much because it’s an empty app, but you should see that the console has some text printed in the form of Listening on http://127.0.0.1:5274 and that a little stop sign appeared at the top of the console. You’ll also notice that you can’t run any commands in the console. This is because R is busy–your R session is currently powering a Shiny app and listening for user interaction (which won’t happen because the app has nothing in it yet). Click the stop button to stop the app, or press the Escape key. Figure 5.2: Shiny Stop App You may have noticed that when you click the Run App button, all it’s doing is just running the function shiny::runApp() in the console. You can run that command instead of clicking the button if you prefer. Exercise: Try running the empty app using the runApp() function instead of using the Run App button. 5.4.1 Alternate way to create a Shiny app: separate UI and server files Another way to define a Shiny app is by separating the UI and server code into two files: ui.R and server.R. This is the preferable way to write Shiny apps when the app is complex and involves more code, but in this tutorial we’ll stick to the simple single file. If you want to break up your app into these two files, you simply put all code that is assigned to the ui variable in ui.R and all the code assigned to the server function in server.R. When RStudio sees these two files in the same folder, it will know you’re writing a Shiny app. Exercise: Try making a new Shiny app by creating the two files ui.R and server.R. Remember that they have to be in the same folder. Also remember to put them in a new, isolated folder (not where your app.R already exists). 5.4.2 Let RStudio fill out a Shiny app template for you You can also create a new Shiny app using RStudio’s menu by selecting File &gt; New File &gt; Shiny Web App…. If you do this, RStudio will let you choose if you want a single-file app (app.R) or a two-file app (ui.R+server.R). RStudio will initialize a simple functional Shiny app with some code in it. I personally don’t use this feature because I find it easier to simply type the few lines of a Shiny app and save the files. 5.5 Load the dataset The raw dataset contains information about all employees of the city of Chicago (employees-all.csv). The processed dataset we’ll be using in this app is the subset of employees who are wage employees (paid hourly), as opposed to salaried employees. This subset is in the employees-wage.csv file. Add a line in your app to load the data into a variable called employ. It should look something like this (be sure to to add library(tidyverse) or library(readr) to the script so you can use the read_csv function): employ &lt;- read_csv(&quot;employees-wage.csv&quot;) Place this line in your app as the third line, just after library(shiny) and library(tidyverse). Make sure the file path and file name are correct, otherwise your app won’t run. Try to run the app to make sure the file can be loaded without errors. If you want to verify that the app can successfully read the data, you can add a print() statement after reading the data. This won’t make anything happen in your Shiny app, but you will see a summary of the dataset printed in the console, which should let you know that the dataset was indeed loaded correctly. You can place the following line after reading the data: print(glimpse(employ)) Once you get confirmation that the data is properly loaded, you can remove that line. Exercise: Load the data file into R and get a feel for what’s in it. How big is it, what variables are there, what are the normal wage ranges, etc. 5.6 Build the basic UI Let’s start populating our app with some elements visually. This is usually the first thing you do when writing a Shiny app - add elements to the UI. 5.6.1 Add plain text to the UI You can place R strings inside fluidPage() to render text. fluidPage(&quot;City of Chicago Wage Employees&quot;, &quot;hourly wage&quot;) Replace the line in your app that assigns an empty fluidPage() into ui with the one above, and run the app. The entire UI will be built by passing comma-separated arguments into the fluidPage() function. By passing regular text, the web page will just render boring unformatted text. Exercise: Add several more strings to fluidPage() and run the app. Nothing too exciting is happening yet, but you should just see all the text appear in one contiguous block. 5.6.2 Add formatted text and other HTML elements If we want our text to be formatted nicer, Shiny has many functions that are wrappers around HTML tags that format text. We can use the h1() function for a top-level header (&lt;h1&gt; in HTML), h2() for a secondary header (&lt;h2&gt; in HTML), strong() to make text bold (&lt;strong&gt; in HTML), em() to make text italicized (&lt;em&gt; in HTML), and many more. There are also functions that are wrappers to other HTML tags, such as br() for a line break, img() for an image, a() for a hyperlink, and others. All of these functions are actually just wrappers to HTML tags with the equivalent name. You can add any arbitrary HTML tag using the tags object, which you can learn more about by reading the help file on tags. Just as a demonstration, try replacing the fluidPage() function in your UI with ui &lt;- fluidPage( h1(&quot;My app&quot;), &quot;Chicago&quot;, &quot;Wage Employees&quot;, br(), &quot;Hourly&quot;, strong(&quot;wage&quot;) ) Run the app with this code as the UI. Notice the formatting of the text and understand why it is rendered that way. For people who know basic HTML: any named argument you pass to an HTML function becomes an attribute of the HTML element, and any unnamed argument will be a child of the element. That means that you can, for example, create blue text with div(\"this is blue\", style = \"color: blue;\"). Exercise: Experiment with different HTML-wrapper functions inside fluidPage(). Run the fluidPage(...) function in the console and see the HTML that it creates. 5.6.3 Add a title We could add a title to the app with h1(), but Shiny also has a special function titlePanel(). Using titlePanel() not only adds a visible big title-like text to the top of the page, but it also sets the “official” title of the web page. This means that when you look at the name of the tab in the browser, you’ll see this title. Overwrite the fluidPage() that you experimented with so far, and replace it with the simple one below, that simply has a title and nothing else. fluidPage( titlePanel(&quot;City of Chicago Wage Employees&quot;) ) Exercise: Look at the documentation for the titlePanel() function and notice it has another argument. Use that argument and see if you can see what it does. 5.6.4 Add a layout You may have noticed that so far, by just adding text and HTML tags, everything is unstructured and the elements simply stack up one below the other in one column. We’ll use sidebarLayout() to add a simple structure. It provides a simple two-column layout with a smaller sidebar and a larger main panel. We’ll build our app such that all the inputs that the user can manipulate will be in the sidebar, and the results will be shown in the main panel on the right. Add the following code after the titlePanel() sidebarLayout( sidebarPanel(&quot;our inputs will go here&quot;), mainPanel(&quot;the results will go here&quot;) ) Remember that all the arguments inside fluidPage() need to be separated by commas. So far our complete app looks like this (hopefully this isn’t a surprise to you) library(shiny) library(tidyverse) employ &lt;- read_csv(&quot;employees-wage.csv&quot;) ui &lt;- fluidPage( titlePanel(&quot;City of Chicago Wage Employees&quot;), sidebarLayout( sidebarPanel(&quot;our inputs will go here&quot;), mainPanel(&quot;the results will go here&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) Figure 5.3: Shiny layout If you want to be a lot more flexible with the design, you can have much more fine control over where things go by using a grid layout. We won’t cover that here, but if you’re interested, look at the documentation for ?column and ?fluidRow. Exercise: Add some UI into each of the two panels (sidebar panel and main panel) and see how your app now has two columns. 5.7 All UI functions are simply HTML wrappers This was already mentioned, but it’s important to remember: the entire UI is just HTML, and Shiny simply gives you easy tools to write it without having to know HTML. To convince yourself of this, look at the output when printing the contents of the ui variable. print(ui) &lt;div class=&quot;container-fluid&quot;&gt; &lt;h2&gt;City of Chicago Wage Employees&lt;/h2&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-sm-4&quot;&gt; &lt;form class=&quot;well&quot;&gt;our inputs will go here&lt;/form&gt; &lt;/div&gt; &lt;div class=&quot;col-sm-8&quot;&gt;the results will go here&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; This should make you appreciate Shiny for not making you write horrendous HTML by hand. 5.8 Add inputs to the UI Inputs are what gives users a way to interact with a Shiny app. Shiny provides many input functions to support many kinds of interactions that the user could have with an app. For example, textInput() is used to let the user enter text, numericInput() lets the user select a number, dateInput() is for selecting a date, selectInput() is for creating a select box (aka a dropdown menu). Figure 5.4: Shiny inputs All input functions have the same first two arguments: inputId and label. The inputId will be the name that Shiny will use to refer to this input when you want to retrieve its current value. It is important to note that every input must have a unique inputId. If you give more than one input the same id, Shiny will unfortunately not give you an explicit error, but your app won’t work correctly. The label argument specifies the text in the display label that goes along with the input widget. Every input can also have multiple other arguments specific to that input type. The only way to find out what arguments you can use with a specific input function is to look at its help file. Exercise: Read the documentation of ?numericInput and try adding a numeric input to the UI. Experiment with the different arguments. Run the app and see how you can interact with this input. Then try different inputs types. 5.8.1 Input for hourly wage The first input we want to have is for specifying a wage range (minimum and maximum hourly wage). The most sensible types of input for this are either numericInput() or sliderInput() since they are both used for selecting numbers. If we use numericInput(), we’d have to use two inputs, one for the minimum value and one for the maximum. Looking at the documentation for sliderInput(), you’ll see that by supplying a vector of length two as the value argument, it can be used to specify a range rather than a single number. This sounds like what we want in this case, so we’ll use sliderInput(). To create a slider input, a maximum value needs to be provided. We could manually determine the highest hourly wage rate in the dataset and hardcode this into the app. But we’re already using R, so let’s calculate it dynamically. That is, write a short piece of R code to determine the largest value in the wage column. max() does exactly that. ## [1] 109 By looking at the documentation for the slider input function, the following piece of code can be constructed. sliderInput(inputId = &quot;wage&quot;, label = &quot;Wage range&quot;, min = 0, max = max(employ$wage, na.rm = TRUE), value = c(0, max(employ$wage, na.rm = TRUE)), pre = &quot;$&quot;) Place the code for the slider input inside sidebarPanel() (replace the text we wrote earlier with this input). Exercise: Run the code of the sliderInput() in the R console and see what it returns. Change some of the parameters of sliderInput(), and see how that changes the result. It’s important to truly understand that all these functions in the UI are simply a convenient way to write HTML, as is apparent whenever you run these functions on their own. 5.8.2 Input for full/part-time While many employees of the city are full-time workers, a large portion only work for the city part-time. Part-time workers are more likely to be seasonal employees, and as such their hourly wages may systematically differ from full-time employees. It will be helpful to include an option to filter the datset between these two types of employees. For this we want some kind of a text input. But allowing the user to enter text freely isn’t the right solution because we want to restrict the user to only two choices. We could either use radio buttons or a select box for our purpose. Let’s use radio buttons for now since there are only two options, so take a look at the documentation for radioButtons() and come up with a reasonable input function code. It should look like this: radioButtons(inputId = &quot;full_time&quot;, label = &quot;Full or part-time&quot;, choices = c(&quot;Full-Time&quot;, &quot;Part-Time&quot;)) Add this input code inside sidebarPanel(), after the previous input (separate them with a comma). 5.8.3 Input for department Different departments will offer different wage structures depending on the value of the skills in demand. The city classifies wage employees into 22 distinct departments: Department Number of Employees Animal Care and Control 19 Aviation 1082 Budget &amp; Management 2 Business Affairs and Consumer Protection 7 City Council 64 Community Development 4 Cultural Affairs and Special Events 7 Emergency Management &amp; Communications 1273 Family &amp; Support 287 Finance 44 Fire 2 General Services 765 Human Resources 4 Law 40 Mayor’s Office 8 Police 10 Procurement Services 2 Public Health 3 Public Library 299 Streets &amp; Sanitation 1862 Transportation 725 Water Management 1513 The most appropriate input type in this case is probably the select box selectInput(). However we don’t want to write out the entire vector by hand: ## c(&quot;Animal Care and Control&quot;, &quot;Aviation&quot;, &quot;Budget &amp; Management&quot;, &quot;Business Affairs and Consumer Protection&quot;, &quot;City Council&quot;, &quot;Community Development&quot;, &quot;Cultural Affairs and Special Events&quot;, &quot;Emergency Management &amp; Communications&quot;, &quot;Family &amp; Support&quot;, &quot;Finance&quot;, &quot;Fire&quot;, &quot;General Services&quot;, &quot;Human Resources&quot;, &quot;Law&quot;, &quot;Mayor&#39;s Office&quot;, &quot;Police&quot;, &quot;Procurement Services&quot;, &quot;Public Health&quot;, &quot;Public Library&quot;, &quot;Streets &amp; Sanitation&quot;, &quot;Transportation&quot;, &quot;Water Management&quot;) Instead, like before we’ll extract these values directly from the data frame: selectInput(inputId = &quot;department&quot;, label = &quot;Department&quot;, choices = sort(unique(employ$department)), multiple = TRUE) Set multiple = TRUE so the user can select more than one department at a time. Add this function as well to your app. If you followed along, your entire app should have this code: library(shiny) library(tidyverse) employ &lt;- read_csv(&quot;employees-wage.csv&quot;) ui &lt;- fluidPage( titlePanel(&quot;City of Chicago Wage Employees&quot;), sidebarLayout( sidebarPanel( sliderInput(inputId = &quot;wage&quot;, label = &quot;Wage range&quot;, min = 0, max = max(employ$wage, na.rm = TRUE), value = c(0, max(employ$wage, na.rm = TRUE)), pre = &quot;$&quot;), radioButtons(inputId = &quot;full_time&quot;, label = &quot;Full or part-time&quot;, choices = c(&quot;Full-Time&quot;, &quot;Part-Time&quot;)), selectInput(inputId = &quot;department&quot;, label = &quot;Department&quot;, choices = sort(unique(employ$department)), multiple = TRUE) ), mainPanel(&quot;the results will go here&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) Figure 5.5: Shiny add inputs 5.9 Add placeholders for outputs After creating all the inputs, we should add elements to the UI to display the outputs. Outputs can be any object that R creates and that we want to display in our app - such as a plot, a table, or text. We’re still only building the UI, so at this point we can only add placeholders for the outputs that will determine where an output will be and what its ID is, but it won’t actually show anything. Each output needs to be constructed in the server code later. Shiny provides several output functions, one for each type of output. Similarly to the input functions, all the output functions have an outputId argument that is used to identify each output, and this argument must be unique for each output. 5.9.1 Output for a plot of the results At the top of the main panel we’ll have a plot showing the distribution of hourly wages. Since we want a plot, the function we use is plotOutput(). Add the following code into the mainPanel() (replace the existing text): plotOutput(&quot;hourlyPlot&quot;) This will add a placeholder in the UI for a plot named hourlyPlot. Exercise: To remind yourself that we are still merely constructing HTML and not creating actual plots yet, run the above plotOutput() function in the console to see that all it does is create some HTML. 5.9.2 Output for a table summary of the results Below the plot, we will have a table that shows a summary of the number of employees per department currently included in the plot. To get a table, we use the tableOutput() function. Here is a simple way to create a UI element that will hold a table output: tableOutput(&quot;employTable&quot;) Add this output to the mainPanel() as well. Maybe add a couple br() in between the two outputs, just as a space buffer so that they aren’t too close to each other. 5.10 Checkpoint: what our app looks like after implementing the UI If you’ve followed along, your app should now have this code: library(shiny) library(tidyverse) employ &lt;- read_csv(&quot;employees-wage.csv&quot;) ui &lt;- fluidPage( titlePanel(&quot;City of Chicago Wage Employees&quot;), sidebarLayout( sidebarPanel( sliderInput(inputId = &quot;wage&quot;, label = &quot;Wage range&quot;, min = 0, max = max(employ$wage, na.rm = TRUE), value = c(0, max(employ$wage, na.rm = TRUE)), pre = &quot;$&quot;), radioButtons(inputId = &quot;full_time&quot;, label = &quot;Full or part-time&quot;, choices = c(&quot;Full-Time&quot;, &quot;Part-Time&quot;)), selectInput(inputId = &quot;department&quot;, label = &quot;Department&quot;, choices = sort(unique(employ$department)), multiple = TRUE) ), mainPanel(plotOutput(&quot;hourlyPlot&quot;), tableOutput(&quot;employTable&quot;)) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 5.11 Implement server logic to create outputs So far we only wrote code inside that was assigned to the ui variable (or code that was written in ui.R). That’s usually the easier part of a Shiny app. Now we have to write the server function, which will be responsible for listening to changes to the inputs and creating outputs to show in the app. If you look at the server function, you’ll notice that it is always defined with two arguments: input and output. You must define these two arguments! Both input and output are list-like objects. As the names suggest, input is a list you will read values from and output is a list you will write values to. input will contain the values of all the different inputs at any given time, and output is where you will save output objects (such as tables and plots) to display in your app. 5.11.1 Building an output Recall that we created two output placeholders: hourlyPlot (a plot) and employTable (a table). We need to write code in R that will tell Shiny what kind of plot or table to display. There are three rules to build an output in Shiny. Save the output object into the output list (remember the app template - every server function has an output argument) Build the object with a render* function, where * is the type of output Access input values using the input list (every server function has an input argument) The third rule is only required if you want your output to depend on some input, so let’s first see how to build a very basic output using only the first two rules. We’ll create a plot and send it to the hourlyPlot output. output$hourlyPlot &lt;- renderPlot({ plot(rnorm(100)) }) This simple code shows the first two rules: we’re creating a plot inside the renderPlot() function, and assigning it to hourlyPlot in the output list. Remember that every output created in the UI must have a unique ID, now we see why. In order to attach an R object to an output with ID x, we assign the R object to output$x. Since hourlyPlot was defined as a plotOutput, we must use the renderPlot function, and we must create a plot inside the renderPlot function. If you add the code above inside the server function, you should see a plot with 100 random points in the app. Exercise: The code inside renderPlot() doesn’t have to be only one line, it can be as long as you’d like as long as it returns a plot. Try making a more complex plot using ggplot2. The plot doesn’t have to use our dataset, it could be anything, just to make sure you can use renderPlot(). 5.11.2 Making an output react to an input Now we’ll take the plot one step further. Instead of always plotting the same plot (100 random numbers), let’s use the minimum wage selected as the number of points to show. It doesn’t make too much sense, but it’s just to learn how to make an output depend on an input. output$hourlyPlot &lt;- renderPlot({ plot(rnorm(input$wage[1])) }) Replace the previous code in your server function with this code, and run the app. Whenever you choose a new minimum price range, the plot will update with a new number of points. Notice that the only thing different in the code is that instead of using the number 100 we are using input$wage[1]. What does this mean? Just like the variable output contains a list of all the outputs (and we need to assign code into them), the variable input contains a list of all the inputs that are defined in the UI. input$wage return a vector of length 2 containing the minimum and maximum wage. Whenever the user manipulates the slider in the app, these values are updated, and whatever code relies on it gets re-evaluated. This is a concept known as reactivity, which we will get to in a few minutes. Notice that these short 3 lines of code are using all the 3 rules for building outputs: we are saving to the output list (output$hourlyPlot &lt;-), we are using a render* function to build the output (renderPlot({})), and we are accessing an input value (input$wage[1]). 5.11.3 Building the plot output Now we have all the knowledge required to build a plot visualizing some aspect of the data. We’ll create a simple histogram of the hourly wage rate for employees by using the same 3 rules to create a plot output. First we need to make sure ggplot2 is loaded, so add a library(ggplot2) at the top (or just continue to use library(tidyverse). Next we’ll return a histogram of hourly wage wage from renderPlot(). Let’s start with just a histogram of the whole data, unfiltered. output$hourlyPlot &lt;- renderPlot({ ggplot(employ, aes(wage)) + geom_histogram() }) If you run the app with this code inside your server, you should see a histogram in the app. But if you change the input values, nothing happens yet, so the next step is to actually filter the dataset based on the inputs. Recall that we have 3 inputs: wage, full_time, and department. We can filter the data based on the values of these three inputs. For now, only filter for wage and full_time – we’ll return to department in a little bit. We’ll use dplyr functions to filter the data, so be sure to include library(dplyr) at the top. Then we’ll plot the filtered data instead of the original data. output$hourlyPlot &lt;- renderPlot({ employ %&gt;% filter(full_time == input$full_time, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) %&gt;% ggplot(aes(wage)) + geom_histogram() }) Place this code in your server function and run the app. If you change the hourly wage or full/part-time inputs, you should see the histogram update. Read this code and understand it. You’ve successfully created an interactive app - the plot is changing according to the user’s selection. To make sure we’re on the same page, here is what your code should look like at this point: library(shiny) library(tidyverse) employ &lt;- read_csv(&quot;employees-wage.csv&quot;) ui &lt;- fluidPage( titlePanel(&quot;City of Chicago Wage Employees&quot;), sidebarLayout( sidebarPanel( sliderInput(inputId = &quot;wage&quot;, label = &quot;Wage range&quot;, min = 0, max = max(employ$wage, na.rm = TRUE), value = c(0, max(employ$wage, na.rm = TRUE)), pre = &quot;$&quot;), radioButtons(inputId = &quot;full_time&quot;, label = &quot;Full or part-time&quot;, choices = c(&quot;Full-Time&quot;, &quot;Part-Time&quot;)), selectInput(inputId = &quot;department&quot;, label = &quot;Department&quot;, choices = sort(unique(employ$department)), multiple = TRUE) ), mainPanel(plotOutput(&quot;hourlyPlot&quot;), tableOutput(&quot;employTable&quot;)) ) ) server &lt;- function(input, output) { output$hourlyPlot &lt;- renderPlot({ employ %&gt;% filter(full_time == input$full_time, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) %&gt;% ggplot(aes(wage)) + geom_histogram() }) } shinyApp(ui = ui, server = server) Figure 5.6: Shiny add plot Exercise: The current plot doesn’t look very nice, you could enhance the plot and make it much more pleasant to look at. 5.11.4 Building the table output Building the next output should be much easier now that we’ve done it once. The other output we have was called employTable (as defined in the UI) and should be a table summarizing the number of employees per department in the filtered data frame. Since it’s a table output, we should use the renderTable() function. We’ll do the exact same filtering on the data, and then simply return the summarized data as a data.frame. Shiny will know that it needs to display it as a table because it’s defined as a tableOutput. The code for creating the table output should make sense to you without too much explanation: output$employTable &lt;- renderTable({ employ %&gt;% filter(full_time == input$full_time, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) %&gt;% count(department) }) Add this code to your server. Don’t overwrite the previous definition of output$hourlyPlot, just add this code before or after that, but inside the server function. Run your app, and be amazed! You can now see a table showing the number of wage employees per department that match your criteria. Notice that in building ui, we are using a predefined function called fluidPage() so all of our different elements for the UI are separated by commas. This is because each element for the page is an argument for the fluidPage() function. In building server, we are writing a new function. For this reason we don’t have to separate each element with a comma. We just write it like we would any other function! Exercise: Add a new output. Either a new plot, a new table, or some piece of text that changes based on the inputs. For example, you could add a text output (textOutput() in the UI, renderText() in the server) that says how many results were found. If you choose to do this, I recommend first adding the output to the UI, then building the output in the server with static text to make sure you have the syntax correct. Only once you can see the text output in your app you should make it reflect the inputs. Pro-tip: since textOutput() is written in the UI, you can wrap it in other UI functions. For example, h2(textOutput(...)) will result in larger text. 5.12 Reactivity 101 Shiny uses a concept called reactive programming. This is what enables your outputs to react to changes in inputs. Reactivity in Shiny is complex, but as an extreme oversimplification, it means that when the value of a variable x changes, then anything that relies on x gets re-evaluated. Notice how this is very different from what you are used to in R. Consider the following code: x &lt;- 5 y &lt;- x + 1 x &lt;- 10 What is the value of y? It’s 6. But in reactive programming, if x and y are reactive variables, then the value of y would be 11 because it would be updated whenever x is changed. This is a very powerful technique that is very useful for creating the responsiveness of Shiny apps, but it might be a bit weird at first because it’s a very different concept from what you’re used to. Only reactive variables behave this way, and in Shiny all inputs are automatically reactive. That’s why you can always use input$x in render functions, and you can be sure that whatever output depends on x will use the updated value of x whenever x changes. You might be wondering what it means to “depend” on a variable. This is not the official terminology, but it simply means that the variable is referenced in the code. So by merely accessing the value of a reactive variable, it causes the current code block to “depend” on that variable. Consider the following sample code to create a plot with a specific number of points in a specific color: output$someoutput &lt;- renderPlot({ col &lt;- input$mycolor num &lt;- input$mynumber plot(rnorm(num), col = col) }) The above render function accesses two different inputs: input$mycolor and input$mynumber. This means that this code block depends on both of these variables, so whenever either one of the two inputs is updated, the code gets re-executed with the new input values and output$someoutput is updated. 5.12.1 Creating and accessing reactive variables One very important thing to remember about reactive variables (such as the input list) is that they can only be used inside reactive contexts. Any render* function is a reactive context, so you can always use input$x or any other reactive variable inside render functions. There are two other common reactive contexts that we’ll get to in a minute: reactive({}) and observe({}). To show you what this means, let’s try accessing the price input value in the server function, without explicitly being inside a reactive context. Simply add print(input$wage) inside the server function, and you will get an error when running the app: Operation not allowed without an active reactive context. (You tried to do something that can only be done from inside a reactive expression or observer.) Shiny is very clear about what the error is: we are trying to access a reactive variable outside of a reactive context. To fix this, we can use the observe({}) function to access the input variable. Inside the server, replace print(input$wage) with observe({ print(input$wage) }), and now the app should run fine. Note that this observe({}) statement depends on input$wage, so whenever you change the value of the price, the code inside this observe({}) will run again, and the new value will be printed. This is actually a very simple yet useful debugging technique in Shiny: often you want to know what value a reactive variable holds, so you need to remember to wrap the cat(input$x) or print(input$x) by an observe({}). So far we only saw one reactive variable: the input list. You can also create your own reactive variables using the reactive({}) function. The reactive({}) function is similar to observe({}) in that it is also a reactive context, which means that it will get re-run whenever any of the reactive variables in it get updated. The difference between them is that reactive({}) returns a value. To see it in action, let’s create a variable called wageDiff that will be the difference between the maximum and minimum wage selected. If you try to naively define wageDiff &lt;- diff(input$wage), you’ll see the same error as before about doing something outside a reactive context. This is because input$wage is a reactive variable, and we can’t use a reactive variable outside a reactive context. Since we want to assign a value, we use the reactive({}) function. Try adding the following line to your server: wageDiff &lt;- reactive({ diff(input$wage) }) Now your app will run. If you want to access a reactive variable defined with reactive({}), you must add parentheses after the variable name, as if it’s a function. To demonstrate this, add observe({ print(wageDiff()) }) to your server function. Notice that we use wageDiff() rather than wageDiff. It’s very important to remember this, because you can get confusing unclear errors if you simply try to access a custom reactive variable without the parentheses. You can think of reactivity as causing a chain reaction: when one reactive value changes, anything that depends on it will get updated. If any of the updated values are themselves reactive variables, then any reactive contexts that depend on those variables will also get updated in turn. As a concrete example, let’s think about what happens when you change the value of the wage on the page. Since input$wage is a reactive variable, any expression that uses it will get updated. This means the two render functions from earlier will execute because they both depend on input$wage, as well as the wageDiff variable because it also depends on it. But since wageDiff is itself a reactive variable, Shiny will check if there is anything that depends on wageDiff, and indeed there is - the observe({}) function that prints the value of wageDiff. So once wageDiff gets updated, the observe({}) function will run, and the value will get printed. Reactivity is usually the hardest part about Shiny to understand, so if you don’t quite get it, don’t feel bad. Try reading this section again, and I promise that with time and experience you will get more comfortable with reactivity. Once you do feel more confident with reactivity, it may be a good idea to read more advanced documentation describing reactivity, since this section greatly simplifies ideas to make them more understandable. A great resource is RStudio’s tutorial on reactivity. Before continuing to the next section, you can remove all the observe({}) and reactive({}) functions we wrote in this section since they were all just for learning purposes. Exercise: Read this section again and really understand what a reactive variable means, what the 3 main reactive contexts are, how you can define reactive variables, and how a reactivity chain of events works. 5.12.2 Using reactive variables to reduce code duplication You may have noticed that we have the exact same code filtering the dataset in two places, once in each render function. We can solve that problem by defining a reactive variable that will hold the filtered dataset, and use that variable in the render functions. The first step would be to create the reactive variable. The following code should be added to the server() function. employ_filter &lt;- reactive({ employ %&gt;% filter( # filter by full or part-time full_time == input$full_time, # filter by hourly wage wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]] ) }) The variable employ_filter is being defined exactly like before, except the body is wrapped by a reactive({}), and it’s defined in the server function instead of inside the individual render functions. Now that we have our reactive variable, we can use it in the output render functions. Try it yourself, and when you think you’re done, check the code below. Don’t forget that in order to access the value of a reactive expression, you must follow the name of the variable with parentheses! This is how your server function should look like now. server &lt;- function(input, output) { employ_filter &lt;- reactive({ employ %&gt;% filter( # filter by full or part-time full_time == input$full_time, # filter by hourly wage wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]] ) }) output$hourlyPlot &lt;- renderPlot({ ggplot(employ_filter(), aes(wage)) + geom_histogram() }) output$employTable &lt;- renderTable({ employ_filter() %&gt;% count(department) }) } As a reminder, Shiny creates a dependency tree with all the reactive expressions to know what value depends on what other value. For example, when the wage input changes, Shiny looks at what values depend on input$wage, and sees that employ_filter is a reactive expression that depends on the price input, so it re-evaluates employ_filter. Then, because employ_filter is changed, Shiny now looks to see what expressions depend on employ_filter, and it finds that the two render functions use employ_filter. So Shiny re-executes the two render functions as well. 5.12.3 Blank plots showing up Let’s now consider how to incorporate the department input. Because input$department is a character vector of varying length (depending on the number of departments selected), we use the %in% operator to correctly filter employ. A simple implementation would look like this: employ_filter &lt;- reactive({ employ %&gt;% filter( # filter by full or part-time full_time == input$full_time, # filter by hourly wage wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]], department %in% input$department ) }) But notice what happens if you run the app. You get a blank plot: Figure 5.7: Shiny App with Blank Plot However once you select a department (e.g. City Council), the histrogram is correctly drawn. What gives? The problem is that if you do not select any values for selectInput(inputId = \"department\"), then the value of input$department is not all the possible values for employ$department – it’s value is NULL. So the employ data frame is being filtered to 0 rows because every observation has a value for department (even if that value is NA). Fixing this is (relatively) simple. Inside the employ_filter reactive function, we should check if the department input exists, and if not then just not filter for that column. It’s easier to do this if we do not write employ_filter as a single piped operation:21 employ_filter &lt;- reactive({ employees &lt;- employ # filter by department if(!is.null(input$department)) { employees &lt;- filter(employees, department %in% input$department) } # filter by full or part-time employees &lt;- filter(employees, full_time == input$full_time) # filter by hourly wage employees &lt;- filter(employees, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) }) Now when the render function tries to access input$department, it will get a NULL value before the user makes a selection and therefore skips filtering employ based on department. This also is a reliable fix if your app generates temporary error messages that vanish after a second. These may occur when the output relies on an object that has not yet been generated by the Shiny server() function. 5.13 Using uiOutput() to create UI elements dynamically One of the output functions you can add in the UI is uiOutput(). According to the naming convention (e.g. plotOutput() is an output to render a plot), this is an output used to render more UI. This may sound a bit confusing, but it’s actually very useful. It’s usually used to create inputs (or any other UI) from the server, or in other words - you can create inputs dynamically. Any input that you normally create in the UI is created when the app starts, and it cannot be changed. But what if one of your inputs depends on another input? In that case, you want to be able to create an input dynamically, in the server, and you would use uiOutput(). uiOutput() can be used to create any UI element, but it’s most often used to create input UI elements. The same rules regarding building outputs apply, which means the output (which is a UI element in this case) is created with the function renderUI(). 5.13.1 Basic example of uiOutput() As a very basic example, consider this app: library(shiny) ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Maximum slider value&quot;, 5), uiOutput(&quot;slider&quot;) ) server &lt;- function(input, output) { output$slider &lt;- renderUI({ sliderInput(&quot;slider&quot;, &quot;Slider&quot;, min = 0, max = input$num, value = 0) }) } shinyApp(ui = ui, server = server) If you run that tiny app, you will see that whenever you change the value of the numeric input, the slider input is re-generated. This behavior can come in handy often. 5.13.2 Use uiOutput() in our app to populate the job titles We can use this concept in our app to populate the choices for the job title selector. employ$job_title contains 145 distinct job titles, not all of which are applicable to every department. It would be helpful if we allow app users to filter the dataset by job title, we only allow as an input job titles which fall within the specified department(s). First we need to add a placeholder for the selectInput() in the UI with: uiOutput(&quot;jobTitle&quot;) Then we need to create the output (which will create a UI element - yeah, it can be a bit confusing at first), so add the following code to the server function: output$jobTitle &lt;- renderUI({ employees &lt;- employ # filter by department if(!is.null(input$department)) { employees &lt;- filter(employees, department %in% input$department) } # filter by full or part-time employees &lt;- filter(employees, full_time == input$full_time) # filter by hourly wage employees &lt;- filter(employees, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) selectInput(inputId = &quot;jobTitle&quot;, label = &quot;Job Title&quot;, choices = sort(unique(employees$job_title)), multiple = TRUE) }) Why can we not just use employ_filter() within this renderUI() function? Because we need to add a filter() in employ_filter() for the new job title selector. Relying on employ_filter() in output$jobTitle will generate a feedback loop preventing a user from reliably using the job title filter. Finally to make sure the data properly updates, change employ_filter in the server function to: employ_filter &lt;- reactive({ employees &lt;- employ # filter by department if(!is.null(input$department)) { employees &lt;- filter(employees, department %in% input$department) } # filter by job title if(!is.null(input$jobTitle)) { employees &lt;- filter(employees, job_title %in% input$jobTitle) } # filter by full or part-time employees &lt;- filter(employees, full_time == input$full_time) # filter by hourly wage employees &lt;- filter(employees, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) }) Now if you run the app, you should be able to see the different job titles available for each department and the available job titles will update as you select specific departments. 5.14 Final Shiny app code In case you got lost somewhere, here is the final code. The app is now functional, but there are plenty of features you can add to make it better. library(shiny) library(tidyverse) employ &lt;- read_csv(&quot;employees-wage.csv&quot;) ui &lt;- fluidPage( titlePanel(&quot;City of Chicago Wage Employees&quot;), sidebarLayout( sidebarPanel( sliderInput(inputId = &quot;wage&quot;, label = &quot;Wage range&quot;, min = 0, max = max(employ$wage, na.rm = TRUE), value = c(0, max(employ$wage, na.rm = TRUE)), pre = &quot;$&quot;), radioButtons(inputId = &quot;full_time&quot;, label = &quot;Full or part-time&quot;, choices = c(&quot;Full-Time&quot;, &quot;Part-Time&quot;)), selectInput(inputId = &quot;department&quot;, label = &quot;Department&quot;, choices = sort(unique(employ$department)), multiple = TRUE), uiOutput(&quot;jobTitle&quot;) ), mainPanel(plotOutput(&quot;hourlyPlot&quot;), tableOutput(&quot;employTable&quot;)) ) ) server &lt;- function(input, output) { employ_filter &lt;- reactive({ employees &lt;- employ # filter by department if(!is.null(input$department)) { employees &lt;- filter(employees, department %in% input$department) } # filter by job title if(!is.null(input$jobTitle)) { employees &lt;- filter(employees, job_title %in% input$jobTitle) } # filter by full or part-time employees &lt;- filter(employees, full_time == input$full_time) # filter by hourly wage employees &lt;- filter(employees, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) }) output$jobTitle &lt;- renderUI({ employees &lt;- employ # filter by department if(!is.null(input$department)) { employees &lt;- filter(employees, department %in% input$department) } # filter by full or part-time employees &lt;- filter(employees, full_time == input$full_time) # filter by hourly wage employees &lt;- filter(employees, wage &gt;= input$wage[[1]], wage &lt;= input$wage[[2]]) selectInput(inputId = &quot;jobTitle&quot;, label = &quot;Job Title&quot;, choices = sort(unique(employees$job_title)), multiple = TRUE) }) output$hourlyPlot &lt;- renderPlot({ ggplot(employ_filter(), aes(wage)) + geom_histogram() }) output$employTable &lt;- renderTable({ employ_filter() %&gt;% count(department) }) } shinyApp(ui = ui, server = server) 5.15 Share your app with the world Remember how every single app is a web page powered by an R session on a computer? So far, you’ve been running Shiny locally, which means your computer was used to power the app. It also means that the app was not accessible to anyone on the internet. If you want to share your app with the world, you need to host it somewhere. 5.15.1 Host on shinyapps.io RStudio provides a service called shinyapps.io which lets you host your apps for free. It is integrated seamlessly into RStudio so that you can publish your apps with the click of a button, and it has a free version. The free version allows a certain number of apps per user and a certain number of activity on each app, but it should be good enough for most of you. It also lets you see some basic stats about usage of your app. Hosting your app on shinyapps.io is the easy and recommended way of getting your app online. Go to www.shinyapps.io and sign up for an account. When you’re ready to publish your app, click on the “Publish Application” button in RStudio and follow their instructions. You might be asked to install a couple packages if it’s your first time. Figure 5.8: Shiny Publish After a successful deployment to shinyapps.io, you will be redirected to your app in the browser. You can use that URL to show off to your family what a cool app you wrote. 5.15.2 Host on a Shiny Server The other option for hosting your app is on your own private Shiny server. Shiny Server is also a product by RStudio that lets you host apps on your own server. This means that instead of RStudio hosting the app for you, you have it on your own private server. This means you have a lot more freedom and flexibility, but it also means you need to have a server and be comfortable administering a server. 5.16 More Shiny features to check out Shiny is extremely powerful and has lots of features that we haven’t covered. Here’s a sneak peek of just a few other common Shiny features that are not too advanced. 5.16.1 Shiny in Rmarkdown You can include Shiny inputs and outputs in an Rmarkdown document! This means that your Rmarkdown document can be interactive. Learn more here. Here’s a simple example of how to include interactive Shiny elements in an R Markdown document: --- output: html_document runtime: shiny --- ` ``{r echo=FALSE, eval = TRUE} sliderInput(&quot;num&quot;, &quot;Choose a number&quot;, 0, 100, 20) renderPlot({ plot(seq(input$num)) }) ` `` 5.16.2 Use conditionalPanel() to conditionally show UI elements You can use conditionalPanel() to either show or hide a UI element based on a simple condition, such as the value of another input. Learn more with ?conditionalPanel. library(shiny) ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Number&quot;, 5, 1, 10), conditionalPanel( &quot;input.num &gt;=5&quot;, &quot;Hello!&quot; ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 5.16.3 Use navbarPage() or tabsetPanel() to have multiple tabs in the UI If your apps requires more than a single “view”, you can have separate tabs. Learn more with ?navbarPage or ?tabsetPanel. library(shiny) ui &lt;- fluidPage( tabsetPanel( tabPanel(&quot;Tab 1&quot;, &quot;Hello&quot;), tabPanel(&quot;Tab 2&quot;, &quot;there!&quot;) ) ) server &lt;- function(input, output) {} shinyApp(ui = ui, server = server) 5.16.4 Use DT for beautiful, interactive tables Whenever you use tableOutput() + renderTable(), the table that Shiny creates is a static and boring-looking table. If you download the DT package, you can replace the default table with a much sleeker table by just using DT::dataTableOutput() + DT::renderDataTable(). It’s worth trying. Learn more on DT’s website. 5.16.5 Use isolate() function to remove a dependency on a reactive variable When you have multiple reactive variables inside a reactive context, the whole code block will get re-executed whenever any of the reactive variables change because all the variables become dependencies of the code. If you want to suppress this behavior and cause a reactive variable to not be a dependency, you can wrap the code that uses that variable inside the isolate() function. Any reactive variables that are inside isolate() will not result in the code re-executing when their value is changed. Read more about this behavior with ?isolate. 5.16.6 Use update*Input() functions to update input values programmatically Any input function has an equivalent update*Input function that can be used to update any of its parameters. library(shiny) ui &lt;- fluidPage( sliderInput(&quot;slider&quot;, &quot;Move me&quot;, value = 5, 1, 10), numericInput(&quot;num&quot;, &quot;Number&quot;, value = 5, 1, 10) ) server &lt;- function(input, output, session) { observe({ updateNumericInput(session, &quot;num&quot;, value = input$slider) }) } shinyApp(ui = ui, server = server) Note that we used an additional argument session when defining the server function. While the input and output arguments are mandatory, the session argument is optional. You need to define the session argument when you want to use functions that need to access the session. The session parameter actually has some useful information in it, you can learn more about it with ?shiny::session. 5.16.7 Scoping rules in Shiny apps Scoping is very important to understand in Shiny once you want to support more than one user at a time. Since your app can be hosted online, multiple users can use your app simultaneously. If there are any variables (such as datasets or global parameters) that should be shared by all users, then you can safely define them globally. But any variable that should be specific to each user’s session should be not be defined globally. You can think of the server function as a sandbox for each user. Any code outside of the server function is run once and is shared by all the instances of your Shiny app. Any code inside the server is run once for every user that visits your app. This means that any user-specific variables should be defined inside server. If you look at the code in our Virginia ABC Store app, you’ll see that we followed this rule: the raw dataset was loaded outside the server and is therefore available to all users, but the employ_filter object is constructed inside the server so that every user has their own version of it. If employ_filter was a global variable, then when one user changes the values in your app, all other users connected to your app would see the change happen. You can learn more about the scoping rules in Shiny here. 5.16.8 Use global.R to define objects available to both ui.R and server.R If there are objects that you want to have available to both ui.R and server.R, you can place them in global.R. You can learn more about global.R and other scoping rules here. 5.16.9 Add images You can add an image to your Shiny app by placing an image under the “www/” folder and using the UI function img(src = \"image.png\"). Shiny will know to automatically look in the “www/” folder for the image. 5.16.10 Add JavaScript/CSS If you know JavaScript or CSS you are more than welcome to use some in your app. library(shiny) ui &lt;- fluidPage( tags$head(tags$script(&quot;alert(&#39;Hello!&#39;);&quot;)), tags$head(tags$style(&quot;body{ color: blue; }&quot;)), &quot;Hello&quot; ) server &lt;- function(input, output) { } shinyApp(ui = ui, server = server) If you do want to add some JavaScript or use common JavaScript functions in your apps, you might want to check out shinyjs. 5.17 Awesome add-on packages to Shiny Many people have written packages that enhance Shiny in some way or add extra functionality. Here is a list of several popular packages that people often use together with Shiny: shinyjs: Easily improve the user interaction and user experience in your Shiny apps in seconds shinythemes: Easily alter the appearance of your app leaflet: Add interactive maps to your apps ggvis: Similar to ggplot2, but the plots are focused on being web-based and are more interactive shinydashboard: Gives you tools to create visual “dashboards” 5.18 Resources Shiny is a very popular package and has lots of resources on the web. Here’s a compiled list of a few resources which are all fairly easy to read and understand. Shiny official tutorial Shiny cheatsheet Lots of short useful articles about different topics in Shiny Shiny in Rmarkdown Get help from the Shiny Google group or StackOverflow Publish your apps for free with shinyapps.io Learn about how reactivity works Learn about useful debugging techniques Shiny tips &amp; tricks for improving your apps and solving common problems Acknowledgments This page is derived in part from “UBC STAT 545A and 547M”, licensed under the CC BY-NC 3.0 Creative Commons License. Session info ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.2 (2021-11-01) ## os macOS Monterey 12.2.1 ## system aarch64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Chicago ## date 2022-03-04 ## pandoc 2.17.1.1 @ /Applications/RStudio.app/Contents/MacOS/quarto/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.1.1) ## bit 4.0.4 2020-08-04 [1] CRAN (R 4.1.1) ## bit64 4.0.5 2020-08-30 [1] CRAN (R 4.1.0) ## bookdown 0.24 2021-09-02 [1] CRAN (R 4.1.1) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.1.1) ## broom 0.7.12 2022-01-28 [1] CRAN (R 4.1.1) ## bslib 0.3.1 2021-10-06 [1] CRAN (R 4.1.1) ## cachem 1.0.6 2021-08-19 [1] CRAN (R 4.1.1) ## callr 3.7.0 2021-04-20 [1] CRAN (R 4.1.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.1.0) ## cli 3.2.0 2022-02-14 [1] CRAN (R 4.1.1) ## codetools 0.2-18 2020-11-04 [1] CRAN (R 4.1.2) ## colorspace 2.0-3 2022-02-21 [1] CRAN (R 4.1.1) ## crayon 1.5.0 2022-02-14 [1] CRAN (R 4.1.1) ## curl 4.3.2 2021-06-23 [1] CRAN (R 4.1.0) ## DBI 1.1.2 2021-12-20 [1] CRAN (R 4.1.1) ## dbplyr 2.1.1 2021-04-06 [1] CRAN (R 4.1.0) ## desc 1.4.0 2021-09-28 [1] CRAN (R 4.1.1) ## devtools 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## digest 0.6.29 2021-12-01 [1] CRAN (R 4.1.1) ## dplyr * 1.0.8 2022-02-08 [1] CRAN (R 4.1.1) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.1.0) ## evaluate 0.15 2022-02-18 [1] CRAN (R 4.1.1) ## fansi 1.0.2 2022-01-14 [1] CRAN (R 4.1.1) ## fastmap 1.1.0 2021-01-25 [1] CRAN (R 4.1.0) ## forcats * 0.5.1 2021-01-27 [1] CRAN (R 4.1.1) ## fs 1.5.2 2021-12-08 [1] CRAN (R 4.1.1) ## generics 0.1.2 2022-01-31 [1] CRAN (R 4.1.1) ## ggplot2 * 3.3.5 2021-06-25 [1] CRAN (R 4.1.1) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.1.1) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.1.1) ## haven 2.4.3 2021-08-04 [1] CRAN (R 4.1.1) ## here * 1.0.1 2020-12-13 [1] CRAN (R 4.1.0) ## highr 0.9 2021-04-16 [1] CRAN (R 4.1.0) ## hms 1.1.1 2021-09-26 [1] CRAN (R 4.1.1) ## htmltools 0.5.2 2021-08-25 [1] CRAN (R 4.1.1) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.1.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.1.0) ## jsonlite 1.8.0 2022-02-22 [1] CRAN (R 4.1.1) ## knitr * 1.37 2021-12-16 [1] CRAN (R 4.1.1) ## lifecycle 1.0.1 2021-09-24 [1] CRAN (R 4.1.1) ## lubridate 1.8.0 2021-10-07 [1] CRAN (R 4.1.1) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.1.1) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.1.1) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.1.0) ## pillar 1.7.0 2022-02-01 [1] CRAN (R 4.1.1) ## pkgbuild 1.3.1 2021-12-20 [1] CRAN (R 4.1.1) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.1.0) ## pkgload 1.2.4 2021-11-30 [1] CRAN (R 4.1.1) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.1.0) ## processx 3.5.2 2021-04-30 [1] CRAN (R 4.1.0) ## ps 1.6.0 2021-02-28 [1] CRAN (R 4.1.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.1.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.1.1) ## Rcpp 1.0.8 2022-01-13 [1] CRAN (R 4.1.1) ## readr * 2.1.2 2022-01-30 [1] CRAN (R 4.1.1) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.1.0) ## remotes 2.4.2 2021-11-30 [1] CRAN (R 4.1.1) ## reprex 2.0.1 2021-08-05 [1] CRAN (R 4.1.1) ## rlang 1.0.1 2022-02-03 [1] CRAN (R 4.1.1) ## rmarkdown 2.11 2021-09-14 [1] CRAN (R 4.1.1) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [1] CRAN (R 4.1.0) ## rvest 1.0.2 2021-10-16 [1] CRAN (R 4.1.1) ## sass 0.4.0 2021-05-12 [1] CRAN (R 4.1.0) ## scales 1.1.1 2020-05-11 [1] CRAN (R 4.1.0) ## sessioninfo 1.2.2 2021-12-06 [1] CRAN (R 4.1.1) ## stringi 1.7.6 2021-11-29 [1] CRAN (R 4.1.1) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.1.1) ## testthat 3.1.2 2022-01-20 [1] CRAN (R 4.1.1) ## tibble * 3.1.6 2021-11-07 [1] CRAN (R 4.1.1) ## tidyr * 1.2.0 2022-02-01 [1] CRAN (R 4.1.1) ## tidyselect 1.1.2 2022-02-21 [1] CRAN (R 4.1.1) ## tidyverse * 1.3.1 2021-04-15 [1] CRAN (R 4.1.0) ## tzdb 0.2.0 2021-10-27 [1] CRAN (R 4.1.1) ## usethis 2.1.5 2021-12-09 [1] CRAN (R 4.1.1) ## utf8 1.2.2 2021-07-24 [1] CRAN (R 4.1.0) ## vctrs 0.3.8 2021-04-29 [1] CRAN (R 4.1.0) ## vroom 1.5.7 2021-11-30 [1] CRAN (R 4.1.1) ## withr 2.4.3 2021-11-30 [1] CRAN (R 4.1.1) ## xfun 0.29 2021-12-14 [1] CRAN (R 4.1.1) ## xml2 1.3.3 2021-11-30 [1] CRAN (R 4.1.1) ## yaml 2.3.5 2022-02-21 [1] CRAN (R 4.1.1) ## ## [1] /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
